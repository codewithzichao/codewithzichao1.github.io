<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://codewithzichao.github.io').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="æœ€è¿‘ä¸€ç›´åœ¨çœ‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‘ç°å¤§éƒ¨åˆ†æ¨¡å‹çš„æºä»£ç åŸºæœ¬ä¸Šéƒ½æ˜¯åœ¨Googleå®˜æ–¹å‘å¸ƒçš„BERTæºç çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹çš„(ä½†æ˜¯å…¨éƒ½æ˜¯TF1.xğŸ˜·ï¼Œè¿™ç‚¹æˆ‘è¦åæ§½äº†ï¼ŒæŒ‰é“ç†TF2.xå‡ºæ¥ä¹‹åï¼ŒGoogleåœ¨å¤§åŠ›æ¨å¹¿TF2.xï¼Œç„¶è€Œè¿Googleè‡ªå·±å‘å¸ƒçš„ELECTRAã€Adapter-BERTã€ALBERTç­‰ç­‰æºä»£ç éƒ½æ˜¯import tensorflow.compat.v1 as tfğŸ˜·ï¼Œexcuse me">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP|BERTæºç è§£è¯»">
<meta property="og:url" content="http://codewithzichao.github.io/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/index.html">
<meta property="og:site_name" content="codewithzichao">
<meta property="og:description" content="æœ€è¿‘ä¸€ç›´åœ¨çœ‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‘ç°å¤§éƒ¨åˆ†æ¨¡å‹çš„æºä»£ç åŸºæœ¬ä¸Šéƒ½æ˜¯åœ¨Googleå®˜æ–¹å‘å¸ƒçš„BERTæºç çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹çš„(ä½†æ˜¯å…¨éƒ½æ˜¯TF1.xğŸ˜·ï¼Œè¿™ç‚¹æˆ‘è¦åæ§½äº†ï¼ŒæŒ‰é“ç†TF2.xå‡ºæ¥ä¹‹åï¼ŒGoogleåœ¨å¤§åŠ›æ¨å¹¿TF2.xï¼Œç„¶è€Œè¿Googleè‡ªå·±å‘å¸ƒçš„ELECTRAã€Adapter-BERTã€ALBERTç­‰ç­‰æºä»£ç éƒ½æ˜¯import tensorflow.compat.v1 as tfğŸ˜·ï¼Œexcuse me">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-04T08:31:43.000Z">
<meta property="article:modified_time" content="2020-07-05T01:10:13.914Z">
<meta property="article:author" content="zichao">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://codewithzichao.github.io/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>NLP|BERTæºç è§£è¯» | codewithzichao</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">codewithzichao</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Confidentï¼ŒModestï¼ŒPatient</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/Friends/" rel="section"><i class="fa fa-fw fa-address-book"></i>Friends</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/codewithzichao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://codewithzichao.github.io/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo.jpg">
      <meta itemprop="name" content="zichao">
      <meta itemprop="description" content="Just learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="codewithzichao">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP|BERTæºç è§£è¯»
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">

            

              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-04 16:31:43" itemprop="dateCreated datePublished" datetime="2020-07-04T16:31:43+08:00">2020-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-05 09:10:13" itemprop="dateModified" datetime="2020-07-05T09:10:13+08:00">2020-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>æœ€è¿‘ä¸€ç›´åœ¨çœ‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‘ç°å¤§éƒ¨åˆ†æ¨¡å‹çš„æºä»£ç åŸºæœ¬ä¸Šéƒ½æ˜¯åœ¨Googleå®˜æ–¹å‘å¸ƒçš„BERTæºç çš„åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹çš„(ä½†æ˜¯å…¨éƒ½æ˜¯TF1.xğŸ˜·ï¼Œè¿™ç‚¹æˆ‘è¦åæ§½äº†ï¼ŒæŒ‰é“ç†TF2.xå‡ºæ¥ä¹‹åï¼ŒGoogleåœ¨å¤§åŠ›æ¨å¹¿TF2.xï¼Œç„¶è€Œè¿Googleè‡ªå·±å‘å¸ƒçš„ELECTRAã€Adapter-BERTã€ALBERTç­‰ç­‰æºä»£ç éƒ½æ˜¯import tensorflow.compat.v1 as tfğŸ˜·ï¼Œexcuse meï¼Ÿ)ã€‚æ‰€ä»¥è¿˜æ˜¯å›å¤´å†ä»”ç»†çœ‹äº†ä¸€éåŸæ¥BERTçš„æºä»£ç ã€‚ä¸è¿‡ï¼Œæ•´ä½“é˜…è¯»ä¸‹æ¥ï¼Œæ„Ÿè§‰è¿˜æ˜¯éå¸¸é¡ºç•…çš„ï¼Œä¸å¾—ä¸è¯´ä»£ç å†™çš„çœŸçš„å¥½ã€‚æ‰€ä»¥è¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯è®°å½•ä¸€ä¸‹è‡ªå·±çœ‹BERTæºä»£ç çš„è¿‡ç¨‹ã€‚</p>
<a id="more"></a>
<h2 id="BERTæ•´ä½“ä»£ç ç»“æ„"><a href="#BERTæ•´ä½“ä»£ç ç»“æ„" class="headerlink" title="BERTæ•´ä½“ä»£ç ç»“æ„"></a>BERTæ•´ä½“ä»£ç ç»“æ„</h2><p>BERTåŸç†æˆ‘åœ¨è¿™é‡Œå°±ä¸å¤šå•°å—¦äº†ï¼Œç½‘ä¸Šä¸€å¤§å †ï¼Œå½“ç„¶æ›´åŠ æ¨èçš„æ˜¯å–çœ‹åŸå§‹è®ºæ–‡ã€‚é¦–å…ˆæ¥çœ‹çœ‹BERTçš„ä»£ç æ–‡ä»¶ä¸æ•´ä½“ç»“æ„ã€‚å¦‚ä¸‹ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">â”œâ”€â”€ README.md</span><br><span class="line">â”œâ”€â”€ create_pretraining_data.py</span><br><span class="line">â”œâ”€â”€ extract_features.py</span><br><span class="line">â”œâ”€â”€ modeling.py</span><br><span class="line">â”œâ”€â”€ modeling_test.py</span><br><span class="line">â”œâ”€â”€ multilingual.md</span><br><span class="line">â”œâ”€â”€ optimization.py</span><br><span class="line">â”œâ”€â”€ optimization_test.py</span><br><span class="line">â”œâ”€â”€ predicting_movie_reviews_with_bert_on_tf_hub.ipynb</span><br><span class="line">â”œâ”€â”€ requirements.txt</span><br><span class="line">â”œâ”€â”€ run_classifier.py</span><br><span class="line">â”œâ”€â”€ run_classifier_with_tfhub.py</span><br><span class="line">â”œâ”€â”€ run_pretraining.py</span><br><span class="line">â”œâ”€â”€ run_squad.py</span><br><span class="line">â”œâ”€â”€ sample_text.txt</span><br><span class="line">â”œâ”€â”€ tokenization.py</span><br><span class="line">â””â”€â”€ tokenization_test.py</span><br></pre></td></tr></table></figure>
<ul>
<li>create_pretraining_data.pyï¼šç”¨æ¥åˆ›å»ºè®­ç»ƒå®ä¾‹ï¼›</li>
<li>extract_features.pyï¼šæå–å‡ºé¢„è®­ç»ƒçš„ç‰¹å¾ï¼›</li>
<li>modeling.pyï¼šBERTçš„æ ¸å¿ƒå»ºæ¨¡æ–‡ä»¶ï¼Œæ¨¡å‹ä¸»ä½“éƒ¨åˆ†ï¼›</li>
<li>modeling_test.pyï¼šå¯¹modeling.pyæ–‡ä»¶è¿›è¡Œunittestæµ‹è¯•ï¼›</li>
<li>optimization.pyï¼šè‡ªå®šä¹‰çš„ä¼˜åŒ–å™¨ï¼›</li>
<li>optimization_test.pyï¼šå¯¹optimization.pyæ–‡ä»¶çš„unittestæµ‹è¯•ï¼›</li>
<li>predicting_movie_reviews_with_bert_on_tf_hub.ipynbï¼šé€šè¿‡è°ƒç”¨tfhubæ¥ä½¿ç”¨BERTè¿›è¡Œé¢„æµ‹ï¼›</li>
<li>run_classifier.pyï¼šåœ¨å¤šç§æ•°æ®é›†ä¸Š(è­¬å¦‚ï¼šMRPCã€XNLIã€MNLIã€COLA)æ¥è¿›è¡ŒBERTæ¨¡å‹çš„finetuneï¼›</li>
<li>run_classifier_with_tfhub.pyï¼šé€šè¿‡è°ƒç”¨tfhubæ¥è¿›è¡Œfinetuneï¼›</li>
<li>run_pretraining.pyï¼šé€šè¿‡MLMä¸NSPä»»åŠ¡æ¥å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼›</li>
<li>run_squad.pyï¼šåœ¨squadæ•°æ®é›†ä¸Šè¿›è¡Œfinetuneï¼›</li>
<li>tokenization.pyï¼šå¯¹åŸå§‹æ•°æ®è¿›è¡Œæ¸…æ´—ã€åˆ†è¯ç­‰æ“ä½œï¼›</li>
<li>tokenization_test.pyï¼šå¯¹ tokenization.pyæ–‡ä»¶è¿›è¡Œunittestæµ‹è¯•ã€‚</li>
</ul>
<p>ä»¥ä¸Šå°±æ˜¯å„æ–‡ä»¶çš„å¤§è‡´ç®€ä»‹ï¼Œä¸‹é¢å°†å¯¹æ ¸å¿ƒä»£ç æ–‡ä»¶è¿›è¡Œèµ°è¯»ï¼Œtnesorçš„ç»´åº¦ä»¥åŠé‡è¦æ³¨é‡Šæˆ‘å‡å·²åœ¨ä»£ç é‡Œå†™æ˜ï½</p>
<h2 id="æ ¸å¿ƒä»£ç æ–‡ä»¶èµ°è¯»"><a href="#æ ¸å¿ƒä»£ç æ–‡ä»¶èµ°è¯»" class="headerlink" title="æ ¸å¿ƒä»£ç æ–‡ä»¶èµ°è¯»"></a>æ ¸å¿ƒä»£ç æ–‡ä»¶èµ°è¯»</h2><h3 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h3><p>modeling.pyæ–‡ä»¶æ˜¯BERTæ¨¡å‹çš„å®ç°ã€‚é¦–å…ˆæ¥çœ‹BertConfigç±»ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Configuration for `BertModel`."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_act=<span class="string">"gelu"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               initializer_range=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs BertConfig.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.</span></span><br><span class="line"><span class="string">      hidden_size: Size of the encoder layers and the pooler layer.</span></span><br><span class="line"><span class="string">      num_hidden_layers: Number of hidden layers in the Transformer encoder.</span></span><br><span class="line"><span class="string">      num_attention_heads: Number of attention heads for each attention layer in</span></span><br><span class="line"><span class="string">        the Transformer encoder.</span></span><br><span class="line"><span class="string">      intermediate_size: The size of the "intermediate" (i.e., feed-forward)</span></span><br><span class="line"><span class="string">        layer in the Transformer encoder.</span></span><br><span class="line"><span class="string">      hidden_act: The non-linear activation function (function or string) in the</span></span><br><span class="line"><span class="string">        encoder and pooler.</span></span><br><span class="line"><span class="string">      hidden_dropout_prob: The dropout probability for all fully connected</span></span><br><span class="line"><span class="string">        layers in the embeddings, encoder, and pooler.</span></span><br><span class="line"><span class="string">      attention_probs_dropout_prob: The dropout ratio for the attention</span></span><br><span class="line"><span class="string">        probabilities.</span></span><br><span class="line"><span class="string">      max_position_embeddings: The maximum sequence length that this model might</span></span><br><span class="line"><span class="string">        ever be used with. Typically set this to something large just in case</span></span><br><span class="line"><span class="string">        (e.g., 512 or 1024 or 2048).</span></span><br><span class="line"><span class="string">      type_vocab_size: The vocabulary size of the `token_type_ids` passed into</span></span><br><span class="line"><span class="string">        `BertModel`.</span></span><br><span class="line"><span class="string">      initializer_range: The stdev of the truncated_normal_initializer for</span></span><br><span class="line"><span class="string">        initializing all weight matrices.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.vocab_size = vocab_size</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.num_hidden_layers = num_hidden_layers</span><br><span class="line">    self.num_attention_heads = num_attention_heads</span><br><span class="line">    self.hidden_act = hidden_act</span><br><span class="line">    self.intermediate_size = intermediate_size</span><br><span class="line">    self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">    self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">    self.max_position_embeddings = max_position_embeddings</span><br><span class="line">    self.type_vocab_size = type_vocab_size</span><br><span class="line">    self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_dict</span><span class="params">(cls, json_object)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a Python dictionary of parameters."""</span></span><br><span class="line">    config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">      config.__dict__[key] = value</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_json_file</span><span class="params">(cls, json_file)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a json file of parameters."""</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      text = reader.read()</span><br><span class="line">    <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a Python dictionary."""</span></span><br><span class="line">    output = copy.deepcopy(self.__dict__)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_json_string</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a JSON string."""</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">"\n"</span></span><br></pre></td></tr></table></figure>
<p>è¿™ä¸ªä¸»è¦æ˜¯BERTæ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼Œæ³¨é‡Šå…¶å®å¾ˆè¯¦ç»†äº†ï¼Œä½†æ˜¯éœ€è¦ç‰¹åˆ«è¯´æ˜çš„æ˜¯<code>type_vocab_size</code>ï¼Œè¿™ä¸ªè¡¨ç¤ºçš„æ˜¯segment idï¼Œé»˜è®¤æ˜¯2ï¼Œä»£ç é‡Œæ²¡æœ‰ä¿®æ”¹ï¼Œä½†æ˜¯åœ¨bert_config.jsonæ–‡ä»¶é‡Œæœ‰éå¸¸è¯¦ç»†çš„è§£é‡Šã€‚</p>
<p>çœ‹å®ŒBertConfigç±»ä¹‹åå°±æ˜¯BERTæ¨¡å‹äº†ï¼Œä½†æ˜¯ç”±äºBERTæ¨¡å‹æ•´ä½“éå¸¸å¤æ‚ï¼Œæˆ‘ä»¬å…ˆæ¥çœ‹çœ‹å®ƒçš„å…¶å®çš„componentã€‚é¦–å…ˆæ¥çœ‹token embeddingéƒ¨åˆ†ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding_lookupç”¨æ¥è·å–token embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#éšæœºåˆå§‹åŒ–embedding table</span></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># flattenï¼Œç»´åº¦å˜ä¸ºï¼š[batch_size*seq_length*input_num]</span></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    <span class="comment"># å˜ä¸ºone-hotå‘é‡ï¼Œç»´åº¦æ˜¯ï¼š[batch_size*seq_length*input_num,vocab_size]</span></span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    <span class="comment"># ç»´åº¦ï¼š[batch_size*seq_length*input_num,embedding_size]</span></span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># ç»´åº¦ï¼š[batch_size*seq_length*input_num,embedding_size]</span></span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># ç»´åº¦ï¼š[batch_size,seq_length,input_num*embedding_size]</span></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure>
<p>å…¶ä¸­æ¯ä¸ªtensorçš„ç»´åº¦æˆ‘éƒ½æ ‡æ³¨çš„éå¸¸æ¸…æ¥šäº†ï¼Œçœ‹æ‡‚ä»£ç åº”è¯¥æ²¡æœ‰ä»€ä¹ˆé—®é¢˜ã€‚é€šè¿‡<code>embedding_lookup</code>å‡½æ•°ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†token embeddingä»¥åŠembedding_tableï¼Œå…¶ä¸­embedding_tableå°±æ˜¯è¯å‘é‡è¡¨ï¼Œå¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨finetuneçš„æ–¹å¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå¯ä»¥å°†è®­ç»ƒå¥½çš„embedding_tableç»™æŠ½å‡ºæ¥ï¼Œç„¶åé‡‡ç”¨feasture basedçš„æ–¹å¼æ¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒã€‚</p>
<p>é™¤äº†token embeddingä¹‹åï¼ŒBERTä¸­è¿˜æœ‰segment embeddingä¸positional embeddingï¼Œæœ€ç»ˆçš„embeddingæ˜¯è¿™ä¸‰ä¸ªembeddingç›¸åŠ å¾—åˆ°çš„ç»“æœã€‚å…·ä½“å®ç°ä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding_postprocessorç”¨æ¥å°†token embeddingã€segment embeddingä»¥åŠpositional embeddingè¿›è¡Œç›¸åŠ ï¼Œ</span></span><br><span class="line"><span class="comment"># æ¥å¾—åˆ°æœ€ç»ˆè¾“å…¥çš„embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_postprocessor</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_token_type=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,<span class="comment"># ä¸€èˆ¬æ˜¯2</span></span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_position_embeddings=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                            position_embedding_name=<span class="string">"position_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            max_position_embeddings=<span class="number">512</span>, <span class="comment"># å¿…é¡»å¤§äºç­‰äºseq_length</span></span></span></span><br><span class="line"><span class="function"><span class="params">                            dropout_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="comment"># åŠ å…¥segment embedding</span></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"`token_type_ids` must be specified if"</span></span><br><span class="line">                       <span class="string">"`use_token_type` is True."</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># This vocab will be small so we always do one-hot here, since it is always</span></span><br><span class="line">    <span class="comment"># faster for a small vocabulary.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*seq_length]</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,token_type_vocab_size]</span></span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,width]</span></span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    <span class="comment"># [batch_size,seq_length,width]</span></span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="comment"># åŠ å…¥positional embedding</span></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># [seq_length,width]</span></span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, <span class="number">-1</span>])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line"></span><br><span class="line">      <span class="comment"># [1,seq_length,width]</span></span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      <span class="comment"># [batch_size,seq_length,width]</span></span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="comment"># ç»è¿‡layer normä»¥åŠdropout</span></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><strong>è¿™é‡Œéœ€è¦æ³¨æ„ä¸€ä¸‹ï¼Œåœ¨BERTä¸­ï¼Œembedding_size=hidden_sizeã€‚</strong>å¾—åˆ°embeddingä¹‹åæˆ‘ä»¬å°±éœ€è¦å°†è¾“å…¥è¾“å…¥åˆ°transformerä¸­äº†ï¼Œåœ¨BERTå½“ä¸­ï¼Œtransformerç”±12ä¸ªself-attention layerå †å è€Œæˆã€‚æ‰€ä»¥ï¼Œé¦–å…ˆæ¥çœ‹çœ‹self-attention layerçš„å®ç°ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_return_2d_tensor=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(input_tensor, batch_size, num_attention_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                           seq_length, width)</span>:</span></span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ä¹‹æ‰€ä»¥éœ€è¦ä½¿ç”¨transposeï¼Œä¸»è¦æ˜¯å› ä¸ºåé¢è¦åšQK.Tï¼Œå…¶å®æˆ‘è§‰å¾—ä½¿ç”¨tf.einsumå°±ä¸éœ€è¦è½¬ç½®äº†</span></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  æˆ‘ä»¬æŠŠfrom_tensorçœ‹ä½œQï¼Œto_tensorçœ‹ä½œkï¼Œvï¼</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*from_seq_length,from_width]</span></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">   <span class="comment"># [batch_size*to_seq_length,to_width]</span></span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*from_seq_length,num_attention_heads*size_per_head]ï¼Œå³è®ºæ–‡é‡Œæåˆ°çš„çº¿æ€§å˜æ¢</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">"query"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*to_seq_length,num_attention_heads*size_per_head]ï¼Œå³è®ºæ–‡é‡Œæåˆ°çš„çº¿æ€§å˜æ¢</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">"key"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*to_seq_length,num_attention_heads*size_per_head]ï¼Œå³è®ºæ–‡é‡Œæåˆ°çš„çº¿æ€§å˜æ¢</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">"value"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,size_per_head]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,to_seq_length,size_per_head]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,to_seq_length]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># æ”¾ç¼©</span></span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># åœ¨softmaxä¹‹åï¼Œå¯¹logitsè¿›è¡Œmaskï¼Œé˜²æ­¢è¢«paddingçš„ä½ç½®çš„å€¼ä¹Ÿå‚ä¸è®¡ç®—ï¼</span></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    åœ¨attention_maskä¸­ï¼Œ1è¡¨ç¤ºçœŸå®çš„é•¿åº¦ï¼Œ0è¡¨ç¤ºæ˜¯paddingçš„</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># æˆ‘ä»¬è®©paddingçš„ä½ç½®çš„å€¼ä¸ºæ— ç©·å°ï¼Œè¿™æ ·softmaxä¹‹åçš„å€¼å°±ä¼šæ¥è¿‘äº0ï¼</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># # [batch_size,num_attention_heads,from_seq_length,to_seq_length]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,to_seq_length,num_attention_heads,size_per_head]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,to_seq_length,size_per_head]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,size_per_head]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,from_seq_length,num_attention_heads,to_seq_length]</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure>
<p>è¿™æ˜¯æ ‡å‡†çš„self-attentionå±‚çš„å®ç°ï¼Œæˆ‘è§‰å¾—æœ‰å¾ˆå¤šåœ°æ–¹å¯ä»¥å€Ÿé‰´ï¼Œè­¬å¦‚ï¼šè®©paddingçš„ä½ç½®ç»è¿‡softmaxçš„å€¼æ— é™æ¥è¿‘äº0ï¼Œä»¥åŠåœ¨åšQK.Tçš„è®¡ç®—çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸€å¼€å§‹å°±æŠŠè¾“å…¥ç»™å®ƒreshapeå±‚2ç»´çš„ï¼Œå³ï¼š<code>[batch_size,seq_length,width_size]</code>è½¬åŒ–åˆ°<code>[batch_size*seq_length,num_attention_heads*size_per_head]</code>ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒï¼Œè¿™ä¸ªå…¶å®æˆ‘ä¹‹å‰éƒ½æ²¡æƒ³è¿‡ï¼Œåªæœ‰åœ¨çœ‹å¼€æºä»£ç çš„æ—¶å€™æ‰èƒ½çŸ¥é“ã€‚å®šä¹‰äº†sefl-attention layerä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹transfomerçš„å®ç°ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,<span class="comment"># æœ€ç»ˆè¾“å‡ºç»“æœçš„ç»´åº¦</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,<span class="comment"># self-attention layerçš„æ•°ç›®</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>, <span class="comment"># æ¯ä¸€ä¸ªself-attention layerä¸­headçš„æ•°ç›®</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>, <span class="comment"># ä¸­é—´ç»´åº¦ï¼Œå³FFNçš„ç»´åº¦</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,<span class="comment"># FFNçš„æ¿€æ´»å‡½æ•°</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span> <span class="comment"># æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„è¾“å‡ºç»“æœ</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*seq_length,input_width]</span></span><br><span class="line">  <span class="comment"># input_width=hidden_size</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># listï¼Œæ€»å…±æœ‰num_hidden_layersï¼Œæ¯ä¸€ä¸ªå…ƒç´ çš„ç»´åº¦æ˜¯ï¼š[batch_size*seq_length,hidden_size]</span></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          <span class="comment"># [batch_size*seq_length,num_attention_heads*size_per_head]</span></span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`.</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          <span class="comment"># ç»´åº¦å˜æ¢ï¼Œå˜ä¸ºï¼š[batch_size*seq_length,hidden]</span></span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          <span class="comment"># dropout</span></span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          <span class="comment"># æ®‹å·®è¿æ¥</span></span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        <span class="comment"># ä½¿ç”¨geluï¼Œå˜ä¸ºï¼š[batch_size*seq_length,intermediate_size]</span></span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        <span class="comment"># æ— æ¿€æ´»å‡½æ•°ï¼Œçº¯çº¿æ€§å˜æ¢ï¼Œå˜ä¸ºï¼š[batch_size*seq_length,hidden_size]</span></span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># æ®‹å·®è¿æ¥</span></span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        <span class="comment"># å­˜å‚¨è¿™ä¸€å±‚çš„ç»“æœ</span></span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>
<p>ç°åœ¨ï¼Œæˆ‘ä»¬å°†embeddingä»¥åŠtransfomerç»™ä¸²èµ·æ¥ï¼Œå¾—åˆ°å®Œæ•´çš„BERTæ¨¡å‹ã€‚å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""BERT model ("Bidirectional Encoder Representations from Transformers").</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  ```python</span></span><br><span class="line"><span class="string">  # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">  pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">  logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for BertModel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      <span class="comment"># å¦‚æœä¸æ˜¯è®­ç»ƒçš„è¯ï¼Œé‚£ä¹ˆä¸èƒ½ä½¿ç”¨dropoutï¼</span></span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        <span class="comment"># è·å–token embeddingï¼Œä»¥åŠembedding table</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores.</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># å¦‚æœdo_return_all_layers=Trueï¼Œè¿”å›ä¸€ä¸ªlistï¼Œå…ƒç´ ä¸ªæ•°æ˜¯num_hidden_layers,</span></span><br><span class="line">        <span class="comment"># æ¯ä¸€ä¸ªå…ƒç´ ç»´åº¦ï¼š[batch_size,seq_length,hidden_size];</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># å¦‚æœdo_return_all_layers=Falseï¼Œé‚£ä¹ˆå°±å€¼è¿”å›æœ€é¡¶å±‚çš„layerçš„è¾“å‡ºç»“æœï¼Œ</span></span><br><span class="line">        <span class="comment"># ç»´åº¦æ˜¯ï¼š[batch_size,seq_length,hidden_size]</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># å–åˆ°æœ€é¡¶å±‚çš„ç»“æœï¼Œç»´åº¦æ˜¯ï¼š[batch_size,seq_length,hidden_size]</span></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [batch_size,hidden_size],ç›¸å½“äºæ˜¯poolingæ“ä½œï¼Œæˆ‘ä»¬åªå–ä¸ç¬¬ä¸€ä¸ªå…ƒç´ ç›¸å…³çš„hidden stateï¼</span></span><br><span class="line">        <span class="comment"># ä¸ºä»€ä¹ˆè¦å–ä¸ç¬¬ä¸€ä¸ªtokenç›¸å…³çš„hidden stateï¼Ÿ</span></span><br><span class="line">        <span class="comment"># å› ä¸ºåœ¨BERTä¸­ï¼Œç¬¬ä¸€ä¸ªtokenæ˜¯[CLS]ï¼Œç”¨äºåˆ†ç±»çš„ï¼è¿™ä¸ªå¾ˆé‡è¦ï¼</span></span><br><span class="line"></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [batch_size,hidden_size]</span></span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_pooled_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.pooled_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_sequence_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets final hidden layer of encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the final hidden of the transformer encoder.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.sequence_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_all_encoder_layers</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets output of the embedding lookup (i.e., input to the transformer).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the output of the embedding layer, after summing the word</span></span><br><span class="line"><span class="string">      embeddings with the positional embeddings and the token type embeddings,</span></span><br><span class="line"><span class="string">      then performing layer normalization. This is the input to the transformer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_table</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_table</span><br></pre></td></tr></table></figure>
<p>è·å–æœ€é¡¶å±‚çš„[CLS]tokençš„tensorç”¨äºè®­ç»ƒNSPä»»åŠ¡ï¼Œå¦‚æœä¸‹æ¸¸ä»»åŠ¡æ˜¯åˆ†ç±»ä»»åŠ¡çš„è¯ï¼Œæˆ‘ä»¬æœ€ç»ˆä¹Ÿæ˜¯åœ¨è¿™ä¸ªçš„åŸºç¡€ä¸Šï¼Œæ¥ä»‹å…¥softmaxæˆ–è€…å…¶ä»–çš„ç»“æ„æ¥åšï¼›æ­¤å¤–ï¼Œè·å–æœ€é¡¶å±‚self-ateention layerçš„è¾“å‡ºç»“æœï¼Œç”¨æ¥è®­ç»ƒMLMä»»åŠ¡ã€‚</p>
<h3 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h3><p>tokenizaton.pyæ–‡ä»¶ç”¨æ¥å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€è¯å¹²åŒ–ã€å°å†™ã€å»é™¤ç©ºæ ¼ç­‰ç­‰æ“ä½œï¼Œå¹¶å°†åŸå§‹æ–‡æœ¬å‘é‡åŒ–ã€‚åœ¨è¿™é‡Œï¼Œéœ€è¦ç‰¹åˆ«æä¸€ä¸‹çš„è¯å°±æ˜¯åˆ†è¯è¿™å—ï¼ŒBERTä½¿ç”¨äº†ä¸¤ç§åˆ†è¯ï¼Œé¦–å…ˆå¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œç²—ç²’åº¦çš„åˆ†è¯ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›è¡Œwordpieceåˆ†è¯ï¼Œå¾—åˆ°æ›´åŠ ç»†ç²’åº¦çš„åˆ†è¯ç»“æœã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<p>é¦–å…ˆæ˜¯ç²—ç²’åº¦çš„åˆ†è¯ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs basic tokenization (punctuation splittingæ ‡ç‚¹ç¬¦å·æ‹†åˆ†, lower casing, etc.)."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, do_lower_case=True)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      do_lower_case: Whether to lower case the input.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="string">"""Tokenizes a piece of text."""</span></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">    <span class="comment"># models. This is also applied to the English models now, but it doesn't</span></span><br><span class="line">    <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">    <span class="comment"># and generally don't have any Chinese data in them (there are Chinese</span></span><br><span class="line">    <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">    <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¢åŠ ä¸­æ–‡æ”¯æŒ</span></span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line"></span><br><span class="line">    orig_tokens = whitespace_tokenize(text)</span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">      <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line"></span><br><span class="line">    output_tokens = whitespace_tokenize(<span class="string">" "</span>.join(split_tokens))</span><br><span class="line">    <span class="comment">#è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²</span></span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<p>ç„¶åæ˜¯wordpieceåˆ†è¯ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">WordpieceTokenizeræ˜¯å°†BasicTokenizerçš„ç»“æœè¿›ä¸€æ­¥åšæ›´ç»†ç²’åº¦çš„åˆ‡åˆ†ã€‚</span></span><br><span class="line"><span class="string">åšè¿™ä¸€æ­¥çš„ç›®çš„ä¸»è¦æ˜¯ä¸ºäº†å»é™¤æœªç™»å½•è¯å¯¹æ¨¡å‹æ•ˆæœçš„å½±å“ã€‚</span></span><br><span class="line"><span class="string">è¿™ä¸€è¿‡ç¨‹å¯¹ä¸­æ–‡æ²¡æœ‰å½±å“ï¼Œ</span></span><br><span class="line"><span class="string">å› ä¸ºåœ¨å‰é¢BasicTokenizeré‡Œé¢å·²ç»åˆ‡åˆ†æˆä»¥å­—ä¸ºå•ä½çš„äº†ã€‚</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs WordPiece tokenziation."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token=<span class="string">"[UNK]"</span>, max_input_chars_per_word=<span class="number">200</span>)</span>:</span></span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word <span class="comment"># æ¯ä¸€ä¸ªtokençš„æœ€å¤§å­—ç¬¦æ•°ç›®</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="comment"># ä½¿ç”¨è´ªå¿ƒçš„æœ€å¤§æ­£å‘åŒ¹é…ç®—æ³•</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    æˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­æ¥çœ‹ä»£ç çš„æ‰§è¡Œè¿‡ç¨‹ã€‚æ¯”å¦‚å‡è®¾è¾“å…¥æ˜¯â€unaffableâ€ã€‚</span></span><br><span class="line"><span class="string">    æˆ‘ä»¬è·³åˆ°whileå¾ªç¯éƒ¨åˆ†ï¼Œè¿™æ˜¯start=0ï¼Œend=len(chars)=9ï¼Œä¹Ÿå°±æ˜¯å…ˆçœ‹çœ‹unaffableåœ¨ä¸åœ¨è¯å…¸é‡Œï¼Œ</span></span><br><span class="line"><span class="string">    å¦‚æœåœ¨ï¼Œé‚£ä¹ˆç›´æ¥ä½œä¸ºä¸€ä¸ªWordPieceï¼Œ</span></span><br><span class="line"><span class="string">    å¦‚æœä¸å†ï¼Œé‚£ä¹ˆend-=1ï¼Œä¹Ÿå°±æ˜¯çœ‹unaffablåœ¨ä¸åœ¨è¯å…¸é‡Œï¼Œæœ€ç»ˆå‘ç°â€unâ€åœ¨è¯å…¸é‡Œï¼ŒæŠŠunåŠ åˆ°ç»“æœé‡Œã€‚</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    æ¥ç€start=2ï¼Œçœ‹affableåœ¨ä¸åœ¨ï¼Œä¸åœ¨å†çœ‹affablï¼Œâ€¦ï¼Œ</span></span><br><span class="line"><span class="string">    æœ€åå‘ç° ##aff åœ¨è¯å…¸é‡Œã€‚</span></span><br><span class="line"><span class="string">    æ³¨æ„ï¼š##è¡¨ç¤ºè¿™ä¸ªè¯æ˜¯æ¥ç€å‰é¢çš„ï¼Œè¿™æ ·ä½¿å¾—WordPieceåˆ‡åˆ†æ˜¯å¯é€†çš„â€”â€”æˆ‘ä»¬å¯ä»¥æ¢å¤å‡ºâ€œçœŸæ­£â€çš„è¯ã€‚</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">"""Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">    using the given vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string">      input = "unaffable"</span></span><br><span class="line"><span class="string">      output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">        already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">    output_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">      chars = list(token)</span><br><span class="line">      <span class="comment"># å¦‚æœå¤§äºè®¾ç½®çš„æ¯ä¸€ä¸ªtokençš„æœ€å¤§å­—ç¬¦æ•°ç›®çš„è¯ï¼Œé‚£ä¹ˆå°±å½“ä½œUNK</span></span><br><span class="line">      <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      is_bad = <span class="literal">False</span></span><br><span class="line">      start = <span class="number">0</span></span><br><span class="line">      sub_tokens = []</span><br><span class="line">      <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">        end = len(chars)</span><br><span class="line">        cur_substr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; end:</span><br><span class="line">          substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">          <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            substr = <span class="string">"##"</span> + substr</span><br><span class="line">          <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          is_bad = <span class="literal">True</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<p>å°†è¿™ä¸¤åˆ†è¯è¿›è¡Œç»“åˆï¼Œå¾—åˆ°ç»†ç²’åº¦çš„åˆ†è¯ç»“æœï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs end-to-end tokenziation."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_file, do_lower_case=True)</span>:</span></span><br><span class="line">    self.vocab = load_vocab(vocab_file)</span><br><span class="line">    <span class="comment"># vè¡¨ç¤ºindexï¼Œkè¡¨ç¤ºtokenæœ¬èº«</span></span><br><span class="line">    self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="comment"># è°ƒç”¨BasicTokenizerç²—ç²’åº¦åˆ†è¯</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text):</span><br><span class="line">       <span class="comment"># è°ƒç”¨WordpieceTokenizerç»†ç²’åº¦åˆ†è¯</span></span><br><span class="line">      <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">        split_tokens.append(sub_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure>
<h3 id="create-pretraining-data-py"><a href="#create-pretraining-data-py" class="headerlink" title="create_pretraining_data.py"></a>create_pretraining_data.py</h3><p>è¿™éƒ¨åˆ†ä¸»è¦æ˜¯åœ¨tokenization.pyçš„åŸºç¡€ä¸Šï¼Œåˆ›å»ºè®­ç»ƒå®ä¾‹ã€‚æˆ‘ä»¬æ¥çœ‹BERTä¸­æ˜¯æ€ä¹ˆå®ç°éšæœºMASKæ“ä½œçš„ã€‚ä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span><span class="params">(tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates the predictions for the masked LM objective."""</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    <span class="comment"># [CLS]å’Œ[SEP]ä¸èƒ½ç”¨äºMASK</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">"[CLS]"</span> <span class="keyword">or</span> token == <span class="string">"[SEP]"</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> len(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">"##"</span>)):</span><br><span class="line">      cand_indexes[<span class="number">-1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  output_tokens = list(tokens)</span><br><span class="line"></span><br><span class="line">  num_to_predict = min(max_predictions_per_seq,</span><br><span class="line">                       max(<span class="number">1</span>, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = set()</span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">"[MASK]"</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#æ³¨æ„ï¼Œè¿™æ˜¯20%çš„å‰50%,æ‰€ä»¥æ˜¯10%ï¼ï¼ï¼</span></span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="comment">#æ³¨æ„ï¼Œè¿™æ˜¯20%çš„å50%,æ‰€ä»¥æ˜¯10%ï¼ï¼ï¼</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, len(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> len(masked_lms) &lt;= num_to_predict</span><br><span class="line">  masked_lms = sorted(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure>
<p>ç„¶åæˆ‘ä»¬å¯ä»¥è¾“å…¥å‘½ä»¤ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python create_pretraining_data.py \</span><br><span class="line">  --input_file=/Users/codewithzichao/Desktop/programs/bert/sample_text.txt \</span><br><span class="line">  --output_file=/Users/codewithzichao/Desktop/programs/bert/tf_examples.tfrecord \</span><br><span class="line">  --vocab_file=/Users/codewithzichao/Downloads/uncased_L-24_H-1024_A-16/vocab.txt \</span><br><span class="line">  --do_lower_case=True \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --masked_lm_prob=0.15 \</span><br><span class="line">  --random_seed=12345 \</span><br><span class="line">  --dupe_factor=5</span><br></pre></td></tr></table></figure>
<p>å¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">I0704 18:10:11.231426 4486237632 create_pretraining_data.py:160] *** Example ***</span><br><span class="line">INFO:tensorflow:tokens: [CLS] and there burst on phil <span class="comment">##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea [MASK] ring ##ed with palaces and towers [MASK] [SEP] like most of [MASK] fellow gold - seekers , cass was super ##sti [MASK] . [SEP]</span></span><br><span class="line">I0704 18:10:11.231508 4486237632 create_pretraining_data.py:162] tokens: [CLS] and there burst on phil <span class="comment">##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea [MASK] ring ##ed with palaces and towers [MASK] [SEP] like most of [MASK] fellow gold - seekers , cass was super ##sti [MASK] . [SEP]</span></span><br><span class="line">INFO:tensorflow:input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 103 3614 2098 2007 22763 1998 7626 103 102 2066 2087 1997 103 3507 2751 1011 24071 1010 16220 2001 3565 16643 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231609 4486237632 create_pretraining_data.py:172] input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 103 3614 2098 2007 22763 1998 7626 103 102 2066 2087 1997 103 3507 2751 1011 24071 1010 16220 2001 3565 16643 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231703 4486237632 create_pretraining_data.py:172] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231793 4486237632 create_pretraining_data.py:172] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_positions: 10 20 23 27 32 39 42 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231858 4486237632 create_pretraining_data.py:172] masked_lm_positions: 10 20 23 27 32 39 42 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_ids: 22741 1010 2007 1012 2010 2001 20771 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231920 4486237632 create_pretraining_data.py:172] masked_lm_ids: 22741 1010 2007 1012 2010 2001 20771 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br><span class="line">I0704 18:10:11.231988 4486237632 create_pretraining_data.py:172] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br><span class="line">INFO:tensorflow:next_sentence_labels: 1</span><br><span class="line">I0704 18:10:11.232054 4486237632 create_pretraining_data.py:172] next_sentence_labels: 1</span><br><span class="line">INFO:tensorflow:Wrote 60 total instances</span><br><span class="line">I0704 18:10:11.242784 4486237632 create_pretraining_data.py:177] Wrote 60 total instances</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¾“å‡ºç»“æœæœ‰ï¼š</p>
<ul>
<li>input_idsï¼špaddingä¹‹åçš„tokensï¼›</li>
<li>input_maskï¼šå¯¹input_idsè¿›è¡Œmaskå¾—åˆ°çš„ç»“æœï¼›</li>
<li>segment_idsï¼š0è¡¨ç¤ºçš„æ˜¯ç¬¬ä¸€ä¸ªå¥å­ï¼Œ1è¡¨ç¤ºç¬¬äºŒä¸ªå¥å­ï¼Œåé¢çš„0è¡¨ç¤ºpadding</li>
<li>masked_lm_positionsï¼šè¡¨ç¤ºè¢«éšæœºMASKæ‰çš„tokenåœ¨instanceä¸­çš„ä½ç½®ï¼›</li>
<li>masked_lm_idsï¼šè¡¨ç¤ºè¢«éšæœºMASKæ‰çš„tokenåœ¨è¯æ±‡è¡¨ä¸­çš„ç¼–ç ï¼›</li>
<li>masked_lm_weigthsï¼šè¡¨ç¤ºè¢«éšæœºMASKæ‰çš„tokençš„åºåˆ—ï¼Œå…¶ä¸­1è¡¨ç¤ºMASKæ‰çš„tokenæ˜¯åŸå§‹çš„æ–‡æœ¬tokenï¼Œ0è¡¨ç¤ºMASKçš„æ˜¯paddingä¹‹åçš„tokenã€‚</li>
</ul>
<p>å½“ç„¶äº†ï¼Œè¾“å…¥çš„æ–‡æœ¬ä¹Ÿæœ‰è¦æ±‚çš„ï¼šä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªå¥å­ï¼Œä¸åŒæ–‡ç« ä¹‹é—´è¦éš”ä¸€ä¸ªç©ºè¡Œã€‚</p>
<h3 id="run-pretraining-py"><a href="#run-pretraining-py" class="headerlink" title="run_pretraining.py"></a>run_pretraining.py</h3><p>è¿™éƒ¨åˆ†æ˜¯å¯¹BERTæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚ä¸€èˆ¬æˆ‘ä»¬è¿™éƒ¨åˆ†éƒ½æ˜¯ä¸ç”¨ç®¡çš„ï¼Œç›´æ¥åŠ è½½å·²ç»è®­ç»ƒå¥½çš„BERTæ¨¡å‹æƒé‡å°±å¯ä»¥äº†(ç›´æ¥è®­ç»ƒä¸ªäººåŸºæœ¬ä¸Šä¸å¤ªå¯èƒ½ï¼Œå¤ªè€—é’±äº†ï¼ŒGoogleéƒ½æ˜¯ç”¨å¤šå—TPUè®­ç»ƒäº†å¥½å‡ å¤©ã€‚ã€‚ã€‚)å¤§è‡´çœ‹ä¸€ä¸‹å®ƒçš„ä»£ç ç»“æ„å§ï½</p>
<p>ç”±äºåœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒBERTæ˜¯ä½¿ç”¨MLMä»»åŠ¡ä¸NSPä»»åŠ¡æ¥è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥å…ˆæ¥çœ‹ä¸€ä¸‹è¿™ä¸¤ä¸ªä»»åŠ¡å§ï½</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®šä¹‰MLMä»»åŠ¡</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_masked_lm_output</span><span class="params">(bert_config, input_tensor, output_weights, positions,</span></span></span><br><span class="line"><span class="function"><span class="params">                         label_ids, label_weights)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the masked LM."""</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size,seq_length,hidden_size]ï¼Œæ˜¯transformeræœ€åä¸€å±‚çš„è¾“å‡ºç»“æœ</span></span><br><span class="line"><span class="string">  output_weights:[vocab_size,embedding_size] è¯æ±‡è¡¨</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># è·å–maskè¯çš„encodeï¼ŒMLM lossåªè®¡ç®—è¢«maskæ‰çš„ä½ç½®çš„ lossï¼</span></span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line">  <span class="comment"># ç»´åº¦ï¼š[batch_size*max_pred_pre_seq,hidden_size]</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/predictions"</span>):</span><br><span class="line">    <span class="comment"># We apply one more non-linear transformation before the output layer.</span></span><br><span class="line">    <span class="comment"># This matrix is not used after pre-training.</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"transform"</span>):</span><br><span class="line">      <span class="comment"># çº¿æ€§å˜æ¢ï¼Œåœ¨è¾“å‡ºä¹‹å‰æ·»åŠ ä¸€ä¸ªéçº¿æ€§å˜æ¢ï¼Œåªåœ¨é¢„è®­ç»ƒé˜¶æ®µèµ·ä½œç”¨</span></span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=bert_config.hidden_size,</span><br><span class="line">          activation=modeling.get_activation(bert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              bert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The output weights are the same as the input embeddings, but there is</span></span><br><span class="line">    <span class="comment"># an output-only bias for each token.</span></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>,</span><br><span class="line">        shape=[bert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment"># å¾—åˆ°è¿™ä¸ªbatchä¸‹çš„ç»“æœ</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq,vocab_size]</span></span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># label_idsè¡¨ç¤ºmaskæ‰çš„Tokençš„id</span></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq]</span></span><br><span class="line">    label_ids = tf.reshape(label_ids, [<span class="number">-1</span>])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq,vocab_size]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The `positions` tensor might be zero-padded (if the sequence is too</span></span><br><span class="line">    <span class="comment"># short to have the maximum number of predictions). The `label_weights`</span></span><br><span class="line">    <span class="comment"># tensor has a value of 1.0 for every real prediction and 0.0 for the</span></span><br><span class="line">    <span class="comment"># padding predictions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq]ï¼Œè¿™ä¸ªç›¸å½“äºæŠŠmaskçš„éƒ¨åˆ†ç»™æŠ½å‡ºæ¥,minimize (-log MLE)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># scalarï¼Œä¸€ä¸ªbatchçš„lossï¼Œè¿™ä¸ªç›¸å½“äºæŠŠpaddingçš„éƒ¨åˆ†ç»™å»é™¤äº†</span></span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    <span class="comment"># scalar</span></span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + <span class="number">1e-5</span></span><br><span class="line">    loss = numerator / denominator <span class="comment"># å¹³å‡loss</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å®šä¹‰NSPä»»åŠ¡</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_sentence_output</span><span class="params">(bert_config, input_tensor, labels)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the next sentence prediction."""</span></span><br><span class="line"></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size,hidden_size]</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Simple binary classification. Note that 0 is "next sentence" and 1 is</span></span><br><span class="line">  <span class="comment"># "random sentence". This weight matrix is not used after pre-training.</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/seq_relationship"</span>):</span><br><span class="line">    <span class="comment"># [2,hidden_size]</span></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">"output_weights"</span>,</span><br><span class="line">        shape=[<span class="number">2</span>, bert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(bert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>, shape=[<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size,2]</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># [batch_size,2]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># minimize (-log MLE)</span></span><br><span class="line">    <span class="comment"># [batch_size]</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># scalar</span></span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gather_indexes</span><span class="params">(sequence_tensor, positions)</span>:</span></span><br><span class="line">  <span class="string">"""Gathers the vectors at the specific positions over a minibatch."""</span></span><br><span class="line">  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = sequence_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = sequence_shape[<span class="number">1</span>]</span><br><span class="line">  width = sequence_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  flat_offsets = tf.reshape(</span><br><span class="line">      tf.range(<span class="number">0</span>, batch_size, dtype=tf.int32) * seq_length, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">  flat_positions = tf.reshape(positions + flat_offsets, [<span class="number">-1</span>])</span><br><span class="line">  flat_sequence_tensor = tf.reshape(sequence_tensor,</span><br><span class="line">                                    [batch_size * seq_length, width])</span><br><span class="line">  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)</span><br><span class="line">  <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure>
<p>å¯¹äºMLMä»»åŠ¡ï¼Œè¾“å…¥çš„æ˜¯æœ€é¡¶å±‚self-attentionå±‚çš„è¾“å‡ºç»“æœï¼Œç»´åº¦æ˜¯ï¼š<code>[batch_size,seq_length,hidden_size]</code>ï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨MLMä¸­ï¼Œæˆ‘ä»¬å…¶å®åªè®¡ç®—è¢«éšæœºMASKæ‰çš„tokençš„lossï¼Œè¿™ä¹Ÿæ˜¯BERTéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ä»¥åŠæ”¶æ•›æ…¢çš„åŸå› ï¼Œæˆ‘ä»¬æœ€ç»ˆéœ€è¦è®¡ç®—çš„æ˜¯ï¼š<code>[batch_size*max_pred_pre_seq,hidden_size]</code>ï¼Œå…¶ä¸­<code>max_pred_pre_seq</code>è¡¨ç¤ºä¸€ä¸ªå¥å­æœ€å¤šè¢«MASKçš„æ•°ç›®ã€‚å¯¹äºMLM lossçš„è®¡ç®—ï¼Œæˆ‘ä»¬æ˜¯<font face="times new roman"><strong><em>minimize (-log MLE)</em></strong></font>ã€‚</p>
<p>å¯¹äºNSPä»»åŠ¡ï¼Œè¾“å…¥çš„æ˜¯æœ€é¡¶å±‚çš„[CLS]tokençš„tensorï¼Œç»´åº¦æ˜¯ï¼š<code>[batch_size,hidden_size]</code>ï¼Œç„¶åä½¿ç”¨softmaxè¿›è¡ŒäºŒåˆ†ç±»ã€‚å…³äºNSP lossï¼Œä»ç„¶ä¸MLM lossä¸€æ ·ï¼Œæ˜¯<font face="times new roman"><strong><em>minimize (-log MLE)</em></strong></font>ã€‚</p>
<p>å®šä¹‰å®Œä¸¤ä¸ªä»»åŠ¡ä¹‹åï¼Œæˆ‘ä»¬å°±éœ€è¦å®šä¹‰æ¨¡å‹ï¼Œæ¥å®Œæˆè®­ç»ƒè¿‡ç¨‹ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å®šä¹‰æ¨¡å‹</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è¾“å…¥éƒ¨åˆ†</span></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>] <span class="comment"># paddingåçš„tokens </span></span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>] <span class="comment"># å¯¹paddingåçš„tokensè¿›è¡Œmask</span></span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>] <span class="comment"># segment idï¼Œç”¨æ¥åŒºåˆ†ä¸¤ä¸ªå¥å­</span></span><br><span class="line">    masked_lm_positions = features[<span class="string">"masked_lm_positions"</span>] <span class="comment"># è¢«éšæœºMASKæ‰çš„tokenåœ¨å¥å­ä¸­çš„ä½ç½®</span></span><br><span class="line">    masked_lm_ids = features[<span class="string">"masked_lm_ids"</span>] <span class="comment"># è¢«éšæœºMASKæ‰çš„tokenåœ¨è¯æ±‡è¡¨ä¸­çš„ç¼–ç </span></span><br><span class="line">    masked_lm_weights = features[<span class="string">"masked_lm_weights"</span>] <span class="comment"># åœ¨è¢«éšæœºMASKæ‰çš„tokenä¸­ï¼Œå¦‚æœtokenæ˜¯åŸæœ¬çš„ï¼Œé‚£ä¹ˆä¸º1ï¼Œå¦‚æœMASKçš„æ˜¯paddingï¼Œé‚£ä¹ˆä¸º0</span></span><br><span class="line">    next_sentence_labels = features[<span class="string">"next_sentence_labels"</span>] <span class="comment"># æ˜¯å¦ä¸ºä¸‹ä¸€å¥ï¼Œæ˜¯ä¸º1ï¼Œå¦åˆ™ä¸º0</span></span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å®ä¾‹åŒ–BERTæ¨¡å‹</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å¾—MLMä»»åŠ¡çš„å¹³å‡æŸå¤±(scalar)ï¼ŒbatchæŸå¤±ï¼ˆ[batch_size]ï¼‰ä»¥åŠé¢„æµ‹æ¦‚ç‡çŸ©é˜µ([batch_size*max_pred_pre_seq,vocab_size])</span></span><br><span class="line">    (masked_lm_loss,</span><br><span class="line">     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">         bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">         masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å¾—NSPä»»åŠ¡çš„å¹³å‡æŸå¤±(scalar)ï¼ŒbatchæŸå¤±([batch_size])ä»¥åŠé¢„æµ‹æ¦‚ç‡çŸ©é˜µ([batch_size,2])</span></span><br><span class="line">    (next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">     next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">         bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ€»çš„æŸå¤±ä¸ºä¸¤è€…ç›¸åŠ </span></span><br><span class="line">    total_loss = masked_lm_loss + next_sentence_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ¨¡å‹æ€»çš„å¯è®­ç»ƒå‚æ•°</span></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># å¦‚æœæœ‰ä¹‹å‰ä¿å­˜çš„æ¨¡å‹ï¼Œåˆ™è¿›è¡Œæ¢å¤</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                    masked_lm_weights, next_sentence_example_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">                    next_sentence_log_probs, next_sentence_labels)</span>:</span></span><br><span class="line">        <span class="string">"""Computes the loss and accuracy of the model."""</span></span><br><span class="line">        <span class="comment"># è®¡ç®—æŸå¤±ä¸å‡†ç¡®ç‡</span></span><br><span class="line">        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,</span><br><span class="line">                                         [<span class="number">-1</span>, masked_lm_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        <span class="comment"># [batch_size,max_pred_pre_seq]</span></span><br><span class="line">        masked_lm_predictions = tf.argmax(</span><br><span class="line">            masked_lm_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_ids = tf.reshape(masked_lm_ids, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_weights = tf.reshape(masked_lm_weights, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># è®¡ç®—åœ¨æ¯ä¸€ä¸ªä½ç½®ä¸Šçš„å‡†ç¡®ç‡</span></span><br><span class="line">        masked_lm_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=masked_lm_ids,</span><br><span class="line">            predictions=masked_lm_predictions,</span><br><span class="line">            weights=masked_lm_weights)</span><br><span class="line">        masked_lm_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=masked_lm_example_loss, weights=masked_lm_weights)</span><br><span class="line"></span><br><span class="line">        next_sentence_log_probs = tf.reshape(</span><br><span class="line">            next_sentence_log_probs, [<span class="number">-1</span>, next_sentence_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        next_sentence_predictions = tf.argmax(</span><br><span class="line">            next_sentence_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        next_sentence_labels = tf.reshape(next_sentence_labels, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># è®¡ç®—åœ¨NSPä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡</span></span><br><span class="line">        next_sentence_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=next_sentence_labels, predictions=next_sentence_predictions)</span><br><span class="line">        next_sentence_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=next_sentence_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"masked_lm_accuracy"</span>: masked_lm_accuracy,</span><br><span class="line">            <span class="string">"masked_lm_loss"</span>: masked_lm_mean_loss,</span><br><span class="line">            <span class="string">"next_sentence_accuracy"</span>: next_sentence_accuracy,</span><br><span class="line">            <span class="string">"next_sentence_loss"</span>: next_sentence_mean_loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn, [</span><br><span class="line">          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span><br><span class="line">          masked_lm_weights, next_sentence_example_loss,</span><br><span class="line">          next_sentence_log_probs, next_sentence_labels</span><br><span class="line">      ])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"Only TRAIN and EVAL modes are supported: %s"</span> % (mode))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure>
<h3 id="run-classifier-py"><a href="#run-classifier-py" class="headerlink" title="run_classifier.py"></a>run_classifier.py</h3><p>å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨BERTåœ¨è‡ªå·±çš„ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹çš„å°±æ˜¯run_classifier.pyæ–‡ä»¶ã€‚å…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<p>é¦–å…ˆæ˜¯å¯¹æ•°æ®å¤„ç†ï¼Œå¯¹äºBERTæ¥è¯´ï¼ŒGoogleå®˜æ–¹å®ç°çš„ä»£ç ä¸­ï¼Œè¾“å…¥æ˜¯from_tensor_slicesï¼Œæ‰€ä»¥é¦–å…ˆéœ€è¦å¯¹æ•°æ®é›†è¿›è¡Œå¤„ç†ï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataProcessor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Base class for data converters for sequence classification data sets."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_train_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the train set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_dev_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the dev set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_test_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for prediction."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_labels</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets the list of labels for this data set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬åªéœ€è¦ç»§æ‰¿è¿™ä¸ªç±»ï¼Œé‡å†™è¿™å‡ ä¸ªå‡½æ•°å³å¯ã€‚æ•°æ®å¤„ç†å®Œä¹‹åï¼Œæ¥çœ‹çœ‹æ€ä¹ˆæ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»¥åˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(bert_config, is_training, input_ids, input_mask, segment_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                 labels, num_labels, use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Creates a classification model."""</span></span><br><span class="line">  model = modeling.BertModel(</span><br><span class="line">      config=bert_config,</span><br><span class="line">      is_training=is_training,</span><br><span class="line">      input_ids=input_ids,</span><br><span class="line">      input_mask=input_mask,</span><br><span class="line">      token_type_ids=segment_ids,</span><br><span class="line">      use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># In the demo, we are doing a simple classification task on the entire</span></span><br><span class="line">  <span class="comment"># segment.</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If you want to use the token-level output, use model.get_sequence_output()</span></span><br><span class="line">  <span class="comment"># instead.</span></span><br><span class="line">  <span class="comment"># å¦‚æœæ˜¯ç”¨äºåˆ†ç±»çš„è¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ˜¯ä½¿ç”¨æœ€åä¸€å±‚çš„[CLS]çš„å‘é‡ï¼Œç»´åº¦æ˜¯ï¼š[batch_size,hidden_size]</span></span><br><span class="line">  output_layer = model.get_pooled_output()</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># è·å–hidden_size</span></span><br><span class="line">  hidden_size = output_layer.shape[<span class="number">-1</span>].value</span><br><span class="line"></span><br><span class="line">  output_weights = tf.get_variable(</span><br><span class="line">      <span class="string">"output_weights"</span>, [num_labels, hidden_size],</span><br><span class="line">      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">  output_bias = tf.get_variable(</span><br><span class="line">      <span class="string">"output_bias"</span>, [num_labels], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"loss"</span>):</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      <span class="comment"># å¦‚æœæ˜¯è®­ç»ƒçš„è¯ï¼Œé‚£å°±åŠ dropoutï¼</span></span><br><span class="line">      <span class="comment"># I.e., 0.1 dropout</span></span><br><span class="line">      output_layer = tf.nn.dropout(output_layer, keep_prob=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># çº¿æ€§å˜æ¢ï¼Œç»´åº¦ä¸ºï¼š[batch_size,num_labels]</span></span><br><span class="line">    logits = tf.matmul(output_layer, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    <span class="comment"># softmax</span></span><br><span class="line">    probabilities = tf.nn.softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># log_softmax,[batch_size,num_labels]</span></span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size,num_labels]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># minimize (-log MLE)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, logits, probabilities)</span><br></pre></td></tr></table></figure>
<p>æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¾—åˆ°æ•´ä¸ªæ¨¡å‹çš„lossä¹‹åï¼Œç„¶åå°±å¼€å§‹finetuneï¼Œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, num_labels, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>]</span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>]</span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>]</span><br><span class="line">    label_ids = features[<span class="string">"label_ids"</span>]</span><br><span class="line">    is_real_example = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"is_real_example"</span> <span class="keyword">in</span> features:</span><br><span class="line">      is_real_example = tf.cast(features[<span class="string">"is_real_example"</span>], dtype=tf.float32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆ›å»ºæ¨¡å‹ï¼Œå¾—åˆ°lossï¼Œbatch lossï¼Œlogitsä»¥åŠæ¦‚ç‡é¢„æµ‹çŸ©é˜µ</span></span><br><span class="line">    (total_loss, per_example_loss, logits, probabilities) = create_model(</span><br><span class="line">        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,</span><br><span class="line">        num_labels, use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># è®­ç»ƒæ¨¡å¼</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      </span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="comment"># è¯„ä¼°æ¨¡å¼</span></span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(per_example_loss, label_ids, logits, is_real_example)</span>:</span></span><br><span class="line">        predictions = tf.argmax(logits, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=label_ids, predictions=predictions, weights=is_real_example)</span><br><span class="line">        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"eval_accuracy"</span>: accuracy,</span><br><span class="line">            <span class="string">"eval_loss"</span>: loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn,</span><br><span class="line">                      [per_example_loss, label_ids, logits, is_real_example])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="comment"># æµ‹è¯•æ¨¡å¼ï¼Œé¢„æµ‹</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          predictions=&#123;<span class="string">"probabilities"</span>: probabilities&#125;,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure>
<p>ä¹‹åè¿è¡Œmainå‡½æ•°å°±å¯ä»¥å®Œæˆæ•´ä¸ªè®­ç»ƒäº†ã€‚</p>
<p>æ•´ä¸ªBERTæ¨¡å‹çš„ä»£ç å°±çœ‹å®Œå•¦ï¼Œè¯»ä¸‹æ¥çš„æ„Ÿå—å°±æ˜¯ï¼šéå¸¸çš„èˆ’çˆ½ï¼Œä¸å¾—ä¸è¯´ï¼ŒGoogleå†™çš„ä»£ç è´¨é‡è¿˜æ˜¯éå¸¸å¥½çš„ï¼Œè™½ç„¶æˆ‘æ“…é•¿çš„æ˜¯tensorflow2.xï¼Œä½†æ˜¯tensorflow1.xçš„ä»£ç è¯»èµ·æ¥è¿˜æ˜¯æ²¡æœ‰ä»€ä¹ˆéšœç¢çš„ã€‚ä¹‹åæœ€å¥½åœ¨è‡ªå·±çš„ä»»åŠ¡ä¸Šä½¿ç”¨BERTæ¥çœ‹çœ‹æ•ˆæœï½é™¤æ­¤ä¹‹å¤–ï¼Œå¦‚éå¿…è¦ï¼Œä»¥åå…³äºå„ç§BERTçš„å˜ä½“æ¨¡å‹å°±ä¸ä¼šè§£è¯»å®ƒçš„æºä»£ç äº†ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯ç…§æ¬BERTçš„æºç å®ç°ï¼Œç„¶ååœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸Šæˆ–è€…BERTæ¨¡å‹ä¸»ä½“éƒ¨åˆ†çš„å®ç°ä¸Šè¿›è¡Œä¸€äº›æ”¹åŠ¨ï¼Œæ•´ä½“ä¸Šå¤§åŒå°å¼‚ï½</p>
<p>overï½â˜•ï¸</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>Would you like to buy me a cup of coffeeâ˜•ï¸ï½</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="zichao WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="zichao Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/BERT/" rel="tag"># BERT</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/02/NLP-former%E6%A8%A1%E5%9E%8B/" rel="prev" title="NLP|{}formeræ¨¡å‹">
      <i class="fa fa-chevron-left"></i> NLP|{}formeræ¨¡å‹
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/11/Pytorch%E7%9A%84%E9%9B%B6%E7%A2%8E%E7%AC%94%E8%AE%B0/" rel="next" title="Pytorchçš„é›¶ç¢ç¬”è®°">
      Pytorchçš„é›¶ç¢ç¬”è®° <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERTæ•´ä½“ä»£ç ç»“æ„"><span class="nav-number">1.</span> <span class="nav-text">BERTæ•´ä½“ä»£ç ç»“æ„</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#æ ¸å¿ƒä»£ç æ–‡ä»¶èµ°è¯»"><span class="nav-number">2.</span> <span class="nav-text">æ ¸å¿ƒä»£ç æ–‡ä»¶èµ°è¯»</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#modeling-py"><span class="nav-number">2.1.</span> <span class="nav-text">modeling.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tokenization-py"><span class="nav-number">2.2.</span> <span class="nav-text">tokenization.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#create-pretraining-data-py"><span class="nav-number">2.3.</span> <span class="nav-text">create_pretraining_data.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#run-pretraining-py"><span class="nav-number">2.4.</span> <span class="nav-text">run_pretraining.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#run-classifier-py"><span class="nav-number">2.5.</span> <span class="nav-text">run_classifier.py</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="zichao"
      src="/images/photo.jpg">
  <p class="site-author-name" itemprop="name">zichao</p>
  <div class="site-description" itemprop="description">Just learning</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">86</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/codewithzichao" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;codewithzichao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lizichao@pku.edu.cn" title="E-Mail â†’ mailto:lizichao@pku.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/codewithzichao" title="Weibo â†’ https:&#x2F;&#x2F;weibo.com&#x2F;codewithzichao" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zichao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme â€“ <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  
















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":225,"height":450},"mobile":{"show":false}});</script></body>
</html>
