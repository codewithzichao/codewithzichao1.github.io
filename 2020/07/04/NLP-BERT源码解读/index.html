<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://codewithzichao.github.io').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="ÊúÄËøë‰∏ÄÁõ¥Âú®ÁúãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÂèëÁé∞Â§ßÈÉ®ÂàÜÊ®°ÂûãÁöÑÊ∫ê‰ª£Á†ÅÂü∫Êú¨‰∏äÈÉΩÊòØÂú®GoogleÂÆòÊñπÂèëÂ∏ÉÁöÑBERTÊ∫êÁ†ÅÁöÑÂü∫Á°Ä‰∏äËøõË°å‰øÆÊîπÁöÑ(‰ΩÜÊòØÂÖ®ÈÉΩÊòØTF1.xüò∑ÔºåËøôÁÇπÊàëË¶ÅÂêêÊßΩ‰∫ÜÔºåÊåâÈÅìÁêÜTF2.xÂá∫Êù•‰πãÂêéÔºåGoogleÂú®Â§ßÂäõÊé®ÂπøTF2.xÔºåÁÑ∂ËÄåËøûGoogleËá™Â∑±ÂèëÂ∏ÉÁöÑELECTRA„ÄÅAdapter-BERT„ÄÅALBERTÁ≠âÁ≠âÊ∫ê‰ª£Á†ÅÈÉΩÊòØimport tensorflow.compat.v1 as tfüò∑Ôºåexcuse me">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP|BERTÊ∫êÁ†ÅËß£ËØª">
<meta property="og:url" content="http://codewithzichao.github.io/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/index.html">
<meta property="og:site_name" content="codewithzichao">
<meta property="og:description" content="ÊúÄËøë‰∏ÄÁõ¥Âú®ÁúãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÂèëÁé∞Â§ßÈÉ®ÂàÜÊ®°ÂûãÁöÑÊ∫ê‰ª£Á†ÅÂü∫Êú¨‰∏äÈÉΩÊòØÂú®GoogleÂÆòÊñπÂèëÂ∏ÉÁöÑBERTÊ∫êÁ†ÅÁöÑÂü∫Á°Ä‰∏äËøõË°å‰øÆÊîπÁöÑ(‰ΩÜÊòØÂÖ®ÈÉΩÊòØTF1.xüò∑ÔºåËøôÁÇπÊàëË¶ÅÂêêÊßΩ‰∫ÜÔºåÊåâÈÅìÁêÜTF2.xÂá∫Êù•‰πãÂêéÔºåGoogleÂú®Â§ßÂäõÊé®ÂπøTF2.xÔºåÁÑ∂ËÄåËøûGoogleËá™Â∑±ÂèëÂ∏ÉÁöÑELECTRA„ÄÅAdapter-BERT„ÄÅALBERTÁ≠âÁ≠âÊ∫ê‰ª£Á†ÅÈÉΩÊòØimport tensorflow.compat.v1 as tfüò∑Ôºåexcuse me">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-04T08:31:43.000Z">
<meta property="article:modified_time" content="2020-07-05T01:10:13.914Z">
<meta property="article:author" content="zichao">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://codewithzichao.github.io/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>NLP|BERTÊ∫êÁ†ÅËß£ËØª | codewithzichao</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">codewithzichao</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">ConfidentÔºåModestÔºåPatient</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/Friends/" rel="section"><i class="fa fa-fw fa-address-book"></i>Friends</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/codewithzichao" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://codewithzichao.github.io/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/photo.jpg">
      <meta itemprop="name" content="zichao">
      <meta itemprop="description" content="Just learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="codewithzichao">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP|BERTÊ∫êÁ†ÅËß£ËØª
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">

            

              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-04 16:31:43" itemprop="dateCreated datePublished" datetime="2020-07-04T16:31:43+08:00">2020-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-05 09:10:13" itemprop="dateModified" datetime="2020-07-05T09:10:13+08:00">2020-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>ÊúÄËøë‰∏ÄÁõ¥Âú®ÁúãÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÂèëÁé∞Â§ßÈÉ®ÂàÜÊ®°ÂûãÁöÑÊ∫ê‰ª£Á†ÅÂü∫Êú¨‰∏äÈÉΩÊòØÂú®GoogleÂÆòÊñπÂèëÂ∏ÉÁöÑBERTÊ∫êÁ†ÅÁöÑÂü∫Á°Ä‰∏äËøõË°å‰øÆÊîπÁöÑ(‰ΩÜÊòØÂÖ®ÈÉΩÊòØTF1.xüò∑ÔºåËøôÁÇπÊàëË¶ÅÂêêÊßΩ‰∫ÜÔºåÊåâÈÅìÁêÜTF2.xÂá∫Êù•‰πãÂêéÔºåGoogleÂú®Â§ßÂäõÊé®ÂπøTF2.xÔºåÁÑ∂ËÄåËøûGoogleËá™Â∑±ÂèëÂ∏ÉÁöÑELECTRA„ÄÅAdapter-BERT„ÄÅALBERTÁ≠âÁ≠âÊ∫ê‰ª£Á†ÅÈÉΩÊòØimport tensorflow.compat.v1 as tfüò∑Ôºåexcuse meÔºü)„ÄÇÊâÄ‰ª•ËøòÊòØÂõûÂ§¥ÂÜç‰ªîÁªÜÁúã‰∫Ü‰∏ÄÈÅçÂéüÊù•BERTÁöÑÊ∫ê‰ª£Á†Å„ÄÇ‰∏çËøáÔºåÊï¥‰ΩìÈòÖËØª‰∏ãÊù•ÔºåÊÑüËßâËøòÊòØÈùûÂ∏∏È°∫ÁïÖÁöÑÔºå‰∏çÂæó‰∏çËØ¥‰ª£Á†ÅÂÜôÁöÑÁúüÁöÑÂ•Ω„ÄÇÊâÄ‰ª•ËøôÁØáÊñáÁ´†‰∏ªË¶ÅÊòØËÆ∞ÂΩï‰∏Ä‰∏ãËá™Â∑±ÁúãBERTÊ∫ê‰ª£Á†ÅÁöÑËøáÁ®ã„ÄÇ</p>
<a id="more"></a>
<h2 id="BERTÊï¥‰Ωì‰ª£Á†ÅÁªìÊûÑ"><a href="#BERTÊï¥‰Ωì‰ª£Á†ÅÁªìÊûÑ" class="headerlink" title="BERTÊï¥‰Ωì‰ª£Á†ÅÁªìÊûÑ"></a>BERTÊï¥‰Ωì‰ª£Á†ÅÁªìÊûÑ</h2><p>BERTÂéüÁêÜÊàëÂú®ËøôÈáåÂ∞±‰∏çÂ§öÂï∞Âó¶‰∫ÜÔºåÁΩë‰∏ä‰∏ÄÂ§ßÂ†ÜÔºåÂΩìÁÑ∂Êõ¥Âä†Êé®ËçêÁöÑÊòØÂèñÁúãÂéüÂßãËÆ∫Êñá„ÄÇÈ¶ñÂÖàÊù•ÁúãÁúãBERTÁöÑ‰ª£Á†ÅÊñá‰ª∂‰∏éÊï¥‰ΩìÁªìÊûÑ„ÄÇÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">‚îú‚îÄ‚îÄ README.md</span><br><span class="line">‚îú‚îÄ‚îÄ create_pretraining_data.py</span><br><span class="line">‚îú‚îÄ‚îÄ extract_features.py</span><br><span class="line">‚îú‚îÄ‚îÄ modeling.py</span><br><span class="line">‚îú‚îÄ‚îÄ modeling_test.py</span><br><span class="line">‚îú‚îÄ‚îÄ multilingual.md</span><br><span class="line">‚îú‚îÄ‚îÄ optimization.py</span><br><span class="line">‚îú‚îÄ‚îÄ optimization_test.py</span><br><span class="line">‚îú‚îÄ‚îÄ predicting_movie_reviews_with_bert_on_tf_hub.ipynb</span><br><span class="line">‚îú‚îÄ‚îÄ requirements.txt</span><br><span class="line">‚îú‚îÄ‚îÄ run_classifier.py</span><br><span class="line">‚îú‚îÄ‚îÄ run_classifier_with_tfhub.py</span><br><span class="line">‚îú‚îÄ‚îÄ run_pretraining.py</span><br><span class="line">‚îú‚îÄ‚îÄ run_squad.py</span><br><span class="line">‚îú‚îÄ‚îÄ sample_text.txt</span><br><span class="line">‚îú‚îÄ‚îÄ tokenization.py</span><br><span class="line">‚îî‚îÄ‚îÄ tokenization_test.py</span><br></pre></td></tr></table></figure>
<ul>
<li>create_pretraining_data.pyÔºöÁî®Êù•ÂàõÂª∫ËÆ≠ÁªÉÂÆû‰æãÔºõ</li>
<li>extract_features.pyÔºöÊèêÂèñÂá∫È¢ÑËÆ≠ÁªÉÁöÑÁâπÂæÅÔºõ</li>
<li>modeling.pyÔºöBERTÁöÑÊ†∏ÂøÉÂª∫Ê®°Êñá‰ª∂ÔºåÊ®°Âûã‰∏ª‰ΩìÈÉ®ÂàÜÔºõ</li>
<li>modeling_test.pyÔºöÂØπmodeling.pyÊñá‰ª∂ËøõË°åunittestÊµãËØïÔºõ</li>
<li>optimization.pyÔºöËá™ÂÆö‰πâÁöÑ‰ºòÂåñÂô®Ôºõ</li>
<li>optimization_test.pyÔºöÂØπoptimization.pyÊñá‰ª∂ÁöÑunittestÊµãËØïÔºõ</li>
<li>predicting_movie_reviews_with_bert_on_tf_hub.ipynbÔºöÈÄöËøáË∞ÉÁî®tfhubÊù•‰ΩøÁî®BERTËøõË°åÈ¢ÑÊµãÔºõ</li>
<li>run_classifier.pyÔºöÂú®Â§öÁßçÊï∞ÊçÆÈõÜ‰∏ä(Ë≠¨Â¶ÇÔºöMRPC„ÄÅXNLI„ÄÅMNLI„ÄÅCOLA)Êù•ËøõË°åBERTÊ®°ÂûãÁöÑfinetuneÔºõ</li>
<li>run_classifier_with_tfhub.pyÔºöÈÄöËøáË∞ÉÁî®tfhubÊù•ËøõË°åfinetuneÔºõ</li>
<li>run_pretraining.pyÔºöÈÄöËøáMLM‰∏éNSP‰ªªÂä°Êù•ÂØπÊ®°ÂûãËøõË°åÈ¢ÑËÆ≠ÁªÉÔºõ</li>
<li>run_squad.pyÔºöÂú®squadÊï∞ÊçÆÈõÜ‰∏äËøõË°åfinetuneÔºõ</li>
<li>tokenization.pyÔºöÂØπÂéüÂßãÊï∞ÊçÆËøõË°åÊ∏ÖÊ¥ó„ÄÅÂàÜËØçÁ≠âÊìç‰ΩúÔºõ</li>
<li>tokenization_test.pyÔºöÂØπ tokenization.pyÊñá‰ª∂ËøõË°åunittestÊµãËØï„ÄÇ</li>
</ul>
<p>‰ª•‰∏äÂ∞±ÊòØÂêÑÊñá‰ª∂ÁöÑÂ§ßËá¥ÁÆÄ‰ªãÔºå‰∏ãÈù¢Â∞ÜÂØπÊ†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂ËøõË°åËµ∞ËØªÔºåtnesorÁöÑÁª¥Â∫¶‰ª•ÂèäÈáçË¶ÅÊ≥®ÈáäÊàëÂùáÂ∑≤Âú®‰ª£Á†ÅÈáåÂÜôÊòéÔΩû</p>
<h2 id="Ê†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂Ëµ∞ËØª"><a href="#Ê†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂Ëµ∞ËØª" class="headerlink" title="Ê†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂Ëµ∞ËØª"></a>Ê†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂Ëµ∞ËØª</h2><h3 id="modeling-py"><a href="#modeling-py" class="headerlink" title="modeling.py"></a>modeling.py</h3><p>modeling.pyÊñá‰ª∂ÊòØBERTÊ®°ÂûãÁöÑÂÆûÁé∞„ÄÇÈ¶ñÂÖàÊù•ÁúãBertConfigÁ±ªÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Configuration for `BertModel`."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_act=<span class="string">"gelu"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               initializer_range=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs BertConfig.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.</span></span><br><span class="line"><span class="string">      hidden_size: Size of the encoder layers and the pooler layer.</span></span><br><span class="line"><span class="string">      num_hidden_layers: Number of hidden layers in the Transformer encoder.</span></span><br><span class="line"><span class="string">      num_attention_heads: Number of attention heads for each attention layer in</span></span><br><span class="line"><span class="string">        the Transformer encoder.</span></span><br><span class="line"><span class="string">      intermediate_size: The size of the "intermediate" (i.e., feed-forward)</span></span><br><span class="line"><span class="string">        layer in the Transformer encoder.</span></span><br><span class="line"><span class="string">      hidden_act: The non-linear activation function (function or string) in the</span></span><br><span class="line"><span class="string">        encoder and pooler.</span></span><br><span class="line"><span class="string">      hidden_dropout_prob: The dropout probability for all fully connected</span></span><br><span class="line"><span class="string">        layers in the embeddings, encoder, and pooler.</span></span><br><span class="line"><span class="string">      attention_probs_dropout_prob: The dropout ratio for the attention</span></span><br><span class="line"><span class="string">        probabilities.</span></span><br><span class="line"><span class="string">      max_position_embeddings: The maximum sequence length that this model might</span></span><br><span class="line"><span class="string">        ever be used with. Typically set this to something large just in case</span></span><br><span class="line"><span class="string">        (e.g., 512 or 1024 or 2048).</span></span><br><span class="line"><span class="string">      type_vocab_size: The vocabulary size of the `token_type_ids` passed into</span></span><br><span class="line"><span class="string">        `BertModel`.</span></span><br><span class="line"><span class="string">      initializer_range: The stdev of the truncated_normal_initializer for</span></span><br><span class="line"><span class="string">        initializing all weight matrices.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.vocab_size = vocab_size</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.num_hidden_layers = num_hidden_layers</span><br><span class="line">    self.num_attention_heads = num_attention_heads</span><br><span class="line">    self.hidden_act = hidden_act</span><br><span class="line">    self.intermediate_size = intermediate_size</span><br><span class="line">    self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">    self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">    self.max_position_embeddings = max_position_embeddings</span><br><span class="line">    self.type_vocab_size = type_vocab_size</span><br><span class="line">    self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_dict</span><span class="params">(cls, json_object)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a Python dictionary of parameters."""</span></span><br><span class="line">    config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">      config.__dict__[key] = value</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_json_file</span><span class="params">(cls, json_file)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a json file of parameters."""</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      text = reader.read()</span><br><span class="line">    <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a Python dictionary."""</span></span><br><span class="line">    output = copy.deepcopy(self.__dict__)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_json_string</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a JSON string."""</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">"\n"</span></span><br></pre></td></tr></table></figure>
<p>Ëøô‰∏™‰∏ªË¶ÅÊòØBERTÊ®°ÂûãÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÊ≥®ÈáäÂÖ∂ÂÆûÂæàËØ¶ÁªÜ‰∫ÜÔºå‰ΩÜÊòØÈúÄË¶ÅÁâπÂà´ËØ¥ÊòéÁöÑÊòØ<code>type_vocab_size</code>ÔºåËøô‰∏™Ë°®Á§∫ÁöÑÊòØsegment idÔºåÈªòËÆ§ÊòØ2Ôºå‰ª£Á†ÅÈáåÊ≤°Êúâ‰øÆÊîπÔºå‰ΩÜÊòØÂú®bert_config.jsonÊñá‰ª∂ÈáåÊúâÈùûÂ∏∏ËØ¶ÁªÜÁöÑËß£Èáä„ÄÇ</p>
<p>ÁúãÂÆåBertConfigÁ±ª‰πãÂêéÂ∞±ÊòØBERTÊ®°Âûã‰∫ÜÔºå‰ΩÜÊòØÁî±‰∫éBERTÊ®°ÂûãÊï¥‰ΩìÈùûÂ∏∏Â§çÊùÇÔºåÊàë‰ª¨ÂÖàÊù•ÁúãÁúãÂÆÉÁöÑÂÖ∂ÂÆûÁöÑcomponent„ÄÇÈ¶ñÂÖàÊù•Áúãtoken embeddingÈÉ®ÂàÜÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding_lookupÁî®Êù•Ëé∑Âèñtoken embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#ÈöèÊú∫ÂàùÂßãÂåñembedding table</span></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># flattenÔºåÁª¥Â∫¶Âèò‰∏∫Ôºö[batch_size*seq_length*input_num]</span></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    <span class="comment"># Âèò‰∏∫one-hotÂêëÈáèÔºåÁª¥Â∫¶ÊòØÔºö[batch_size*seq_length*input_num,vocab_size]</span></span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    <span class="comment"># Áª¥Â∫¶Ôºö[batch_size*seq_length*input_num,embedding_size]</span></span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Áª¥Â∫¶Ôºö[batch_size*seq_length*input_num,embedding_size]</span></span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Áª¥Â∫¶Ôºö[batch_size,seq_length,input_num*embedding_size]</span></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure>
<p>ÂÖ∂‰∏≠ÊØè‰∏™tensorÁöÑÁª¥Â∫¶ÊàëÈÉΩÊ†áÊ≥®ÁöÑÈùûÂ∏∏Ê∏ÖÊ•ö‰∫ÜÔºåÁúãÊáÇ‰ª£Á†ÅÂ∫îËØ•Ê≤°Êúâ‰ªÄ‰πàÈóÆÈ¢ò„ÄÇÈÄöËøá<code>embedding_lookup</code>ÂáΩÊï∞ÔºåÊàë‰ª¨Â∞±ÂæóÂà∞‰∫Ütoken embedding‰ª•Âèäembedding_tableÔºåÂÖ∂‰∏≠embedding_tableÂ∞±ÊòØËØçÂêëÈáèË°®ÔºåÂ¶ÇÊûúÊàë‰ª¨‰∏ç‰ΩøÁî®finetuneÁöÑÊñπÂºèÔºåÈÇ£‰πàÊàë‰ª¨‰πüÂèØ‰ª•Â∞ÜËÆ≠ÁªÉÂ•ΩÁöÑembedding_tableÁªôÊäΩÂá∫Êù•ÔºåÁÑ∂ÂêéÈááÁî®feasture basedÁöÑÊñπÂºèÊù•ËøõË°å‰∏ãÊ∏∏‰ªªÂä°ÁöÑËÆ≠ÁªÉ„ÄÇ</p>
<p>Èô§‰∫Ütoken embedding‰πãÂêéÔºåBERT‰∏≠ËøòÊúâsegment embedding‰∏épositional embeddingÔºåÊúÄÁªàÁöÑembeddingÊòØËøô‰∏â‰∏™embeddingÁõ∏Âä†ÂæóÂà∞ÁöÑÁªìÊûú„ÄÇÂÖ∑‰ΩìÂÆûÁé∞‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># embedding_postprocessorÁî®Êù•Â∞Ütoken embedding„ÄÅsegment embedding‰ª•Âèäpositional embeddingËøõË°åÁõ∏Âä†Ôºå</span></span><br><span class="line"><span class="comment"># Êù•ÂæóÂà∞ÊúÄÁªàËæìÂÖ•ÁöÑembedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_postprocessor</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_token_type=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,<span class="comment"># ‰∏ÄËà¨ÊòØ2</span></span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_position_embeddings=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                            position_embedding_name=<span class="string">"position_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            max_position_embeddings=<span class="number">512</span>, <span class="comment"># ÂøÖÈ°ªÂ§ß‰∫éÁ≠â‰∫éseq_length</span></span></span></span><br><span class="line"><span class="function"><span class="params">                            dropout_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Âä†ÂÖ•segment embedding</span></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"`token_type_ids` must be specified if"</span></span><br><span class="line">                       <span class="string">"`use_token_type` is True."</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># This vocab will be small so we always do one-hot here, since it is always</span></span><br><span class="line">    <span class="comment"># faster for a small vocabulary.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*seq_length]</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,token_type_vocab_size]</span></span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,width]</span></span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    <span class="comment"># [batch_size,seq_length,width]</span></span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Âä†ÂÖ•positional embedding</span></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># [seq_length,width]</span></span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, <span class="number">-1</span>])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line"></span><br><span class="line">      <span class="comment"># [1,seq_length,width]</span></span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      <span class="comment"># [batch_size,seq_length,width]</span></span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="comment"># ÁªèËøálayer norm‰ª•Âèädropout</span></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><strong>ËøôÈáåÈúÄË¶ÅÊ≥®ÊÑè‰∏Ä‰∏ãÔºåÂú®BERT‰∏≠Ôºåembedding_size=hidden_size„ÄÇ</strong>ÂæóÂà∞embedding‰πãÂêéÊàë‰ª¨Â∞±ÈúÄË¶ÅÂ∞ÜËæìÂÖ•ËæìÂÖ•Âà∞transformer‰∏≠‰∫ÜÔºåÂú®BERTÂΩì‰∏≠ÔºåtransformerÁî±12‰∏™self-attention layerÂ†ÜÂè†ËÄåÊàê„ÄÇÊâÄ‰ª•ÔºåÈ¶ñÂÖàÊù•ÁúãÁúãself-attention layerÁöÑÂÆûÁé∞ÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_return_2d_tensor=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(input_tensor, batch_size, num_attention_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                           seq_length, width)</span>:</span></span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ‰πãÊâÄ‰ª•ÈúÄË¶Å‰ΩøÁî®transposeÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂêéÈù¢Ë¶ÅÂÅöQK.TÔºåÂÖ∂ÂÆûÊàëËßâÂæó‰ΩøÁî®tf.einsumÂ∞±‰∏çÈúÄË¶ÅËΩ¨ÁΩÆ‰∫Ü</span></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  Êàë‰ª¨Êääfrom_tensorÁúã‰ΩúQÔºåto_tensorÁúã‰ΩúkÔºåvÔºÅ</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*from_seq_length,from_width]</span></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">   <span class="comment"># [batch_size*to_seq_length,to_width]</span></span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*from_seq_length,num_attention_heads*size_per_head]ÔºåÂç≥ËÆ∫ÊñáÈáåÊèêÂà∞ÁöÑÁ∫øÊÄßÂèòÊç¢</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">"query"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*to_seq_length,num_attention_heads*size_per_head]ÔºåÂç≥ËÆ∫ÊñáÈáåÊèêÂà∞ÁöÑÁ∫øÊÄßÂèòÊç¢</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">"key"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*to_seq_length,num_attention_heads*size_per_head]ÔºåÂç≥ËÆ∫ÊñáÈáåÊèêÂà∞ÁöÑÁ∫øÊÄßÂèòÊç¢</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">"value"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,size_per_head]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,to_seq_length,size_per_head]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,to_seq_length]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># ÊîæÁº©</span></span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Âú®softmax‰πãÂêéÔºåÂØπlogitsËøõË°åmaskÔºåÈò≤Ê≠¢Ë¢´paddingÁöÑ‰ΩçÁΩÆÁöÑÂÄº‰πüÂèÇ‰∏éËÆ°ÁÆóÔºÅ</span></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Âú®attention_mask‰∏≠Ôºå1Ë°®Á§∫ÁúüÂÆûÁöÑÈïøÂ∫¶Ôºå0Ë°®Á§∫ÊòØpaddingÁöÑ</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Êàë‰ª¨ËÆ©paddingÁöÑ‰ΩçÁΩÆÁöÑÂÄº‰∏∫Êó†Á©∑Â∞èÔºåËøôÊ†∑softmax‰πãÂêéÁöÑÂÄºÂ∞±‰ºöÊé•Ëøë‰∫é0ÔºÅ</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># # [batch_size,num_attention_heads,from_seq_length,to_seq_length]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,to_seq_length,num_attention_heads,size_per_head]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,to_seq_length,size_per_head]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,size_per_head]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,from_seq_length,num_attention_heads,to_seq_length]</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure>
<p>ËøôÊòØÊ†áÂáÜÁöÑself-attentionÂ±ÇÁöÑÂÆûÁé∞ÔºåÊàëËßâÂæóÊúâÂæàÂ§öÂú∞ÊñπÂèØ‰ª•ÂÄüÈâ¥ÔºåË≠¨Â¶ÇÔºöËÆ©paddingÁöÑ‰ΩçÁΩÆÁªèËøásoftmaxÁöÑÂÄºÊó†ÈôêÊé•Ëøë‰∫é0Ôºå‰ª•ÂèäÂú®ÂÅöQK.TÁöÑËÆ°ÁÆóÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨‰∏ÄÂºÄÂßãÂ∞±ÊääËæìÂÖ•ÁªôÂÆÉreshapeÂ±Ç2Áª¥ÁöÑÔºåÂç≥Ôºö<code>[batch_size,seq_length,width_size]</code>ËΩ¨ÂåñÂà∞<code>[batch_size*seq_length,num_attention_heads*size_per_head]</code>Ôºå‰ªéËÄåÂä†Âø´ËÆ≠ÁªÉÔºåËøô‰∏™ÂÖ∂ÂÆûÊàë‰πãÂâçÈÉΩÊ≤°ÊÉ≥ËøáÔºåÂè™ÊúâÂú®ÁúãÂºÄÊ∫ê‰ª£Á†ÅÁöÑÊó∂ÂÄôÊâçËÉΩÁü•ÈÅì„ÄÇÂÆö‰πâ‰∫Üsefl-attention layer‰πãÂêéÔºåÊàë‰ª¨Êù•ÁúãtransfomerÁöÑÂÆûÁé∞ÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,<span class="comment"># ÊúÄÁªàËæìÂá∫ÁªìÊûúÁöÑÁª¥Â∫¶</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,<span class="comment"># self-attention layerÁöÑÊï∞ÁõÆ</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>, <span class="comment"># ÊØè‰∏Ä‰∏™self-attention layer‰∏≠headÁöÑÊï∞ÁõÆ</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>, <span class="comment"># ‰∏≠Èó¥Áª¥Â∫¶ÔºåÂç≥FFNÁöÑÁª¥Â∫¶</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,<span class="comment"># FFNÁöÑÊøÄÊ¥ªÂáΩÊï∞</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span> <span class="comment"># ÊòØÂê¶ËøîÂõûÊâÄÊúâÂ±ÇÁöÑËæìÂá∫ÁªìÊûú</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*seq_length,input_width]</span></span><br><span class="line">  <span class="comment"># input_width=hidden_size</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># listÔºåÊÄªÂÖ±Êúânum_hidden_layersÔºåÊØè‰∏Ä‰∏™ÂÖÉÁ¥†ÁöÑÁª¥Â∫¶ÊòØÔºö[batch_size*seq_length,hidden_size]</span></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          <span class="comment"># [batch_size*seq_length,num_attention_heads*size_per_head]</span></span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`.</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          <span class="comment"># Áª¥Â∫¶ÂèòÊç¢ÔºåÂèò‰∏∫Ôºö[batch_size*seq_length,hidden]</span></span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          <span class="comment"># dropout</span></span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          <span class="comment"># ÊÆãÂ∑ÆËøûÊé•</span></span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        <span class="comment"># ‰ΩøÁî®geluÔºåÂèò‰∏∫Ôºö[batch_size*seq_length,intermediate_size]</span></span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        <span class="comment"># Êó†ÊøÄÊ¥ªÂáΩÊï∞ÔºåÁ∫ØÁ∫øÊÄßÂèòÊç¢ÔºåÂèò‰∏∫Ôºö[batch_size*seq_length,hidden_size]</span></span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># ÊÆãÂ∑ÆËøûÊé•</span></span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        <span class="comment"># Â≠òÂÇ®Ëøô‰∏ÄÂ±ÇÁöÑÁªìÊûú</span></span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>
<p>Áé∞Âú®ÔºåÊàë‰ª¨Â∞Üembedding‰ª•ÂèätransfomerÁªô‰∏≤Ëµ∑Êù•ÔºåÂæóÂà∞ÂÆåÊï¥ÁöÑBERTÊ®°Âûã„ÄÇÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""BERT model ("Bidirectional Encoder Representations from Transformers").</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  ```python</span></span><br><span class="line"><span class="string">  # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">  pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">  logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for BertModel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      <span class="comment"># Â¶ÇÊûú‰∏çÊòØËÆ≠ÁªÉÁöÑËØùÔºåÈÇ£‰πà‰∏çËÉΩ‰ΩøÁî®dropoutÔºÅ</span></span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        <span class="comment"># Ëé∑Âèñtoken embeddingÔºå‰ª•Âèäembedding table</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores.</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Â¶ÇÊûúdo_return_all_layers=TrueÔºåËøîÂõû‰∏Ä‰∏™listÔºåÂÖÉÁ¥†‰∏™Êï∞ÊòØnum_hidden_layers,</span></span><br><span class="line">        <span class="comment"># ÊØè‰∏Ä‰∏™ÂÖÉÁ¥†Áª¥Â∫¶Ôºö[batch_size,seq_length,hidden_size];</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Â¶ÇÊûúdo_return_all_layers=FalseÔºåÈÇ£‰πàÂ∞±ÂÄºËøîÂõûÊúÄÈ°∂Â±ÇÁöÑlayerÁöÑËæìÂá∫ÁªìÊûúÔºå</span></span><br><span class="line">        <span class="comment"># Áª¥Â∫¶ÊòØÔºö[batch_size,seq_length,hidden_size]</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># ÂèñÂà∞ÊúÄÈ°∂Â±ÇÁöÑÁªìÊûúÔºåÁª¥Â∫¶ÊòØÔºö[batch_size,seq_length,hidden_size]</span></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [batch_size,hidden_size],Áõ∏ÂΩì‰∫éÊòØpoolingÊìç‰ΩúÔºåÊàë‰ª¨Âè™Âèñ‰∏éÁ¨¨‰∏Ä‰∏™ÂÖÉÁ¥†Áõ∏ÂÖ≥ÁöÑhidden stateÔºÅ</span></span><br><span class="line">        <span class="comment"># ‰∏∫‰ªÄ‰πàË¶ÅÂèñ‰∏éÁ¨¨‰∏Ä‰∏™tokenÁõ∏ÂÖ≥ÁöÑhidden stateÔºü</span></span><br><span class="line">        <span class="comment"># Âõ†‰∏∫Âú®BERT‰∏≠ÔºåÁ¨¨‰∏Ä‰∏™tokenÊòØ[CLS]ÔºåÁî®‰∫éÂàÜÁ±ªÁöÑÔºÅËøô‰∏™ÂæàÈáçË¶ÅÔºÅ</span></span><br><span class="line"></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [batch_size,hidden_size]</span></span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_pooled_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.pooled_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_sequence_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets final hidden layer of encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the final hidden of the transformer encoder.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.sequence_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_all_encoder_layers</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets output of the embedding lookup (i.e., input to the transformer).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the output of the embedding layer, after summing the word</span></span><br><span class="line"><span class="string">      embeddings with the positional embeddings and the token type embeddings,</span></span><br><span class="line"><span class="string">      then performing layer normalization. This is the input to the transformer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_table</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_table</span><br></pre></td></tr></table></figure>
<p>Ëé∑ÂèñÊúÄÈ°∂Â±ÇÁöÑ[CLS]tokenÁöÑtensorÁî®‰∫éËÆ≠ÁªÉNSP‰ªªÂä°ÔºåÂ¶ÇÊûú‰∏ãÊ∏∏‰ªªÂä°ÊòØÂàÜÁ±ª‰ªªÂä°ÁöÑËØùÔºåÊàë‰ª¨ÊúÄÁªà‰πüÊòØÂú®Ëøô‰∏™ÁöÑÂü∫Á°Ä‰∏äÔºåÊù•‰ªãÂÖ•softmaxÊàñËÄÖÂÖ∂‰ªñÁöÑÁªìÊûÑÊù•ÂÅöÔºõÊ≠§Â§ñÔºåËé∑ÂèñÊúÄÈ°∂Â±Çself-ateention layerÁöÑËæìÂá∫ÁªìÊûúÔºåÁî®Êù•ËÆ≠ÁªÉMLM‰ªªÂä°„ÄÇ</p>
<h3 id="tokenization-py"><a href="#tokenization-py" class="headerlink" title="tokenization.py"></a>tokenization.py</h3><p>tokenizaton.pyÊñá‰ª∂Áî®Êù•ÂØπÂéüÂßãÊñáÊú¨ËøõË°åÂàÜËØç„ÄÅËØçÂπ≤Âåñ„ÄÅÂ∞èÂÜô„ÄÅÂéªÈô§Á©∫Ê†ºÁ≠âÁ≠âÊìç‰ΩúÔºåÂπ∂Â∞ÜÂéüÂßãÊñáÊú¨ÂêëÈáèÂåñ„ÄÇÂú®ËøôÈáåÔºåÈúÄË¶ÅÁâπÂà´Êèê‰∏Ä‰∏ãÁöÑËØùÂ∞±ÊòØÂàÜËØçËøôÂùóÔºåBERT‰ΩøÁî®‰∫Ü‰∏§ÁßçÂàÜËØçÔºåÈ¶ñÂÖàÂØπÂéüÂßãÊñáÊú¨ËøõË°åÁ≤óÁ≤íÂ∫¶ÁöÑÂàÜËØçÔºåÁÑ∂ÂêéÂú®Ê≠§Âü∫Á°Ä‰∏äÔºåËøõË°åwordpieceÂàÜËØçÔºåÂæóÂà∞Êõ¥Âä†ÁªÜÁ≤íÂ∫¶ÁöÑÂàÜËØçÁªìÊûú„ÄÇÂÖ∑‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<p>È¶ñÂÖàÊòØÁ≤óÁ≤íÂ∫¶ÁöÑÂàÜËØçÔºå‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs basic tokenization (punctuation splittingÊ†áÁÇπÁ¨¶Âè∑ÊãÜÂàÜ, lower casing, etc.)."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, do_lower_case=True)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      do_lower_case: Whether to lower case the input.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="string">"""Tokenizes a piece of text."""</span></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">    <span class="comment"># models. This is also applied to the English models now, but it doesn't</span></span><br><span class="line">    <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">    <span class="comment"># and generally don't have any Chinese data in them (there are Chinese</span></span><br><span class="line">    <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">    <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Â¢ûÂä†‰∏≠ÊñáÊîØÊåÅ</span></span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line"></span><br><span class="line">    orig_tokens = whitespace_tokenize(text)</span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">      <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line"></span><br><span class="line">    output_tokens = whitespace_tokenize(<span class="string">" "</span>.join(split_tokens))</span><br><span class="line">    <span class="comment">#ËøîÂõû‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤</span></span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<p>ÁÑ∂ÂêéÊòØwordpieceÂàÜËØçÔºåÂÖ∑‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">WordpieceTokenizerÊòØÂ∞ÜBasicTokenizerÁöÑÁªìÊûúËøõ‰∏ÄÊ≠•ÂÅöÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑÂàáÂàÜ„ÄÇ</span></span><br><span class="line"><span class="string">ÂÅöËøô‰∏ÄÊ≠•ÁöÑÁõÆÁöÑ‰∏ªË¶ÅÊòØ‰∏∫‰∫ÜÂéªÈô§Êú™ÁôªÂΩïËØçÂØπÊ®°ÂûãÊïàÊûúÁöÑÂΩ±Âìç„ÄÇ</span></span><br><span class="line"><span class="string">Ëøô‰∏ÄËøáÁ®ãÂØπ‰∏≠ÊñáÊ≤°ÊúâÂΩ±ÂìçÔºå</span></span><br><span class="line"><span class="string">Âõ†‰∏∫Âú®ÂâçÈù¢BasicTokenizerÈáåÈù¢Â∑≤ÁªèÂàáÂàÜÊàê‰ª•Â≠ó‰∏∫Âçï‰ΩçÁöÑ‰∫Ü„ÄÇ</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs WordPiece tokenziation."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token=<span class="string">"[UNK]"</span>, max_input_chars_per_word=<span class="number">200</span>)</span>:</span></span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word <span class="comment"># ÊØè‰∏Ä‰∏™tokenÁöÑÊúÄÂ§ßÂ≠óÁ¨¶Êï∞ÁõÆ</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="comment"># ‰ΩøÁî®Ë¥™ÂøÉÁöÑÊúÄÂ§ßÊ≠£ÂêëÂåπÈÖçÁÆóÊ≥ï</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Êàë‰ª¨Áî®‰∏Ä‰∏™‰æãÂ≠êÊù•Áúã‰ª£Á†ÅÁöÑÊâßË°åËøáÁ®ã„ÄÇÊØîÂ¶ÇÂÅáËÆæËæìÂÖ•ÊòØ‚Äùunaffable‚Äù„ÄÇ</span></span><br><span class="line"><span class="string">    Êàë‰ª¨Ë∑≥Âà∞whileÂæ™ÁéØÈÉ®ÂàÜÔºåËøôÊòØstart=0Ôºåend=len(chars)=9Ôºå‰πüÂ∞±ÊòØÂÖàÁúãÁúãunaffableÂú®‰∏çÂú®ËØçÂÖ∏ÈáåÔºå</span></span><br><span class="line"><span class="string">    Â¶ÇÊûúÂú®ÔºåÈÇ£‰πàÁõ¥Êé•‰Ωú‰∏∫‰∏Ä‰∏™WordPieceÔºå</span></span><br><span class="line"><span class="string">    Â¶ÇÊûú‰∏çÂÜçÔºåÈÇ£‰πàend-=1Ôºå‰πüÂ∞±ÊòØÁúãunaffablÂú®‰∏çÂú®ËØçÂÖ∏ÈáåÔºåÊúÄÁªàÂèëÁé∞‚Äùun‚ÄùÂú®ËØçÂÖ∏ÈáåÔºåÊääunÂä†Âà∞ÁªìÊûúÈáå„ÄÇ</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Êé•ÁùÄstart=2ÔºåÁúãaffableÂú®‰∏çÂú®Ôºå‰∏çÂú®ÂÜçÁúãaffablÔºå‚Ä¶Ôºå</span></span><br><span class="line"><span class="string">    ÊúÄÂêéÂèëÁé∞ ##aff Âú®ËØçÂÖ∏Èáå„ÄÇ</span></span><br><span class="line"><span class="string">    Ê≥®ÊÑèÔºö##Ë°®Á§∫Ëøô‰∏™ËØçÊòØÊé•ÁùÄÂâçÈù¢ÁöÑÔºåËøôÊ†∑‰ΩøÂæóWordPieceÂàáÂàÜÊòØÂèØÈÄÜÁöÑ‚Äî‚ÄîÊàë‰ª¨ÂèØ‰ª•ÊÅ¢Â§çÂá∫‚ÄúÁúüÊ≠£‚ÄùÁöÑËØç„ÄÇ</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">"""Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">    using the given vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string">      input = "unaffable"</span></span><br><span class="line"><span class="string">      output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">        already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">    output_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">      chars = list(token)</span><br><span class="line">      <span class="comment"># Â¶ÇÊûúÂ§ß‰∫éËÆæÁΩÆÁöÑÊØè‰∏Ä‰∏™tokenÁöÑÊúÄÂ§ßÂ≠óÁ¨¶Êï∞ÁõÆÁöÑËØùÔºåÈÇ£‰πàÂ∞±ÂΩì‰ΩúUNK</span></span><br><span class="line">      <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      is_bad = <span class="literal">False</span></span><br><span class="line">      start = <span class="number">0</span></span><br><span class="line">      sub_tokens = []</span><br><span class="line">      <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">        end = len(chars)</span><br><span class="line">        cur_substr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; end:</span><br><span class="line">          substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">          <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            substr = <span class="string">"##"</span> + substr</span><br><span class="line">          <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          is_bad = <span class="literal">True</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<p>Â∞ÜËøô‰∏§ÂàÜËØçËøõË°åÁªìÂêàÔºåÂæóÂà∞ÁªÜÁ≤íÂ∫¶ÁöÑÂàÜËØçÁªìÊûúÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs end-to-end tokenziation."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_file, do_lower_case=True)</span>:</span></span><br><span class="line">    self.vocab = load_vocab(vocab_file)</span><br><span class="line">    <span class="comment"># vË°®Á§∫indexÔºåkË°®Á§∫tokenÊú¨Ë∫´</span></span><br><span class="line">    self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="comment"># Ë∞ÉÁî®BasicTokenizerÁ≤óÁ≤íÂ∫¶ÂàÜËØç</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text):</span><br><span class="line">       <span class="comment"># Ë∞ÉÁî®WordpieceTokenizerÁªÜÁ≤íÂ∫¶ÂàÜËØç</span></span><br><span class="line">      <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">        split_tokens.append(sub_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure>
<h3 id="create-pretraining-data-py"><a href="#create-pretraining-data-py" class="headerlink" title="create_pretraining_data.py"></a>create_pretraining_data.py</h3><p>ËøôÈÉ®ÂàÜ‰∏ªË¶ÅÊòØÂú®tokenization.pyÁöÑÂü∫Á°Ä‰∏äÔºåÂàõÂª∫ËÆ≠ÁªÉÂÆû‰æã„ÄÇÊàë‰ª¨Êù•ÁúãBERT‰∏≠ÊòØÊÄé‰πàÂÆûÁé∞ÈöèÊú∫MASKÊìç‰ΩúÁöÑ„ÄÇ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span><span class="params">(tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates the predictions for the masked LM objective."""</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    <span class="comment"># [CLS]Âíå[SEP]‰∏çËÉΩÁî®‰∫éMASK</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">"[CLS]"</span> <span class="keyword">or</span> token == <span class="string">"[SEP]"</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> len(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">"##"</span>)):</span><br><span class="line">      cand_indexes[<span class="number">-1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  output_tokens = list(tokens)</span><br><span class="line"></span><br><span class="line">  num_to_predict = min(max_predictions_per_seq,</span><br><span class="line">                       max(<span class="number">1</span>, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = set()</span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">"[MASK]"</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#Ê≥®ÊÑèÔºåËøôÊòØ20%ÁöÑÂâç50%,ÊâÄ‰ª•ÊòØ10%ÔºÅÔºÅÔºÅ</span></span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="comment">#Ê≥®ÊÑèÔºåËøôÊòØ20%ÁöÑÂêé50%,ÊâÄ‰ª•ÊòØ10%ÔºÅÔºÅÔºÅ</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, len(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> len(masked_lms) &lt;= num_to_predict</span><br><span class="line">  masked_lms = sorted(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure>
<p>ÁÑ∂ÂêéÊàë‰ª¨ÂèØ‰ª•ËæìÂÖ•ÂëΩ‰ª§ÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python create_pretraining_data.py \</span><br><span class="line">  --input_file=/Users/codewithzichao/Desktop/programs/bert/sample_text.txt \</span><br><span class="line">  --output_file=/Users/codewithzichao/Desktop/programs/bert/tf_examples.tfrecord \</span><br><span class="line">  --vocab_file=/Users/codewithzichao/Downloads/uncased_L-24_H-1024_A-16/vocab.txt \</span><br><span class="line">  --do_lower_case=True \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --masked_lm_prob=0.15 \</span><br><span class="line">  --random_seed=12345 \</span><br><span class="line">  --dupe_factor=5</span><br></pre></td></tr></table></figure>
<p>ÂæóÂà∞ÁªìÊûúÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">I0704 18:10:11.231426 4486237632 create_pretraining_data.py:160] *** Example ***</span><br><span class="line">INFO:tensorflow:tokens: [CLS] and there burst on phil <span class="comment">##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea [MASK] ring ##ed with palaces and towers [MASK] [SEP] like most of [MASK] fellow gold - seekers , cass was super ##sti [MASK] . [SEP]</span></span><br><span class="line">I0704 18:10:11.231508 4486237632 create_pretraining_data.py:162] tokens: [CLS] and there burst on phil <span class="comment">##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea [MASK] ring ##ed with palaces and towers [MASK] [SEP] like most of [MASK] fellow gold - seekers , cass was super ##sti [MASK] . [SEP]</span></span><br><span class="line">INFO:tensorflow:input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 103 3614 2098 2007 22763 1998 7626 103 102 2066 2087 1997 103 3507 2751 1011 24071 1010 16220 2001 3565 16643 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231609 4486237632 create_pretraining_data.py:172] input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 103 3614 2098 2007 22763 1998 7626 103 102 2066 2087 1997 103 3507 2751 1011 24071 1010 16220 2001 3565 16643 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231703 4486237632 create_pretraining_data.py:172] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231793 4486237632 create_pretraining_data.py:172] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_positions: 10 20 23 27 32 39 42 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231858 4486237632 create_pretraining_data.py:172] masked_lm_positions: 10 20 23 27 32 39 42 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_ids: 22741 1010 2007 1012 2010 2001 20771 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231920 4486237632 create_pretraining_data.py:172] masked_lm_ids: 22741 1010 2007 1012 2010 2001 20771 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br><span class="line">I0704 18:10:11.231988 4486237632 create_pretraining_data.py:172] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br><span class="line">INFO:tensorflow:next_sentence_labels: 1</span><br><span class="line">I0704 18:10:11.232054 4486237632 create_pretraining_data.py:172] next_sentence_labels: 1</span><br><span class="line">INFO:tensorflow:Wrote 60 total instances</span><br><span class="line">I0704 18:10:11.242784 4486237632 create_pretraining_data.py:177] Wrote 60 total instances</span><br></pre></td></tr></table></figure>
<p>Êàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÔºåËæìÂá∫ÁªìÊûúÊúâÔºö</p>
<ul>
<li>input_idsÔºöpadding‰πãÂêéÁöÑtokensÔºõ</li>
<li>input_maskÔºöÂØπinput_idsËøõË°åmaskÂæóÂà∞ÁöÑÁªìÊûúÔºõ</li>
<li>segment_idsÔºö0Ë°®Á§∫ÁöÑÊòØÁ¨¨‰∏Ä‰∏™Âè•Â≠êÔºå1Ë°®Á§∫Á¨¨‰∫å‰∏™Âè•Â≠êÔºåÂêéÈù¢ÁöÑ0Ë°®Á§∫padding</li>
<li>masked_lm_positionsÔºöË°®Á§∫Ë¢´ÈöèÊú∫MASKÊéâÁöÑtokenÂú®instance‰∏≠ÁöÑ‰ΩçÁΩÆÔºõ</li>
<li>masked_lm_idsÔºöË°®Á§∫Ë¢´ÈöèÊú∫MASKÊéâÁöÑtokenÂú®ËØçÊ±áË°®‰∏≠ÁöÑÁºñÁ†ÅÔºõ</li>
<li>masked_lm_weigthsÔºöË°®Á§∫Ë¢´ÈöèÊú∫MASKÊéâÁöÑtokenÁöÑÂ∫èÂàóÔºåÂÖ∂‰∏≠1Ë°®Á§∫MASKÊéâÁöÑtokenÊòØÂéüÂßãÁöÑÊñáÊú¨tokenÔºå0Ë°®Á§∫MASKÁöÑÊòØpadding‰πãÂêéÁöÑtoken„ÄÇ</li>
</ul>
<p>ÂΩìÁÑ∂‰∫ÜÔºåËæìÂÖ•ÁöÑÊñáÊú¨‰πüÊúâË¶ÅÊ±ÇÁöÑÔºö‰∏ÄË°åË°®Á§∫‰∏Ä‰∏™Âè•Â≠êÔºå‰∏çÂêåÊñáÁ´†‰πãÈó¥Ë¶ÅÈöî‰∏Ä‰∏™Á©∫Ë°å„ÄÇ</p>
<h3 id="run-pretraining-py"><a href="#run-pretraining-py" class="headerlink" title="run_pretraining.py"></a>run_pretraining.py</h3><p>ËøôÈÉ®ÂàÜÊòØÂØπBERTÊ®°ÂûãËøõË°åÈ¢ÑËÆ≠ÁªÉ„ÄÇ‰∏ÄËà¨Êàë‰ª¨ËøôÈÉ®ÂàÜÈÉΩÊòØ‰∏çÁî®ÁÆ°ÁöÑÔºåÁõ¥Êé•Âä†ËΩΩÂ∑≤ÁªèËÆ≠ÁªÉÂ•ΩÁöÑBERTÊ®°ÂûãÊùÉÈáçÂ∞±ÂèØ‰ª•‰∫Ü(Áõ¥Êé•ËÆ≠ÁªÉ‰∏™‰∫∫Âü∫Êú¨‰∏ä‰∏çÂ§™ÂèØËÉΩÔºåÂ§™ËÄóÈí±‰∫ÜÔºåGoogleÈÉΩÊòØÁî®Â§öÂùóTPUËÆ≠ÁªÉ‰∫ÜÂ•ΩÂá†Â§©„ÄÇ„ÄÇ„ÄÇ)Â§ßËá¥Áúã‰∏Ä‰∏ãÂÆÉÁöÑ‰ª£Á†ÅÁªìÊûÑÂêßÔΩû</p>
<p>Áî±‰∫éÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÔºåBERTÊòØ‰ΩøÁî®MLM‰ªªÂä°‰∏éNSP‰ªªÂä°Êù•ËøõË°åÈ¢ÑËÆ≠ÁªÉÁöÑÔºåÊâÄ‰ª•ÂÖàÊù•Áúã‰∏Ä‰∏ãËøô‰∏§‰∏™‰ªªÂä°ÂêßÔΩû</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ÂÆö‰πâMLM‰ªªÂä°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_masked_lm_output</span><span class="params">(bert_config, input_tensor, output_weights, positions,</span></span></span><br><span class="line"><span class="function"><span class="params">                         label_ids, label_weights)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the masked LM."""</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size,seq_length,hidden_size]ÔºåÊòØtransformerÊúÄÂêé‰∏ÄÂ±ÇÁöÑËæìÂá∫ÁªìÊûú</span></span><br><span class="line"><span class="string">  output_weights:[vocab_size,embedding_size] ËØçÊ±áË°®</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Ëé∑ÂèñmaskËØçÁöÑencodeÔºåMLM lossÂè™ËÆ°ÁÆóË¢´maskÊéâÁöÑ‰ΩçÁΩÆÁöÑ lossÔºÅ</span></span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line">  <span class="comment"># Áª¥Â∫¶Ôºö[batch_size*max_pred_pre_seq,hidden_size]</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/predictions"</span>):</span><br><span class="line">    <span class="comment"># We apply one more non-linear transformation before the output layer.</span></span><br><span class="line">    <span class="comment"># This matrix is not used after pre-training.</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"transform"</span>):</span><br><span class="line">      <span class="comment"># Á∫øÊÄßÂèòÊç¢ÔºåÂú®ËæìÂá∫‰πãÂâçÊ∑ªÂä†‰∏Ä‰∏™ÈùûÁ∫øÊÄßÂèòÊç¢ÔºåÂè™Âú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµËµ∑‰ΩúÁî®</span></span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=bert_config.hidden_size,</span><br><span class="line">          activation=modeling.get_activation(bert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              bert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The output weights are the same as the input embeddings, but there is</span></span><br><span class="line">    <span class="comment"># an output-only bias for each token.</span></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>,</span><br><span class="line">        shape=[bert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment"># ÂæóÂà∞Ëøô‰∏™batch‰∏ãÁöÑÁªìÊûú</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq,vocab_size]</span></span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># label_idsË°®Á§∫maskÊéâÁöÑTokenÁöÑid</span></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq]</span></span><br><span class="line">    label_ids = tf.reshape(label_ids, [<span class="number">-1</span>])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq,vocab_size]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The `positions` tensor might be zero-padded (if the sequence is too</span></span><br><span class="line">    <span class="comment"># short to have the maximum number of predictions). The `label_weights`</span></span><br><span class="line">    <span class="comment"># tensor has a value of 1.0 for every real prediction and 0.0 for the</span></span><br><span class="line">    <span class="comment"># padding predictions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq]ÔºåËøô‰∏™Áõ∏ÂΩì‰∫éÊäämaskÁöÑÈÉ®ÂàÜÁªôÊäΩÂá∫Êù•,minimize (-log MLE)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># scalarÔºå‰∏Ä‰∏™batchÁöÑlossÔºåËøô‰∏™Áõ∏ÂΩì‰∫éÊääpaddingÁöÑÈÉ®ÂàÜÁªôÂéªÈô§‰∫Ü</span></span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    <span class="comment"># scalar</span></span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + <span class="number">1e-5</span></span><br><span class="line">    loss = numerator / denominator <span class="comment"># Âπ≥Âùáloss</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ÂÆö‰πâNSP‰ªªÂä°</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_sentence_output</span><span class="params">(bert_config, input_tensor, labels)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the next sentence prediction."""</span></span><br><span class="line"></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size,hidden_size]</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Simple binary classification. Note that 0 is "next sentence" and 1 is</span></span><br><span class="line">  <span class="comment"># "random sentence". This weight matrix is not used after pre-training.</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/seq_relationship"</span>):</span><br><span class="line">    <span class="comment"># [2,hidden_size]</span></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">"output_weights"</span>,</span><br><span class="line">        shape=[<span class="number">2</span>, bert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(bert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>, shape=[<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size,2]</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># [batch_size,2]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># minimize (-log MLE)</span></span><br><span class="line">    <span class="comment"># [batch_size]</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># scalar</span></span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gather_indexes</span><span class="params">(sequence_tensor, positions)</span>:</span></span><br><span class="line">  <span class="string">"""Gathers the vectors at the specific positions over a minibatch."""</span></span><br><span class="line">  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = sequence_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = sequence_shape[<span class="number">1</span>]</span><br><span class="line">  width = sequence_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  flat_offsets = tf.reshape(</span><br><span class="line">      tf.range(<span class="number">0</span>, batch_size, dtype=tf.int32) * seq_length, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">  flat_positions = tf.reshape(positions + flat_offsets, [<span class="number">-1</span>])</span><br><span class="line">  flat_sequence_tensor = tf.reshape(sequence_tensor,</span><br><span class="line">                                    [batch_size * seq_length, width])</span><br><span class="line">  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)</span><br><span class="line">  <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure>
<p>ÂØπ‰∫éMLM‰ªªÂä°ÔºåËæìÂÖ•ÁöÑÊòØÊúÄÈ°∂Â±Çself-attentionÂ±ÇÁöÑËæìÂá∫ÁªìÊûúÔºåÁª¥Â∫¶ÊòØÔºö<code>[batch_size,seq_length,hidden_size]</code>Ôºå‰ΩÜÊòØÊàë‰ª¨ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÂú®MLM‰∏≠ÔºåÊàë‰ª¨ÂÖ∂ÂÆûÂè™ËÆ°ÁÆóË¢´ÈöèÊú∫MASKÊéâÁöÑtokenÁöÑlossÔºåËøô‰πüÊòØBERTÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ≠ÁªÉÊï∞ÊçÆ‰ª•ÂèäÊî∂ÊïõÊÖ¢ÁöÑÂéüÂõ†ÔºåÊàë‰ª¨ÊúÄÁªàÈúÄË¶ÅËÆ°ÁÆóÁöÑÊòØÔºö<code>[batch_size*max_pred_pre_seq,hidden_size]</code>ÔºåÂÖ∂‰∏≠<code>max_pred_pre_seq</code>Ë°®Á§∫‰∏Ä‰∏™Âè•Â≠êÊúÄÂ§öË¢´MASKÁöÑÊï∞ÁõÆ„ÄÇÂØπ‰∫éMLM lossÁöÑËÆ°ÁÆóÔºåÊàë‰ª¨ÊòØ<font face="times new roman"><strong><em>minimize (-log MLE)</em></strong></font>„ÄÇ</p>
<p>ÂØπ‰∫éNSP‰ªªÂä°ÔºåËæìÂÖ•ÁöÑÊòØÊúÄÈ°∂Â±ÇÁöÑ[CLS]tokenÁöÑtensorÔºåÁª¥Â∫¶ÊòØÔºö<code>[batch_size,hidden_size]</code>ÔºåÁÑ∂Âêé‰ΩøÁî®softmaxËøõË°å‰∫åÂàÜÁ±ª„ÄÇÂÖ≥‰∫éNSP lossÔºå‰ªçÁÑ∂‰∏éMLM loss‰∏ÄÊ†∑ÔºåÊòØ<font face="times new roman"><strong><em>minimize (-log MLE)</em></strong></font>„ÄÇ</p>
<p>ÂÆö‰πâÂÆå‰∏§‰∏™‰ªªÂä°‰πãÂêéÔºåÊàë‰ª¨Â∞±ÈúÄË¶ÅÂÆö‰πâÊ®°ÂûãÔºåÊù•ÂÆåÊàêËÆ≠ÁªÉËøáÁ®ãÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ÂÆö‰πâÊ®°Âûã</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ËæìÂÖ•ÈÉ®ÂàÜ</span></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>] <span class="comment"># paddingÂêéÁöÑtokens </span></span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>] <span class="comment"># ÂØπpaddingÂêéÁöÑtokensËøõË°åmask</span></span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>] <span class="comment"># segment idÔºåÁî®Êù•Âå∫ÂàÜ‰∏§‰∏™Âè•Â≠ê</span></span><br><span class="line">    masked_lm_positions = features[<span class="string">"masked_lm_positions"</span>] <span class="comment"># Ë¢´ÈöèÊú∫MASKÊéâÁöÑtokenÂú®Âè•Â≠ê‰∏≠ÁöÑ‰ΩçÁΩÆ</span></span><br><span class="line">    masked_lm_ids = features[<span class="string">"masked_lm_ids"</span>] <span class="comment"># Ë¢´ÈöèÊú∫MASKÊéâÁöÑtokenÂú®ËØçÊ±áË°®‰∏≠ÁöÑÁºñÁ†Å</span></span><br><span class="line">    masked_lm_weights = features[<span class="string">"masked_lm_weights"</span>] <span class="comment"># Âú®Ë¢´ÈöèÊú∫MASKÊéâÁöÑtoken‰∏≠ÔºåÂ¶ÇÊûútokenÊòØÂéüÊú¨ÁöÑÔºåÈÇ£‰πà‰∏∫1ÔºåÂ¶ÇÊûúMASKÁöÑÊòØpaddingÔºåÈÇ£‰πà‰∏∫0</span></span><br><span class="line">    next_sentence_labels = features[<span class="string">"next_sentence_labels"</span>] <span class="comment"># ÊòØÂê¶‰∏∫‰∏ã‰∏ÄÂè•ÔºåÊòØ‰∏∫1ÔºåÂê¶Âàô‰∏∫0</span></span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ÂÆû‰æãÂåñBERTÊ®°Âûã</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Ëé∑ÂæóMLM‰ªªÂä°ÁöÑÂπ≥ÂùáÊçüÂ§±(scalar)ÔºåbatchÊçüÂ§±Ôºà[batch_size]Ôºâ‰ª•ÂèäÈ¢ÑÊµãÊ¶ÇÁéáÁü©Èòµ([batch_size*max_pred_pre_seq,vocab_size])</span></span><br><span class="line">    (masked_lm_loss,</span><br><span class="line">     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">         bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">         masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Ëé∑ÂæóNSP‰ªªÂä°ÁöÑÂπ≥ÂùáÊçüÂ§±(scalar)ÔºåbatchÊçüÂ§±([batch_size])‰ª•ÂèäÈ¢ÑÊµãÊ¶ÇÁéáÁü©Èòµ([batch_size,2])</span></span><br><span class="line">    (next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">     next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">         bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ÊÄªÁöÑÊçüÂ§±‰∏∫‰∏§ËÄÖÁõ∏Âä†</span></span><br><span class="line">    total_loss = masked_lm_loss + next_sentence_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Ê®°ÂûãÊÄªÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞</span></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># Â¶ÇÊûúÊúâ‰πãÂâç‰øùÂ≠òÁöÑÊ®°ÂûãÔºåÂàôËøõË°åÊÅ¢Â§ç</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                    masked_lm_weights, next_sentence_example_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">                    next_sentence_log_probs, next_sentence_labels)</span>:</span></span><br><span class="line">        <span class="string">"""Computes the loss and accuracy of the model."""</span></span><br><span class="line">        <span class="comment"># ËÆ°ÁÆóÊçüÂ§±‰∏éÂáÜÁ°ÆÁéá</span></span><br><span class="line">        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,</span><br><span class="line">                                         [<span class="number">-1</span>, masked_lm_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        <span class="comment"># [batch_size,max_pred_pre_seq]</span></span><br><span class="line">        masked_lm_predictions = tf.argmax(</span><br><span class="line">            masked_lm_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_ids = tf.reshape(masked_lm_ids, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_weights = tf.reshape(masked_lm_weights, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># ËÆ°ÁÆóÂú®ÊØè‰∏Ä‰∏™‰ΩçÁΩÆ‰∏äÁöÑÂáÜÁ°ÆÁéá</span></span><br><span class="line">        masked_lm_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=masked_lm_ids,</span><br><span class="line">            predictions=masked_lm_predictions,</span><br><span class="line">            weights=masked_lm_weights)</span><br><span class="line">        masked_lm_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=masked_lm_example_loss, weights=masked_lm_weights)</span><br><span class="line"></span><br><span class="line">        next_sentence_log_probs = tf.reshape(</span><br><span class="line">            next_sentence_log_probs, [<span class="number">-1</span>, next_sentence_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        next_sentence_predictions = tf.argmax(</span><br><span class="line">            next_sentence_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        next_sentence_labels = tf.reshape(next_sentence_labels, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># ËÆ°ÁÆóÂú®NSP‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÁéá</span></span><br><span class="line">        next_sentence_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=next_sentence_labels, predictions=next_sentence_predictions)</span><br><span class="line">        next_sentence_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=next_sentence_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"masked_lm_accuracy"</span>: masked_lm_accuracy,</span><br><span class="line">            <span class="string">"masked_lm_loss"</span>: masked_lm_mean_loss,</span><br><span class="line">            <span class="string">"next_sentence_accuracy"</span>: next_sentence_accuracy,</span><br><span class="line">            <span class="string">"next_sentence_loss"</span>: next_sentence_mean_loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn, [</span><br><span class="line">          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span><br><span class="line">          masked_lm_weights, next_sentence_example_loss,</span><br><span class="line">          next_sentence_log_probs, next_sentence_labels</span><br><span class="line">      ])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"Only TRAIN and EVAL modes are supported: %s"</span> % (mode))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure>
<h3 id="run-classifier-py"><a href="#run-classifier-py" class="headerlink" title="run_classifier.py"></a>run_classifier.py</h3><p>Â¶ÇÊûúÊàë‰ª¨ÊÉ≥‰ΩøÁî®BERTÂú®Ëá™Â∑±ÁöÑ‰ªªÂä°‰∏äÔºåÊàë‰ª¨ÈúÄË¶Å‰øÆÊîπÁöÑÂ∞±ÊòØrun_classifier.pyÊñá‰ª∂„ÄÇÂÖ∑‰Ωì‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<p>È¶ñÂÖàÊòØÂØπÊï∞ÊçÆÂ§ÑÁêÜÔºåÂØπ‰∫éBERTÊù•ËØ¥ÔºåGoogleÂÆòÊñπÂÆûÁé∞ÁöÑ‰ª£Á†Å‰∏≠ÔºåËæìÂÖ•ÊòØfrom_tensor_slicesÔºåÊâÄ‰ª•È¶ñÂÖàÈúÄË¶ÅÂØπÊï∞ÊçÆÈõÜËøõË°åÂ§ÑÁêÜÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataProcessor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Base class for data converters for sequence classification data sets."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_train_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the train set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_dev_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the dev set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_test_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for prediction."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_labels</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets the list of labels for this data set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br></pre></td></tr></table></figure>
<p>Êàë‰ª¨Âè™ÈúÄË¶ÅÁªßÊâøËøô‰∏™Á±ªÔºåÈáçÂÜôËøôÂá†‰∏™ÂáΩÊï∞Âç≥ÂèØ„ÄÇÊï∞ÊçÆÂ§ÑÁêÜÂÆå‰πãÂêéÔºåÊù•ÁúãÁúãÊÄé‰πàÊé•ÂÖ•‰∏ãÊ∏∏‰ªªÂä°Ôºå‰ª•ÂàÜÁ±ª‰ªªÂä°‰∏∫‰æãÔºå‰ª£Á†ÅÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(bert_config, is_training, input_ids, input_mask, segment_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                 labels, num_labels, use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Creates a classification model."""</span></span><br><span class="line">  model = modeling.BertModel(</span><br><span class="line">      config=bert_config,</span><br><span class="line">      is_training=is_training,</span><br><span class="line">      input_ids=input_ids,</span><br><span class="line">      input_mask=input_mask,</span><br><span class="line">      token_type_ids=segment_ids,</span><br><span class="line">      use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># In the demo, we are doing a simple classification task on the entire</span></span><br><span class="line">  <span class="comment"># segment.</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If you want to use the token-level output, use model.get_sequence_output()</span></span><br><span class="line">  <span class="comment"># instead.</span></span><br><span class="line">  <span class="comment"># Â¶ÇÊûúÊòØÁî®‰∫éÂàÜÁ±ªÁöÑËØùÔºåÈÇ£‰πàÊàë‰ª¨ÊòØ‰ΩøÁî®ÊúÄÂêé‰∏ÄÂ±ÇÁöÑ[CLS]ÁöÑÂêëÈáèÔºåÁª¥Â∫¶ÊòØÔºö[batch_size,hidden_size]</span></span><br><span class="line">  output_layer = model.get_pooled_output()</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Ëé∑Âèñhidden_size</span></span><br><span class="line">  hidden_size = output_layer.shape[<span class="number">-1</span>].value</span><br><span class="line"></span><br><span class="line">  output_weights = tf.get_variable(</span><br><span class="line">      <span class="string">"output_weights"</span>, [num_labels, hidden_size],</span><br><span class="line">      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">  output_bias = tf.get_variable(</span><br><span class="line">      <span class="string">"output_bias"</span>, [num_labels], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"loss"</span>):</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      <span class="comment"># Â¶ÇÊûúÊòØËÆ≠ÁªÉÁöÑËØùÔºåÈÇ£Â∞±Âä†dropoutÔºÅ</span></span><br><span class="line">      <span class="comment"># I.e., 0.1 dropout</span></span><br><span class="line">      output_layer = tf.nn.dropout(output_layer, keep_prob=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Á∫øÊÄßÂèòÊç¢ÔºåÁª¥Â∫¶‰∏∫Ôºö[batch_size,num_labels]</span></span><br><span class="line">    logits = tf.matmul(output_layer, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    <span class="comment"># softmax</span></span><br><span class="line">    probabilities = tf.nn.softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># log_softmax,[batch_size,num_labels]</span></span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size,num_labels]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># minimize (-log MLE)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, logits, probabilities)</span><br></pre></td></tr></table></figure>
<p>Êé•ÂÖ•‰∏ãÊ∏∏‰ªªÂä°ÔºåÂæóÂà∞Êï¥‰∏™Ê®°ÂûãÁöÑloss‰πãÂêéÔºåÁÑ∂ÂêéÂ∞±ÂºÄÂßãfinetuneÔºåÂ¶Ç‰∏ãÔºö</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, num_labels, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>]</span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>]</span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>]</span><br><span class="line">    label_ids = features[<span class="string">"label_ids"</span>]</span><br><span class="line">    is_real_example = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"is_real_example"</span> <span class="keyword">in</span> features:</span><br><span class="line">      is_real_example = tf.cast(features[<span class="string">"is_real_example"</span>], dtype=tf.float32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ÂàõÂª∫Ê®°ÂûãÔºåÂæóÂà∞lossÔºåbatch lossÔºålogits‰ª•ÂèäÊ¶ÇÁéáÈ¢ÑÊµãÁü©Èòµ</span></span><br><span class="line">    (total_loss, per_example_loss, logits, probabilities) = create_model(</span><br><span class="line">        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,</span><br><span class="line">        num_labels, use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># ËÆ≠ÁªÉÊ®°Âºè</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      </span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="comment"># ËØÑ‰º∞Ê®°Âºè</span></span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(per_example_loss, label_ids, logits, is_real_example)</span>:</span></span><br><span class="line">        predictions = tf.argmax(logits, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=label_ids, predictions=predictions, weights=is_real_example)</span><br><span class="line">        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"eval_accuracy"</span>: accuracy,</span><br><span class="line">            <span class="string">"eval_loss"</span>: loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn,</span><br><span class="line">                      [per_example_loss, label_ids, logits, is_real_example])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="comment"># ÊµãËØïÊ®°ÂºèÔºåÈ¢ÑÊµã</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          predictions=&#123;<span class="string">"probabilities"</span>: probabilities&#125;,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure>
<p>‰πãÂêéËøêË°åmainÂáΩÊï∞Â∞±ÂèØ‰ª•ÂÆåÊàêÊï¥‰∏™ËÆ≠ÁªÉ‰∫Ü„ÄÇ</p>
<p>Êï¥‰∏™BERTÊ®°ÂûãÁöÑ‰ª£Á†ÅÂ∞±ÁúãÂÆåÂï¶ÔºåËØª‰∏ãÊù•ÁöÑÊÑüÂèóÂ∞±ÊòØÔºöÈùûÂ∏∏ÁöÑËàíÁàΩÔºå‰∏çÂæó‰∏çËØ¥ÔºåGoogleÂÜôÁöÑ‰ª£Á†ÅË¥®ÈáèËøòÊòØÈùûÂ∏∏Â•ΩÁöÑÔºåËôΩÁÑ∂ÊàëÊìÖÈïøÁöÑÊòØtensorflow2.xÔºå‰ΩÜÊòØtensorflow1.xÁöÑ‰ª£Á†ÅËØªËµ∑Êù•ËøòÊòØÊ≤°Êúâ‰ªÄ‰πàÈöúÁ¢çÁöÑ„ÄÇ‰πãÂêéÊúÄÂ•ΩÂú®Ëá™Â∑±ÁöÑ‰ªªÂä°‰∏ä‰ΩøÁî®BERTÊù•ÁúãÁúãÊïàÊûúÔΩûÈô§Ê≠§‰πãÂ§ñÔºåÂ¶ÇÈùûÂøÖË¶ÅÔºå‰ª•ÂêéÂÖ≥‰∫éÂêÑÁßçBERTÁöÑÂèò‰ΩìÊ®°ÂûãÂ∞±‰∏ç‰ºöËß£ËØªÂÆÉÁöÑÊ∫ê‰ª£Á†Å‰∫ÜÔºåÂü∫Êú¨‰∏äÈÉΩÊòØÁÖßÊê¨BERTÁöÑÊ∫êÁ†ÅÂÆûÁé∞ÔºåÁÑ∂ÂêéÂú®È¢ÑËÆ≠ÁªÉ‰ªªÂä°‰∏äÊàñËÄÖBERTÊ®°Âûã‰∏ª‰ΩìÈÉ®ÂàÜÁöÑÂÆûÁé∞‰∏äËøõË°å‰∏Ä‰∫õÊîπÂä®ÔºåÊï¥‰Ωì‰∏äÂ§ßÂêåÂ∞èÂºÇÔΩû</p>
<p>overÔΩû‚òïÔ∏è</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>Would you like to buy me a cup of coffee‚òïÔ∏èÔΩû</div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="zichao WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="zichao Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/BERT/" rel="tag"># BERT</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/02/NLP-former%E6%A8%A1%E5%9E%8B/" rel="prev" title="NLP|{}formerÊ®°Âûã">
      <i class="fa fa-chevron-left"></i> NLP|{}formerÊ®°Âûã
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/11/Pytorch%E7%9A%84%E9%9B%B6%E7%A2%8E%E7%AC%94%E8%AE%B0/" rel="next" title="PytorchÁöÑÈõ∂Á¢éÁ¨îËÆ∞">
      PytorchÁöÑÈõ∂Á¢éÁ¨îËÆ∞ <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#BERTÊï¥‰Ωì‰ª£Á†ÅÁªìÊûÑ"><span class="nav-number">1.</span> <span class="nav-text">BERTÊï¥‰Ωì‰ª£Á†ÅÁªìÊûÑ</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ê†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂Ëµ∞ËØª"><span class="nav-number">2.</span> <span class="nav-text">Ê†∏ÂøÉ‰ª£Á†ÅÊñá‰ª∂Ëµ∞ËØª</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#modeling-py"><span class="nav-number">2.1.</span> <span class="nav-text">modeling.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tokenization-py"><span class="nav-number">2.2.</span> <span class="nav-text">tokenization.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#create-pretraining-data-py"><span class="nav-number">2.3.</span> <span class="nav-text">create_pretraining_data.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#run-pretraining-py"><span class="nav-number">2.4.</span> <span class="nav-text">run_pretraining.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#run-classifier-py"><span class="nav-number">2.5.</span> <span class="nav-text">run_classifier.py</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="zichao"
      src="/images/photo.jpg">
  <p class="site-author-name" itemprop="name">zichao</p>
  <div class="site-description" itemprop="description">Just learning</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">86</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/codewithzichao" title="GitHub ‚Üí https:&#x2F;&#x2F;github.com&#x2F;codewithzichao" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lizichao@pku.edu.cn" title="E-Mail ‚Üí mailto:lizichao@pku.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/codewithzichao" title="Weibo ‚Üí https:&#x2F;&#x2F;weibo.com&#x2F;codewithzichao" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zichao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme ‚Äì <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  
















  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":225,"height":450},"mobile":{"show":false}});</script></body>
</html>
