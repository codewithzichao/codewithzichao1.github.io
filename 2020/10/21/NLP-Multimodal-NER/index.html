<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"codewithzichao.github.io","root":"/","scheme":"Muse","version":"8.0.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>

  <meta name="description" content="近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要解读一下目前多模态NER上质量比较好的论文：《Adaptive Co-Attention Network for Named Entity Recognition in Tweets》、《Visual Attention Model for Name Tagging in Multi">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP|Multimodal NER">
<meta property="og:url" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/index.html">
<meta property="og:site_name" content="codewithzichao">
<meta property="og:description" content="近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要解读一下目前多模态NER上质量比较好的论文：《Adaptive Co-Attention Network for Named Entity Recognition in Tweets》、《Visual Attention Model for Name Tagging in Multi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/ACN_model.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/ACN_result.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/VATT_model.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/gvatt_result.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/MNER_model.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/MNER_result.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/UMT_model.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/UMT_result.jpg">
<meta property="og:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/UMT_ablation.jpg">
<meta property="article:published_time" content="2020-10-21T12:23:45.000Z">
<meta property="article:modified_time" content="2020-10-27T15:46:43.223Z">
<meta property="article:author" content="zichao">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Multimodal NER">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/ACN_model.jpg">


<link rel="canonical" href="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>NLP|Multimodal NER | codewithzichao</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">codewithzichao</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Confident，Modest，Patient</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#adaptive-co-attention-network-for-named-entity-recognition-in-tweetsaaai2018"><span class="nav-number">1.</span> <span class="nav-text">Adaptive Co-Attention Network for Named Entity Recognition in Tweets(AAAI2018)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#background"><span class="nav-number">1.1.</span> <span class="nav-text">background</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-number">1.2.</span> <span class="nav-text">model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#experiment"><span class="nav-number">1.3.</span> <span class="nav-text">experiment</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#visual-attention-model-for-name-tagging-in-multimodal-social-mediaacl2018"><span class="nav-number">2.</span> <span class="nav-text">Visual Attention Model for Name Tagging in Multimodal Social Media(ACL2018)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#background-1"><span class="nav-number">2.1.</span> <span class="nav-text">background</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-1"><span class="nav-number">2.2.</span> <span class="nav-text">model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#experiment-1"><span class="nav-number">2.3.</span> <span class="nav-text">experiment</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-named-entity-recognition-for-short-social-media-postsnaacl2018"><span class="nav-number">3.</span> <span class="nav-text">Multimodal Named Entity Recognition for Short Social Media Posts(NAACL2018)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#background-2"><span class="nav-number">3.1.</span> <span class="nav-text">background</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-2"><span class="nav-number">3.2.</span> <span class="nav-text">model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#experiment-2"><span class="nav-number">3.3.</span> <span class="nav-text">experiment</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#improving-multimodal-named-entity-recognition-via-entity-span-detection-with-uniﬁed-multimodal-transformeracl2020"><span class="nav-number">4.</span> <span class="nav-text">Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer(ACL2020)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#background-3"><span class="nav-number">4.1.</span> <span class="nav-text">background</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-3"><span class="nav-number">4.2.</span> <span class="nav-text">model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#experiment-3"><span class="nav-number">4.3.</span> <span class="nav-text">experiment</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#references"><span class="nav-number">5.</span> <span class="nav-text">references</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zichao</p>
  <div class="site-description" itemprop="description">Just learning</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">80</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">117</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://codewithzichao.github.io/2020/10/21/NLP-Multimodal-NER/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zichao">
      <meta itemprop="description" content="Just learning">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="codewithzichao">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP|Multimodal NER
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-10-21 20:23:45" itemprop="dateCreated datePublished" datetime="2020-10-21T20:23:45+08:00">2020-10-21</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2020-10-27 23:46:43" itemprop="dateModified" datetime="2020-10-27T23:46:43+08:00">2020-10-27</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/" itemprop="url" rel="index"><span itemprop="name">命名实体识别</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要解读一下目前多模态NER上质量比较好的论文：《Adaptive Co-Attention Network for Named Entity Recognition in Tweets》、《Visual Attention Model for Name Tagging in Multimodal Social Media》、《Multimodal Named Entity Recognition for Short Social Media Posts》、《Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer》。</p>
<a id="more"></a>
<h2 id="adaptive-co-attention-network-for-named-entity-recognition-in-tweetsaaai2018">Adaptive Co-Attention Network for Named Entity Recognition in Tweets(AAAI2018)</h2>
<h3 id="background">background</h3>
<p>NER是NLP最基础且重要的任务之一，但是传统的NER都是只针对与textual content进行建模。在社交媒体上，<strong>很多的推文都是文字+图片的形式，并且社交媒体上的文字往往比较口语化、语法也比较informal、字数比较少、很多文字内容只有结合image才能理解，</strong>单纯对text进行建模，往往得不到很好的结果，而image对于NER提供了很好的辅助信息，所以如何利用image信息来进行NER，是一个非常重要的问题。这篇paper正是针对这样一个问题，构建了twitter数据集，并提出了ACN模型。</p>
<h3 id="model">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/ACN_model.jpg"></p>
<p>ACN模型主要分为三部分：<strong>feature extractor、adaptive co-attention netowrk、CRF tagging</strong>。</p>
<ul>
<li><p><strong>feature extractor：</strong>这一部分分为两个小部分：image feature extraction与text feature extraction。</p>
<ul>
<li>对于image，采用VGG16来对iamge进行encoding，我们抽取最后一层的输出作为image的representation，表示为：<span class="math inline">\(\tilde v_I=\{\tilde v_i|\tilde v_i\in R^{d_v},i=1,...,N\}\)</span>，其中<span class="math inline">\(d_v\)</span>表示每一个region的维度，共有7x7个region，即：<span class="math inline">\(N=49\)</span>，所以一张image输出的维度是：<span class="math inline">\(N\times d_v\)</span>。为了能够与之后的text进行交互，维度与text相同，对其进行了非线性变换，并使用tanh进行激活，得：<span class="math inline">\(v_I=tanh(W_I\tilde v_I+B+I)\)</span>，<span class="math inline">\(v_I\)</span>是最终的image的representation。</li>
<li>对于text，是character-level word embedding与word embedding的concat。首先是character embedding，使用多个不同的cnn来对其进行编码，并使用tanh进行激活，然后对每一个结果使用max-over-time pooling，最后全部进行concat(常规操作了，没啥新颖的。)，得到character-level word embedding：<span class="math inline">\(w^{&#39;}\)</span>；将<span class="math inline">\(w^{&#39;}\)</span>与<span class="math inline">\(w^{&#39;&#39;}\)</span>进行concat，得到<span class="math inline">\(w\)</span>，将<span class="math inline">\(w\)</span>输入到BILSTM，得到encoding后的句子的token的表示：<span class="math inline">\(x=\{h_j|h_j\in R^d,j\in 1,2,...,n\}\)</span>，<span class="math inline">\(n\)</span>表示句子的长度。</li>
</ul></li>
<li><p><strong>Adaptive co-attention network：</strong>在得到image与text的representation之后，我们希望将两者进行融合。但是对于sentence中的每一个token，不是所有的image region都有用，反过来也是一样的，对于image中的每一个region，不是sentence中的所有token都与其相关，那么一种很好的方式就是使用co-attention，即：image对text进行attention(word-guided visual attention)与text对text进行attention(image-guided textual attention)与。具体如下：</p>
<ul>
<li><p><strong>word-guided visual attention：</strong>不是所有的image region都与sentence中的token相关，如果将image中所有的信息全部给sentence，那么会引入很多噪音，损害模型性能。所以我们设计了word-guided visual attention，从而让模型自动过滤掉与sentnece无关的image region，只留下最相关的region。公式如下： <span class="math display">\[
z_t=tanh(W_{v_I}v_I\oplus (W_{h_t}h_t+b_{h_t})) \\
\alpha_t=softmax(W_{\alpha_t}z_t+b_{\alpha_t}) \\
\tilde v_t=\sum_{i=1}^{N}\alpha_{t,i}v_i \\
v_i\in v_I,i=1,...,N
\]</span> 注意，<span class="math inline">\(v_I\in E^{N\times d_v}\)</span>，<span class="math inline">\(h_t\in R^d\)</span>，最终的<span class="math inline">\(\tilde v_t\in R^{d&#39;}\)</span>。</p></li>
<li><p><strong>image-guided textual attention：</strong>除了image中的信息外，我们还希望对text本身进行建模，也就是在text中找到与每一个token相关的words。做法是：在word-guided visual attention的基础上，继续使用相似的co-attention机制，来得到新的结果。具体如下： <span class="math display">\[
z_t^{&#39;}=tanh(W_{x}x\oplus (W_{x,\tilde v_t}\tilde v_t+b_{x,\tilde v_t})) \\
\beta_t=softmax(W_{\beta_t}z_t^{&#39;}+b_{\beta_t}) \\
\tilde h_t=\sum_{i=1}^{n}\beta_{t,i}h_i \\
h_i\in x,i=1,...,n
\]</span> 但实际上，这种做法并不是很好，有没有更好的？当然有啊，BERT！今年ACL2020的UMT模型就使用BERT来做，取得了很大的提高。</p></li>
<li><p><strong>Gated multimodal fusion：</strong>得到attention之后的<span class="math inline">\(\tilde v_t\)</span>与<span class="math inline">\(\tilde h_t\)</span>，<span class="math inline">\(t=1,...,n\)</span>，然后我们设计GMF机制，来过滤一些信息。公式如下： <span class="math display">\[
h_{\tilde v_t}=tanh(W_{\tilde v_t}\tilde v^t+b_{\tilde v_t}) \\
h_{\tilde h_t}=tanh(W_{\tilde h_t}\tilde h^t+b_{\tilde h_t}) \\
g_t=sigmoid(W_{g_t}(h_{\tilde v_t}\oplus h_{\tilde h_t})) \\
m_t=g_th_{\tilde v_t}+(1-g_t)h_{\tilde h_t}
\]</span></p></li>
<li><p><strong>Filtration gate：</strong>由于image的信息不是必须的，而text的信息是必须的，所以在最终输入到CRF之前，需要再次调整两者的比重。具体如下： <span class="math display">\[
s_t=sigmoid(W_{s_t,h_t}h_t \oplus (W_{m,s_t}m_t+b_{m_t,s_t})) \\
u_t=s_t(tanh(W_{m_tm_t+b_{m_t}})) \\
\tilde m_t=W_{\tilde m_t}(h_t\oplus u_t)
\]</span> 从公式中可以看出，<span class="math inline">\(s_t\)</span>控制着image信息的比例，当我们不需要image信息的时候，我们就让其越接近于0，如果越需要image的信息，我们就让其越接近于1。</p></li>
</ul></li>
<li><p><strong>CRF tagging：</strong>没什么可讲的，就是标准的CRF层。</p></li>
</ul>
<h3 id="experiment">experiment</h3>
<ol type="1">
<li><p>数据集：twitter数据集(released by this paper)</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/ACN_result.jpg"></p>
<p>可以看到，这篇paper提出的model效果还是可以的，另外·，从ablation实验中可以看到，各个component设计的还是比较合理的，总之是一片很经典的论文，值得多读一读。</p></li>
</ol>
<h2 id="visual-attention-model-for-name-tagging-in-multimodal-social-mediaacl2018">Visual Attention Model for Name Tagging in Multimodal Social Media(ACL2018)</h2>
<h3 id="background-1">background</h3>
<p>这篇paper同样是针对multimodal NER，针对的问题也是一样的：<strong>如何更好的利用image信息来复制NER任务？</strong>在这篇paper中，设计了一种的新的attention机制和gate机制，从而取得了SOTA的结果。</p>
<h3 id="model-1">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/VATT_model.jpg"></p>
<p>GVAtt的整体结构分为部分：<strong>feature representation、Visual attention model、visual modulation module。</strong></p>
<ul>
<li><p><strong>feature representation：</strong>分为两部分：text representation与visual representation。对于前者，是word embedding与character-level word embedding的concat。所谓的character-level word embedding是指将character embedding输入到BILSTM当中(常规操作)。对于后者，采用的是ResNet，保留两部分：<span class="math inline">\(V_g\)</span>与<span class="math inline">\(V_r\)</span>，<span class="math inline">\(V_g\)</span>是指最后的全连接层的输出，维度是：<span class="math inline">\(V_g\in R^{1024}\)</span>，<span class="math inline">\(V_r\)</span>是输入到最后的全连接层之前的输出，其维度是：<span class="math inline">\(V_r\in R^{1024\times 49}\)</span>。这里之所以保留了两个，是因为后面做了实验比较，并不是整张image都与sentence相关，不相关的部分反而会引入噪音。</p></li>
<li><p><strong>visual attention model：</strong>这一部分主要是要解决：对于sentence中的每一个token，如何保留与其最相关的image region，并且去除与其不相关的region？即：提取出与sentence中相关的image region feature，具体公式如下： <span class="math display">\[
Q=LSTM(S) \\
P_t=tanh(W_tQ) \\
P_v=tanh(W_vV_r) \\
A=P_t\oplus P_v \\
E=softmax(W_aA+b_a) \\
v_c=\sum\alpha_iv_i,\alpha\in E,v_i\in V_r
\]</span> 其中，<span class="math inline">\(S\)</span>表示feature representation中的concat后的text embedding，最后得到的<span class="math inline">\(v_c\)</span>最为之后的BISLTM的初始输入。另外再提一句，这里之所以使用的是<span class="math inline">\(V_r\)</span>，是因为之后的实验表明了使用<span class="math inline">\(V_r\)</span>的效果要比<span class="math inline">\(v_g\)</span>更好。</p></li>
<li><p><strong>visual modulation module：</strong>设置这一层的目的是：将第二步提取出的visual信息与text embedding进行结合。首次将第一步得到的text embedding输出到BILSTM当中，得到<span class="math inline">\(h_i\)</span>；然后仿照LSTM的更新方式，设计了gate机制，具体如下： <span class="math display">\[
\beta_v=sigmoid(W_vh_i+U_vv_c+b_v) \\
\beta_w=sigmoid(W_wh_i+U_wv_c+b_w) \\
m=tanh(W_mh_i+U_mv_c+b_m) \\
w_m=\beta_wh_i+\beta_vm
\]</span> 最终计算得到的<span class="math inline">\(w_m\)</span>作为CRF层的输入，CRF就是标准的CRF层，这里不再赘述。</p></li>
</ul>
<h3 id="experiment-1">experiment</h3>
<ol type="1">
<li><p>数据集：twitter、snap captions</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/gvatt_result.jpg"></p>
<p>从结果来看，很不错，虽然paper里面没有个ACN模型进行对比，但是从twitter数据集结果来看，F1值高了近10个百分点！nb！</p></li>
</ol>
<h2 id="multimodal-named-entity-recognition-for-short-social-media-postsnaacl2018">Multimodal Named Entity Recognition for Short Social Media Posts(NAACL2018)</h2>
<h3 id="background-2">background</h3>
<p>这篇paper也是针对multimodal NER，解决的问题也是两个：1.social media上的tweets是非常不规范的、informal并且字数较少，所以一方面因为没有足够的信息会引起一词多义性问题，此外，由于不规范的使用，使得会产生很多未知的token，这个怎么解决？(其实现在来看就很好解决，上BERT啊)；2.怎么让image与text更好进行交互？为了解决这些问题，就有了MNER模型。</p>
<h3 id="model-2">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/MNER_model.jpg" style="zoom:50%;"></p>
<p>整体模型分为三部分：<strong>features、modality attention module、BILSTM+CRF</strong>。</p>
<ul>
<li><p><strong>task definition：</strong>输入的sentence表示为：<span class="math inline">\(x=\{x_t\}_{t=1}^{T}\)</span>，其对应的标签是:<span class="math inline">\(y=\{y_t\}_{t=1}^{T}\)</span>，每一个token由三种modality组成：<span class="math inline">\(x_t=\{x_t^{(w)},x_t^{(c)},x_t^{(v)}\}\)</span>，分别表示word embedding、character embedding、visual embedding。</p></li>
<li><p><strong>features：</strong>共分为三种：word embedding、character embedding、visual embedding。word embedding由GloVe初始化，并在training过程进行更新；character embedding是将character embedding送入BILSTM当中，得出的输出；visual embedding是使用GoogleNet的最后一层的输出。</p></li>
<li><p><strong>modality attention module：</strong>在之前的paper中，大部分都是concat text与image的信息，然后再输入到CRF当中，但是concat的方式会损失信息，造成效果下降。为了解决这个问题，这篇paper分别对三个modality进行编码，再进行加权求和。具体公式如下： <span class="math display">\[
[a_t^{(w)},a_t^{(c)},a_t^{(v)}]=sigmoid(W_m[x_t^{(w)};x_t^{(c)};x_t^{(v)}]+b_m) \\
\alpha_{t}^{(m)}=\frac{exp(a_t^{(m)})}{\sum_{m^{&#39;}\in \{w,c,v\}}exp(a_t^{(m^{&#39;})})} ,\forall m\in \{w,c,v\} \\
\overline x_t=\sum_{m\in \{w,c,v\}}\alpha_t^{(m)}x_t^{(m)}
\]</span> <span class="math inline">\(\overline x_t\)</span>是最终到BILSTM+CRF的输入。</p></li>
<li><p><strong>BILSTM+CRF：</strong>这就是标准的BILSTM+CRF，不再赘述。</p></li>
</ul>
<h3 id="experiment-2">experiment</h3>
<ol type="1">
<li><p>数据集：snap caption</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/MNER_result.jpg"></p>
<p>从结果来看，还行吧，没啥亮点。</p></li>
</ol>
<h2 id="improving-multimodal-named-entity-recognition-via-entity-span-detection-with-uniﬁed-multimodal-transformeracl2020">Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer(ACL2020)</h2>
<h3 id="background-3">background</h3>
<p>这是最新的multimodal NER的paper，主要解决的问题有：1.text与image的交互不够，即便能够得到简单交互后的结果，但是text对image context并不敏感；2.之前的研究并没有考虑image带来的bias。针对于第一个问题，这篇paper提出了MMI module，针对于第二个问题，这篇paper设计了ESD辅助任务来解决。具体的下面小节将详细的介绍。</p>
<h3 id="model-3">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/UMT_model.jpg"></p>
<p>整体模型分为三部分：<strong>unimodal input representation、multimodal transformer、unified multimodal transformer。</strong></p>
<ul>
<li><p><strong>unimodal input representation：</strong>主要分为两部分：word representation与visual representation。对于前者，paper中使用了BERT，一方面是能够得到更加强大的representation，另一方面也能够解决一词多义性问题。假设输入的句子为：<span class="math inline">\(S^{&#39;}=(s_0,s_1,...,s_{n+1})\)</span>，其中<span class="math inline">\(s_0、s_{n+1}\)</span>分别表示[CLS]与[SEP] token，通过三种embedding相加，得到的句子表示为：<span class="math inline">\(X=(x_0,x_1,...,x_{n+1})\)</span>，通过BERT之后，为：<span class="math inline">\(C=(c_0,c_1,...,c_{n+1})\)</span>，其中<span class="math inline">\(c_i\in R^d\)</span>；对于后者，采用ResNet来对image进行编码，我们保留ResNet的最后一个卷积层的输出作为该image的representation，为：<span class="math inline">\(U=(u_1,u_2,...,u_49)\)</span>，<span class="math inline">\(u_i\)</span>表示一个region，且其维度是：<span class="math inline">\(R^{2048}\)</span>，然后我们通过一个线性变换，让其维度与word representation的维度一样，为：<span class="math inline">\(V=(v_1,...,v_{49}),v_i\in R^d\)</span>。</p></li>
<li><p><strong>multimodal transformer：</strong>在正式对word与visual进行交互之前，我们首先对word representation再使用一层标准的transformer layer，我个人理解是增加非线性，不加其实也说的通，得到<span class="math inline">\(R=(r_0,r_1,...,r_{n+1})\)</span>。接下来，设计了MMI module，让word与visual进行交互。具体分为两部分：image-aware word representation与word-aware visual representation。</p>
<ul>
<li><p><strong>image-aware word representation：</strong>这一步是image对word进行attention，得到更好的word representation。那么，就是<span class="math inline">\(V\)</span>作为query，<span class="math inline">\(R\)</span>作为key和value，使用m-head cross-modal attention，公式如下： <span class="math display">\[
CA_i(V,R)=softmax(\frac{[W_{q_i}V]^T[W_{k_i}R]}{\sqrt {d/m}})[W_{v_i}R]^T \\
MH-CA(V,R)=W^{&#39;}[CA_1(V,R),CA_2(V,R),...,CA_m(V,R)]^T \\
\tilde P=LN(V+MH-CA(V,R)) \\
P=LN(\tilde P+FFN(\tilde P))
\]</span> 其中，<span class="math inline">\(P=(p_1,...,p_{49})\)</span>。但是这个与我们最终想要得到更好的word representation的目标不一致，所以我们将<span class="math inline">\(R\)</span>作为query，<span class="math inline">\(P\)</span>作为key和value，再输入到一个与上述一样的transformer layer，得到<span class="math inline">\(A=(a_0,a_1,...,a_{n+1})\)</span>。</p></li>
<li><p><strong>word-aware visual representation：</strong>这里是word对visual进行attention，所以将word <span class="math inline">\(R\)</span>作为query，<span class="math inline">\(V\)</span>作为key和value，应用和上述一样的transformer layer，得到<span class="math inline">\(Q=(q_0,q_1,...,q_{n+1})\)</span>。然后我们希望能够控制visual information的输入，所以有设计了visual gate，具体公式如下： <span class="math display">\[
g=sigmoid(W_a^TA+W_q^TQ) \\
B=gQ
\]</span> <span class="math inline">\(B\)</span>就是最终的visual representation。</p></li>
<li><p><strong>CRF：</strong>我们将<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>进行concat，并且输送到标准的CRF kayer当中。</p></li>
</ul></li>
<li><p><strong>unified multimodal transformer：</strong>在上一步的multimodal transformer当中，都是在关注如何对text和image之间的关系进行建模，但是这样很容易导致模型过分去关注image中有的entity，对于在sentence中但是不在image中的实体关注不够，因此为了解决这个问题，设计了ESD辅助任务，用来预测entity的type。假设一个句子的标注为：<span class="math inline">\(z=(z_0,...,z_{n+1})\)</span>，其中<span class="math inline">\(z_i\in \cal Z\)</span>，<span class="math inline">\(\cal Z=\{B,I,O\}\)</span>，我们将其输送到一个transformer layer+CRF layer。然后我们想要将其辅助与最终的预测，所以设计了一个conversion matrix：<span class="math inline">\(W^c\in R^{|\cal Z|\times |\cal Y|}\)</span>，其中<span class="math inline">\(\cal Z=\{B,I,O\}\)</span>，<span class="math inline">\({\cal Y}=\{B-PEC,I-PEC,...\}\)</span>，如果<span class="math inline">\(\cal Z_j\)</span>不是<span class="math inline">\(\cal Y_k\)</span>的相关label，那么<span class="math inline">\(W^c_{j,k}=0\)</span>，否则为<span class="math inline">\(W^c_{j,k}=\frac{1}{|C_j|}\)</span>。</p></li>
</ul>
<h3 id="experiment-3">experiment</h3>
<ol type="1">
<li><p>数据集：twitter2015、twitter2017</p></li>
<li><p>details：训练过程中，resnet参数不更新，bert参数更新，seq_length为128，batch_size为16。</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/UMT_result.jpg"></p>
<p>从结果来看，提升还是挺大的，个人觉得是得益于BERT的使用以及设计的ESD辅助任务，还是不错的。</p></li>
<li><p>消融实验</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/UMT_ablation.jpg"></p>
<p>这是paper当中消融实验的结果，可以看到，在UMT模型中，各个component都比较重要，去掉都会有影响，但是相比来说，ESD辅助任务发挥了更大的作用，之后可以搞一波。</p></li>
<li><p>base case分析：在paper的base case分析中，主要的错误在于：当image与text完全不相关的时候，BERT-CRF预测会正确，而UMT预测错误，实际上就相当于image是噪音，在整个数据集中，大概有5%的比例。</p></li>
</ol>
<p>小小的总结一下吧：对与Multimodal NER任务，目前主要的难点是：1.如何设计更好的机制，从而让token在image找到与其最相关的region？2.在1的基础上，怎么对text和image region更好地进行融合？3.image当中其实只有很少的实体，对于sentence中的其他实体，image其实并不能提供有效的信息，怎么去解决这种情况？4.当image与text完全不相关时，怎么解决？</p>
<h2 id="references">references</h2>
<p>《Adaptive Co-Attention Network for Named Entity Recognition in Tweets》</p>
<p>《Visual Attention Model for Name Tagging in Multimodal Social Media》</p>
<p>《Multimodal Named Entity Recognition for Short Social Media Posts》</p>
<p>《Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer》</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/Multimodal-NER/" rel="tag"># Multimodal NER</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/10/19/NLP-Cross-Lingual-NER/" rel="prev" title="NLP|Cross-Lingual NER">
                  <i class="fa fa-chevron-left"></i> NLP|Cross-Lingual NER
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/" rel="next" title="NLP|RE-DocRED and SCIREX Dataset">
                  NLP|RE-DocRED and SCIREX Dataset <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zichao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  


















  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"live2d-widget-model-hijiki"},"display":{"position":"right","width":225,"height":450},"mobile":{"show":false}});</script></body>
</html>
