<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>C++|STL库总结</title>
    <url>/2020/02/17/C-STL%E5%BA%93%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>一直在用C++刷题，STL库用的很频繁，但是总是记不全，所以写一篇blog来总结一下。</p>
<a id="more"></a>
<h4 id="vector">vector</h4>
<p>Vector实际上就是长度可变的数组。</p>
<blockquote>
<p><code>vector&lt;int&gt; vec</code>:定义了vector类型的变量vec，其中的元素为int类型。实际上相当于定义了长度可变的一位数组。</p>
<p><code>vector&lt;vector&lt;int&gt; &gt; vec</code>:定义了vector类型的变量vec，其中的元素为vector类型。实际上相当于定义了长度可变的二维数组。其中元素的长度也是可变的。</p>
</blockquote>
<h5 id="常用函数">常用函数</h5>
<p>首先定义一个vector对象:<code>vector&lt;int&gt; vec</code>。 常用函数如下：</p>
<p><strong>vec.push_back(a)</strong> :将元素加入vec中。</p>
<p><strong>vec.pop_back()</strong></p>
<p><strong>vec.insert</strong>:</p>
<hr>
<ol type="1">
<li><code>set</code>基于红黑树实现，红黑树具有<strong>自动排序的功能</strong>，因此map内部所有的数据，在任何时候，都是有序的。</li>
<li><code>unordered_set</code>基于哈希表，数据插入和查找的时间复杂度很低，几乎是常数时间，而代价是消耗比较多的内存，<strong>无自动排序功能</strong>。底层实现上，使用一个下标范围比较大的数组来存储元素，形成很多的桶，利用<code>hash</code>函数对<code>key</code>进行映射到不同区域进行保存。</li>
</ol>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>STL库</tag>
      </tags>
  </entry>
  <entry>
    <title>C++|十大排序算法C++实现</title>
    <url>/2020/03/10/C-%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95C-%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>十大排序算法实在是太过于经典了，因此，这篇博客将对十大排序算法进行总结，以C++实现为主。</p>
<a id="more"></a>
<h2 id="快速排序">快速排序</h2>
<p><strong>平均时间复杂度为：O(nlogn)；空间复杂度为O(nlogn)。</strong>不稳定排序。n大的时候，性能更加优越。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//划分的过程，因为快排是分治思想的，所以需要划分。</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp; vec,<span class="keyword">int</span> start,<span class="keyword">int</span> <span class="built_in">end</span>)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> <span class="built_in">point</span>=vec[start];</span><br><span class="line">	<span class="keyword">int</span> i=start;</span><br><span class="line">	<span class="keyword">int</span> j=<span class="built_in">end</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span>(i&lt;j)&#123;</span><br><span class="line">		<span class="keyword">while</span>(i&lt;j &amp;&amp; vec[j]&gt;=<span class="built_in">point</span>) j--;</span><br><span class="line">		vec[i]=vec[j];</span><br><span class="line">		<span class="keyword">while</span>(i&lt;j &amp;&amp; vec[i]&lt;<span class="built_in">point</span>) i++;</span><br><span class="line">		vec[j]=vec[i];</span><br><span class="line">	&#125;	</span><br><span class="line">	vec[i]=<span class="built_in">point</span>;</span><br><span class="line">	<span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//快排的结构</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quick_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vec,<span class="keyword">int</span> start,<span class="keyword">int</span> <span class="built_in">end</span>)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(start&gt;<span class="built_in">end</span>) <span class="keyword">return</span>;</span><br><span class="line">	<span class="keyword">int</span> j=partition(vec,start,<span class="built_in">end</span>);</span><br><span class="line">	quick_sort(vec,start,j<span class="number">-1</span>);</span><br><span class="line">	quick_sort(vec,j+<span class="number">1</span>,<span class="built_in">end</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	quick_sort(vec,<span class="number">0</span>,n<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="冒泡排序">冒泡排序</h2>
<p><strong>平均时间复杂度为：O(n^2)；空间复杂度为O(1)。</strong>稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubble_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;  &amp;vec)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> <span class="built_in">size</span>=vec.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">size</span><span class="number">-1</span>;i++)&#123;<span class="comment">//这一轮循环是要比较的元素，也就睡还没有排好序的元素，最初有n个，但是其实只要比较n-1个元素就可以，因为最后一个元素不用比较。</span></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="built_in">size</span>-i<span class="number">-1</span>;j++)&#123;</span><br><span class="line">			<span class="keyword">if</span>(vec[j]&gt;vec[j+<span class="number">1</span>]) swap(vec[j],vec[j+<span class="number">1</span>]);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	bubble_sort(vec);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt;vec[i]&lt;&lt;<span class="string">" "</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="归并排序">归并排序</h2>
<p><strong>平均时间复杂度为：O(nlogn)；空间复杂度为O(nlogn)。</strong>稳定排序。n大的时候，性能更加优越。</p>
<p><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp; vec,<span class="keyword">int</span> low,<span class="keyword">int</span> mid,<span class="keyword">int</span> high)</span></span>&#123;</span><br><span class="line">	<span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">temp</span><span class="params">(high-low+<span class="number">1</span>,<span class="number">0</span>)</span></span>;</span><br><span class="line">	<span class="keyword">int</span> left_low=low;</span><br><span class="line">	<span class="keyword">int</span> left_high=mid;</span><br><span class="line">	<span class="keyword">int</span> right_low=mid+<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">int</span> right_high=high;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;left_low&lt;=left_high &amp;&amp; right_low&lt;=right_high;i++)&#123;</span><br><span class="line">		<span class="keyword">if</span>(vec[left_low]&lt;=vec[right_low])&#123;</span><br><span class="line">			temp[i]=vec[left_low];</span><br><span class="line">			left_low++;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">else</span>&#123;</span><br><span class="line">			temp[i]=vec[right_low];</span><br><span class="line">			right_low++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span>(left_low&lt;=left_high)&#123;</span><br><span class="line">		<span class="keyword">while</span>(left_low&lt;=left_high)&#123;</span><br><span class="line">			temp[i]=vec[left_low];</span><br><span class="line">			i++;</span><br><span class="line">			left_low++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span>(right_low&lt;=right_high)&#123;</span><br><span class="line">		<span class="keyword">while</span>(right_low&lt;=right_high)&#123;</span><br><span class="line">			temp[i]=vec[right_low];</span><br><span class="line">			i++;</span><br><span class="line">			right_low++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;(high-low+<span class="number">1</span>);j++)&#123;</span><br><span class="line">		vec[low+j]=temp[j];</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vec,<span class="keyword">int</span> start,<span class="keyword">int</span> <span class="built_in">end</span>)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(start&gt;=<span class="built_in">end</span>) <span class="keyword">return</span>;</span><br><span class="line">	<span class="keyword">int</span> mid=start+(<span class="built_in">end</span>-start)/<span class="number">2</span>;</span><br><span class="line">	merge_sort(vec,start,mid);</span><br><span class="line">	merge_sort(vec,mid+<span class="number">1</span>,<span class="built_in">end</span>);</span><br><span class="line">	merge(vec,start,mid,<span class="built_in">end</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	merge_sort(vec,<span class="number">0</span>,n<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="堆排序">堆排序</h2>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span>  <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//要建成最小堆，首先是要建成最大堆，然后调整为最小堆！</span></span><br><span class="line"><span class="comment">//调整成最大堆</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">adjust_heap</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vec,<span class="keyword">int</span> <span class="built_in">size</span>,<span class="keyword">int</span> index)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> left=<span class="number">2</span>*index+<span class="number">1</span>;</span><br><span class="line">	<span class="keyword">int</span> right=<span class="number">2</span>*index+<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> max_index=index;</span><br><span class="line">	<span class="keyword">if</span>(left&lt;<span class="built_in">size</span> &amp;&amp; vec[left]&gt;vec[max_index]) max_index=left;</span><br><span class="line">	<span class="keyword">if</span>(right&lt;<span class="built_in">size</span> &amp;&amp; vec[right]&gt;vec[max_index]) max_index=right;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span>(max_index!=index)&#123;</span><br><span class="line">		swap(vec[max_index],vec[index]);</span><br><span class="line">		adjust_heap(vec,<span class="built_in">size</span>,max_index);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heap_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vec,<span class="keyword">int</span> <span class="built_in">size</span>)</span></span>&#123;</span><br><span class="line">  <span class="comment">//建立堆</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="built_in">size</span>/<span class="number">2</span><span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)&#123;</span><br><span class="line">		adjust_heap(vec,<span class="built_in">size</span>,i);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">  	<span class="comment">//调整为最小堆</span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="built_in">size</span><span class="number">-1</span>;i&gt;=<span class="number">1</span>;i--)&#123;</span><br><span class="line">		swap(vec[<span class="number">0</span>],vec[i]);</span><br><span class="line">		adjust_heap(vec,i,<span class="number">0</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	heap_sort(vec,vec.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="计数排序">计数排序</h2>
<p><strong>平均时间复杂度为：O(n+k)；空间复杂度为O(n+k)。</strong>稳定排序，其中，k表示序列中最大值。所以，计数排序使用与最大值不是很大的序列。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">count_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp; vec)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(vec.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span> ;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> maxvalue=<span class="number">0</span>,minvalue=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;vec.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">		maxvalue=<span class="built_in">max</span>(maxvalue,vec[i]);</span><br><span class="line">		minvalue=<span class="built_in">min</span>(minvalue,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> range=maxvalue-minvalue+<span class="number">1</span>;</span><br><span class="line">	<span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">temp</span><span class="params">(range,<span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;vec.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">		temp[vec[i]-minvalue]+=<span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span> j=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;range;i++)&#123;</span><br><span class="line">		<span class="keyword">while</span>(temp[i]!=<span class="number">0</span>)&#123;</span><br><span class="line">			vec[j]=i+minvalue;</span><br><span class="line">			temp[i]--;</span><br><span class="line">			j++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	count_sort(vec);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="选择排序">选择排序</h2>
<p><strong>平均时间复杂度为：O(n^2)；空间复杂度为O(1)。</strong>不稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//选择排序与冒泡排序非常的类似，选择排序是选择一个最小的放在序列前面，而冒泡排序是选择一个最大的元素放在序列后面。</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">select_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vec)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;vec.<span class="built_in">size</span>()<span class="number">-1</span>;i++)&#123;<span class="comment">//同样，只用比较n-1个元素</span></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=i+<span class="number">1</span>;j&lt;vec.<span class="built_in">size</span>();j++)&#123;</span><br><span class="line">			<span class="keyword">if</span>(vec[j]&lt;vec[i]) swap(vec[j],vec[i]);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	select_sort(vec);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="插入排序">插入排序</h2>
<p><strong>平均时间复杂度为：O(n^2)；空间复杂度为O(1)。</strong>稳定排序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;vec)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(vec.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;vec.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> temp=vec[i];</span><br><span class="line">		<span class="keyword">int</span> j=i;</span><br><span class="line">		<span class="keyword">while</span>(j&gt;<span class="number">0</span> &amp;&amp; temp&lt;vec[j<span class="number">-1</span>])&#123;</span><br><span class="line">			vec[j]=vec[j<span class="number">-1</span>];</span><br><span class="line">			j--;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		vec[j]=temp;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span> &gt; vec;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		vec.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	insert_sort(vec);</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,vec[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="各大算法时间与空间复杂度对比">各大算法时间与空间复杂度对比</h2>
<p><img src="/2020/03/10/C-%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95C-%E5%AE%9E%E7%8E%B0/0.jpg"></p>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>C++|拓扑排序</title>
    <url>/2020/10/02/C-%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<p>好久没刷题了，今天刷刷题，保持一下手感，同时也对碰到的很多好久不用的基础的数据结构做一下回顾。</p>
<a id="more"></a>
<h2 id="拓扑排序">拓扑排序</h2>
<p><strong>定义：</strong>对一个有向无环图(Directed Acyclic Graph, DAG)G进行拓扑排序，是将G中所有顶点排成一个线性序列，使得图中任意一对顶点u和v，若&lt;u，v&gt; ∈E(G)，则u在线性序列中出现在v之前。通常，这样的线性序列称为满足拓扑次序(Topological Order)的序列，简称<strong>拓扑序列</strong>。</p>
<p>也就是说：<strong>只有DAG才有拓扑排序，反过来说，如果一个有向图没有拓扑排序，说明这个图是非DAG的，逆否命题当然成立。</strong>一般来说，拓扑排序用来解决图的是否有环的问题。</p>
<p><strong>拓扑排序的步骤是：</strong></p>
<ul>
<li><p>从有向图中选择所有入度为0的node，然后输出；</p></li>
<li><p>从有向图中删除该节点以及以这个node为尾的边(即所有指向该node的边)</p></li>
<li><p>重复上面两步，知道最后图为空（是DAG，可以输出拓扑排序序列），或者图不为空但是无法找出入度为0的node（非DAG）。</p></li>
</ul>
<p>下面有两个例子，来自leetcode。</p>
<h2 id="课程表">课程表</h2>
<p>你这个学期必须选修 <code>numCourse</code> 门课程，记为 <code>0</code> 到 <code>numCourse-1</code> 。</p>
<p>在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们：<code>[0,1]</code></p>
<p>给定课程总量以及它们的先决条件，请你判断是否可能完成所有课程的学习？</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: 2, [[1,0]] </span><br><span class="line">输出: true</span><br><span class="line">解释: 总共有 2 门课程。学习课程 1 之前，你需要完成课程 0。所以这是可能的。</span><br></pre></td></tr></table></figure>
<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: 2, [[1,0],[0,1]]</span><br><span class="line">输出: false</span><br><span class="line">解释: 总共有 2 门课程。学习课程 1 之前，你需要先完成课程 0；并且学习课程 0 之前，你还应先完成课程 1。这是不可能的。</span><br></pre></td></tr></table></figure>
<p><strong>提示：</strong></p>
<ol type="1">
<li>输入的先决条件是由 <strong>边缘列表</strong> 表示的图形，而不是 邻接矩阵 。详情请参见<a href="http://blog.csdn.net/woaidapaopao/article/details/51732947" target="_blank" rel="noopener">图的表示法</a>。</li>
<li>你可以假定输入的先决条件中没有重复的边。</li>
<li><code>1 &lt;= numCourses &lt;= 10^5</code></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canFinish</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(numCourses==<span class="number">0</span> || prerequisites.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">indegree</span><span class="params">(numCourses,<span class="number">0</span>)</span></span>;<span class="comment">//入度表</span></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; <span class="title">graph</span><span class="params">(numCourses,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;())</span></span>; <span class="comment">//有向图的邻接矩阵，记得要初始化，至少要初始化其个数</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;prerequisites.<span class="built_in">size</span>();i++)&#123;<span class="comment">//构建邻接矩阵</span></span><br><span class="line">            graph[prerequisites[i][<span class="number">1</span>]].push_back(prerequisites[i][<span class="number">0</span>]);</span><br><span class="line">            indegree[prerequisites[i][<span class="number">0</span>]]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;<span class="comment">//队列，BFS，其实拓扑排序还可以用DFS，但是没有BFS这个好理解，感兴趣的可以自己百度/Google搜一下</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;indegree.<span class="built_in">size</span>();i++)&#123;<span class="comment">//将入度为0的node加入队列中</span></span><br><span class="line">            <span class="keyword">if</span>(indegree[i]==<span class="number">0</span>) q.push(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> cnt=<span class="number">0</span>;<span class="comment">//统计入度为0的节点数</span></span><br><span class="line">        <span class="keyword">while</span>(q.empty()==<span class="literal">false</span>)&#123;</span><br><span class="line">            <span class="keyword">int</span> temp=q.front();</span><br><span class="line">            q.pop();<span class="comment">//删除节点</span></span><br><span class="line">            cnt++;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;graph[temp].<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">                indegree[graph[temp][i]]--;<span class="comment">//删除相应的边</span></span><br><span class="line">                <span class="keyword">if</span>(indegree[graph[temp][i]]==<span class="number">0</span>)&#123;</span><br><span class="line">                    q.push(graph[temp][i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cnt==numCourses;<span class="comment">//如果与最初的节点数相同，说明无环，如果不相同，说明有环。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="课程表2">课程表2</h2>
<p>现在你总共有 n 门课需要选，记为 0 到 n-1。</p>
<p>在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们: [0,1]</p>
<p>给定课程总量以及它们的先决条件，返回你为了学完所有课程所安排的学习顺序。</p>
<p>可能会有多个正确的顺序，你只要返回一种就可以了。如果不可能完成所有课程，返回一个空数组。</p>
<p>示例 1:</p>
<p>​ 输入: 2, [[1,0]] ​ 输出: [0,1] ​ 解释: 总共有 2 门课程。要学习课程 1，你需要先完成课程 0。因此，正确的课程顺序为 [0,1] 。 示例 2:</p>
<p>​ 输入: 4, [[1,0],[2,0],[3,1],[3,2]] ​ 输出: [0,1,2,3] or [0,2,1,3] ​ 解释: 总共有 4 门课程。要学习课程 3，你应该先完成课程 1 和课程 2。并且课程 1 和课程 2 都应该排在课程 0 之后。 ​ 因此，一个正确的课程顺序是 [0,1,2,3] 。另一个正确的排序是 [0,2,1,3] 。 说明:</p>
<p>输入的先决条件是由边缘列表表示的图形，而不是邻接矩阵。详情请参见图的表示法。 你可以假定输入的先决条件中没有重复的边。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">findOrder</span><span class="params">(<span class="keyword">int</span> numCourses, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; prerequisites)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(numCourses==<span class="number">0</span>) <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">indegree</span><span class="params">(numCourses,<span class="number">0</span>)</span></span>;</span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; <span class="title">graph</span><span class="params">(numCourses,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;())</span></span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;prerequisites.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">            graph[prerequisites[i][<span class="number">1</span>]].push_back(prerequisites[i][<span class="number">0</span>]);</span><br><span class="line">            indegree[prerequisites[i][<span class="number">0</span>]]++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">        <span class="keyword">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;indegree.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(indegree[i]==<span class="number">0</span>)&#123;</span><br><span class="line">                q.push(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(q.empty()==<span class="literal">false</span>)&#123;</span><br><span class="line">            <span class="keyword">int</span> temp=q.front();</span><br><span class="line">            q.pop();</span><br><span class="line">            cnt++;</span><br><span class="line">            ans.push_back(temp);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;graph[temp].<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">                indegree[graph[temp][i]]--;</span><br><span class="line">                <span class="keyword">if</span>(indegree[graph[temp][i]]==<span class="number">0</span>)&#123;</span><br><span class="line">                    q.push(graph[temp][i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(cnt==numCourses) <span class="keyword">return</span> ans; <span class="comment">//要做一个判断，判断输出的节点数与最开始的节点数是否相同，相同则输出，不相同说明有环，则不输出。</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>over～☕️</p>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>拓扑排序</tag>
      </tags>
  </entry>
  <entry>
    <title>C++|滑动窗口专题</title>
    <url>/2020/02/22/C-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E4%B8%93%E9%A2%98/</url>
    <content><![CDATA[<p>最近在整理之前刷过的leetcode题目，想写写一些常见的、典型的解题技巧或者方法。最开始刷题的时候，总觉得题目好难，其实刷多了题，也就那么回事😆。这篇博客主要想介绍一下滑动窗口方法(sliding window)，对于解决字符串问题非常有用🤩～</p>
<a id="more"></a>
<blockquote>
<p>这一篇解答写的非常的透彻，详细请参看链接：<a href="https://leetcode-cn.com/problems/minimum-window-substring/solution/hua-dong-chuang-kou-suan-fa-tong-yong-si-xiang-by-/" target="_blank" rel="noopener">滑动窗口思想</a></p>
</blockquote>
<p><strong>滑动窗口</strong>，主要是用来解决子串问题。一般的解决思路是：<strong>首先，我们定义left与right两个标志，用来界定滑动窗口的左右边界；接着，我们不断地移动右边界，即right++；当满足某一条件之后，我们不断地移动左边界，即left++，缩减滑动窗口的大小，直到满足最后的条件，然后返回结果。</strong>下面以leetcode上的三道题为例：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">3.无重复字符的最长子串</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">示例 1:</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">输入: "abcabcbb"</span></span><br><span class="line"><span class="comment">输出: 3 </span></span><br><span class="line"><span class="comment">解释: 因为无重复字符的最长子串是 "abc"，所以其长度为 3。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">滑动窗口的思想。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(s.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> left=<span class="number">0</span>,right=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; m;</span><br><span class="line">        <span class="keyword">int</span> maxsize=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(right&lt;s.<span class="built_in">size</span>())&#123;</span><br><span class="line">            <span class="keyword">char</span> c1=s[right];</span><br><span class="line">            m[c1]++;</span><br><span class="line">            right++;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(m[c1]&gt;<span class="number">1</span>)&#123;</span><br><span class="line">                <span class="keyword">char</span> c2=s[left];</span><br><span class="line">                m[c2]--;</span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            maxsize=<span class="built_in">max</span>(maxsize,right-left);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>  maxsize;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="built_in">string</span> s;</span><br><span class="line">	<span class="built_in">cin</span>&gt;&gt;s;</span><br><span class="line">	Solution so1;</span><br><span class="line">	<span class="keyword">int</span> ans=so1.lengthOfLongestSubstring(s);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"%d\n"</span>,ans);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">76. 最小覆盖子串</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">给你一个字符串 S、一个字符串 T，请在字符串 S 里面找出：包含 T 所有字母的最小子串。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">示例：</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">输入: S = "ADOBECODEBANC", T = "ABC"</span></span><br><span class="line"><span class="comment">输出: "BANC"</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>&#123;</span></span><br><span class="line">	<span class="keyword">public</span>:</span><br><span class="line">	<span class="comment">/*</span></span><br><span class="line"><span class="comment">	滑动窗口思想。碰到字串问题，第一时间就应该想到滑动窗口。</span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line">	<span class="function"><span class="built_in">string</span> <span class="title">minWindow</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">string</span> t)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">if</span>(t.<span class="built_in">size</span>()==<span class="number">0</span> || s.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">		<span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; <span class="keyword">m_t</span>;</span><br><span class="line">		<span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; window;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;t.<span class="built_in">size</span>();i++) <span class="keyword">m_t</span>[t[i]]++;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">int</span> left=<span class="number">0</span>,right=<span class="number">0</span>;<span class="comment">//双指针，滑动窗口的左右位置</span></span><br><span class="line">		<span class="keyword">int</span> start=<span class="number">0</span>,minlen=INT_MAX;<span class="comment">//start表示最后的字符串的开始的位置,minlen表示最后的字符串的长度</span></span><br><span class="line">		<span class="keyword">int</span> match=<span class="number">0</span>;<span class="comment">//窗口内与目标字符串匹配的数目</span></span><br><span class="line"></span><br><span class="line">		<span class="comment">//移动right</span></span><br><span class="line">		<span class="keyword">while</span>(right&lt;s.<span class="built_in">size</span>())&#123;</span><br><span class="line">			<span class="keyword">char</span> c1=s[right];</span><br><span class="line">			<span class="keyword">if</span>(<span class="keyword">m_t</span>.count(c1))&#123;</span><br><span class="line">				window[c1]++;</span><br><span class="line">				<span class="keyword">if</span>(window[c1]==<span class="keyword">m_t</span>[c1])&#123;</span><br><span class="line">					match++;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			right++;</span><br><span class="line"></span><br><span class="line">			<span class="comment">//如果已经匹配了，说明窗口内的字符串包含了目标字符串，接下来，就需要缩减窗口大小，移动left，找到包含目标字符串的最小子串</span></span><br><span class="line">			<span class="keyword">while</span>(match==<span class="keyword">m_t</span>.<span class="built_in">size</span>())&#123;</span><br><span class="line">				<span class="keyword">char</span> c2=s[left];</span><br><span class="line">				<span class="keyword">if</span>(right-left&lt;minlen)&#123;</span><br><span class="line">                    start=left;</span><br><span class="line">                    minlen=right-left;</span><br><span class="line">                &#125;</span><br><span class="line">				<span class="keyword">if</span>(<span class="keyword">m_t</span>.count(c2))&#123;</span><br><span class="line">					window[c2]--;</span><br><span class="line">					<span class="keyword">if</span>(window[c2]&lt;<span class="keyword">m_t</span>[c2])&#123;</span><br><span class="line">						match--;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				left++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> minlen==INT_MAX?<span class="string">""</span>:s.substr(start,minlen);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="built_in">string</span> s,t;</span><br><span class="line">	<span class="built_in">cin</span>&gt;&gt;s&gt;&gt;t;</span><br><span class="line"></span><br><span class="line">	Solution so1;</span><br><span class="line">	<span class="built_in">string</span> ans=so1.minWindow(s,t);</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;ans&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">438. 找到字符串中所有字母异位词</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">给定一个字符串 s 和一个非空字符串 p，找到 s 中所有是 p 的字母异位词的子串，返回这些子串的起始索引。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">字符串只包含小写英文字母，并且字符串 s 和 p 的长度都不超过 20100。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">说明：</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">字母异位词指字母相同，但排列不同的字符串。</span></span><br><span class="line"><span class="comment">不考虑答案输出的顺序。</span></span><br><span class="line"><span class="comment">示例 1:</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">输入:</span></span><br><span class="line"><span class="comment">s: "cbaebabacd" p: "abc"</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">输出:</span></span><br><span class="line"><span class="comment">[0, 6]</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">解释:</span></span><br><span class="line"><span class="comment">起始索引等于 0 的子串是 "cba", 它是 "abc" 的字母异位词。</span></span><br><span class="line"><span class="comment">起始索引等于 6 的子串是 "bac", 它是 "abc" 的字母异位词。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>&#123;</span></span><br><span class="line">	<span class="keyword">public</span>:</span><br><span class="line">	<span class="comment">/*</span></span><br><span class="line"><span class="comment">	滑动窗口思想。https://leetcode-cn.com/problems/minimum-window-substring/solution/hua-dong-chuang-kou-suan-fa-tong-yong-si-xiang-by-/</span></span><br><span class="line"><span class="comment">	*/</span></span><br><span class="line">	<span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">findAnagrams</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">string</span> p)</span> </span>&#123;</span><br><span class="line">		<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans;</span><br><span class="line">		<span class="keyword">if</span>(s.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span> ans;</span><br><span class="line"></span><br><span class="line">		<span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; m_p;</span><br><span class="line">		<span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt; window;</span><br><span class="line">		<span class="keyword">int</span> left=<span class="number">0</span>,right=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">int</span> match=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;p.<span class="built_in">size</span>();i++) m_p[p[i]]++;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span>(right&lt;s.<span class="built_in">size</span>())&#123;</span><br><span class="line">			<span class="keyword">char</span> c1=s[right];</span><br><span class="line">			<span class="keyword">if</span>(m_p.count(c1))&#123;</span><br><span class="line">				window[c1]++;</span><br><span class="line">				<span class="keyword">if</span>(window[c1]==m_p[c1])&#123;</span><br><span class="line">					match++;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			right++;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">while</span>(match==m_p.<span class="built_in">size</span>())&#123;</span><br><span class="line">				<span class="keyword">char</span> c2=s[left];</span><br><span class="line">				<span class="keyword">if</span>(right-left==p.<span class="built_in">size</span>())&#123;</span><br><span class="line">					ans.push_back(left);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">if</span>(m_p.count(c2))&#123;</span><br><span class="line">					window[c2]--;</span><br><span class="line">					<span class="keyword">if</span>(window[c2]&lt;m_p[c2])&#123;</span><br><span class="line">						match--;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">				left++;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> ans;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="built_in">string</span> s,p;</span><br><span class="line">	<span class="built_in">cin</span>&gt;&gt;s&gt;&gt;p;</span><br><span class="line">	Solution so1;</span><br><span class="line"></span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ans=so1.findAnagrams(s,p);</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;ans.<span class="built_in">size</span>();i++)&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"%d "</span>,ans[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>具体代码应该很清楚啦，over☕️～</p>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>leetcode</tag>
        <tag>滑动窗口</tag>
      </tags>
  </entry>
  <entry>
    <title>C++|一些知识点总结</title>
    <url>/2020/02/17/C-%EF%BD%9C%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>最近在看《剑指offer》，也一直在用C++刷题，总是会遇到一些易混的地方，记录一下，以便以后翻阅。</p>
<a id="more"></a>
<p>###this指针</p>
<blockquote>
<p>参考链接：</p>
<p>https://blog.csdn.net/xiaohaijiejie/article/details/51787351</p>
<p>https://blog.csdn.net/qq_40354578/article/details/88088048</p>
</blockquote>
<p><strong>this指针是指向实例本身，主要在运行对象自身的函数对函数数据成员重名的情况有用。</strong></p>
<p>下面我们设置重名的函数数据成员。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Array</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Array(<span class="keyword">int</span> len)<span class="comment">//有参构造函数</span></span><br><span class="line">	&#123;</span><br><span class="line">		len=len;</span><br><span class="line">	&#125;</span><br><span class="line">	~Array()<span class="comment">//析构函数</span></span><br><span class="line">	&#123;</span><br><span class="line"> </span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">setLen</span><span class="params">(<span class="keyword">int</span> len)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		len=len;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">getLen</span><span class="params">()</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> len;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">printInfo</span><span class="params">()</span><span class="comment">//打印值</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt;<span class="string">"len="</span>&lt;&lt;len&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">	<span class="keyword">int</span> len;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">Array <span class="title">arr1</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">	arr1.printInfo();</span><br><span class="line">	system(<span class="string">"pause"</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果如下:</p>
<p><img src="/2020/02/17/C-%EF%BD%9C%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/1.jpg"></p>
<p>我们可以发现结果错误。接下来，我们使用this指针。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Array</span>&#123;</span></span><br><span class="line">	<span class="keyword">private</span>:</span><br><span class="line">	<span class="keyword">int</span> len;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span>:</span><br><span class="line">	Array(<span class="keyword">int</span> len)&#123;</span><br><span class="line">		<span class="keyword">this</span>-&gt;len=len;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	~Array()&#123;&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">setLen</span><span class="params">(<span class="keyword">int</span> len)</span></span>&#123;</span><br><span class="line">		<span class="keyword">this</span>-&gt;len=len;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">getLen</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">this</span>-&gt;len;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">void</span> <span class="title">PrintInfo</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="built_in">cout</span>&lt;&lt;<span class="string">"len:"</span>&lt;&lt;<span class="keyword">this</span>-&gt;len&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="function">Array <span class="title">a</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">	<span class="keyword">int</span> b =a.getLen();</span><br><span class="line">	a.PrintInfo();</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="/2020/02/17/C-%EF%BD%9C%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/2.jpg"></p>
<p>运行成功。</p>
<p>###const的使用</p>
<h5 id="const修饰普通变量">const修饰普通变量</h5>
<ul>
<li>const修饰普通变量的时候，必须初始化；</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> a=<span class="number">14</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>不能通过修改常量的引用来修改绑定的对象；</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i=<span class="number">42</span>;</span><br><span class="line"><span class="keyword">int</span> &amp;r1=i;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;r2=i;<span class="comment">//r2也绑定了对象i,但不允许通过r2修改i的值</span></span><br><span class="line">r1=<span class="number">0</span>;           <span class="comment">//正确</span></span><br><span class="line">r2=<span class="number">0</span>;           <span class="comment">//错误，r2是一个常量引用</span></span><br></pre></td></tr></table></figure>
<ul>
<li>不能将非常量引用指向一个常量对象；</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i=<span class="number">42</span>;</span><br><span class="line"><span class="keyword">int</span> &amp;r1=i;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;r2=i;<span class="comment">//r2也绑定了对象i,但不允许通过r2修改i的值</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> ci=<span class="number">1024</span>;</span><br><span class="line"><span class="keyword">int</span> &amp;r2=ci;<span class="comment">//错误</span></span><br></pre></td></tr></table></figure>
<h5 id="const修饰函数的返回类型">const修饰函数的返回类型</h5>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">const</span> <span class="keyword">void</span> <span class="title">func</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="keyword">const</span> <span class="title">func</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">//两种写法等价</span></span><br></pre></td></tr></table></figure>
<h5 id="const修饰函数的形参变量">const修饰函数的形参变量</h5>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">int</span> <span class="keyword">const</span> val)</span></span>;</span><br><span class="line"><span class="comment">//两种写法等价</span></span><br></pre></td></tr></table></figure>
<p><strong>值得注意的是，</strong>有时候会遇到这种情况：<code>void func(const int &amp;val)</code>.const被用来修饰函数参数，它的作用是：防止传入的参数内容在函数体内被改变，但是仅对指针和引用有用。因为如果是按值传递，传给参数的仅仅是实参的副本，即使在函数体内改变了形参，实参也不会得到影响。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> i)</span></span>&#123;</span><br><span class="line">	i = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在函数体中，是不能改变i的值的。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">h</span><span class="params">(<span class="keyword">const</span> A &amp; a)</span></span>&#123;</span><br><span class="line">…………</span><br><span class="line">…………</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>传递进来的参数a是实参对象的副本，要调用构造函数来构造这个副本，而且函数结束后要调用析构函数来释放这个副本，在空间和时间上都造成了浪费，所以函数参数为类对象的情况，推荐用引用。但按引用传递，造成了安全隐患，通过函数参数的引用可以修改实参的内部数据成员，所以用const来保护实参。</p>
<p>###&amp;引用</p>
<p>在函数参数中，使用&amp;会避免大量的数据开销，提高性能。因为对引用的修改，就是对原值的修改。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">//交换两个值 </span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span> <span class="params">(<span class="keyword">int</span>&amp; x, <span class="keyword">int</span>&amp; y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    temp = x;</span><br><span class="line">    x= y;</span><br><span class="line">    y =temp;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> b = <span class="number">2</span>;</span><br><span class="line">    swap(a,b);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt;a &lt;&lt;<span class="string">"   "</span> &lt;&lt;b&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(<span class="keyword">int</span> val)</span><span class="comment">//按值传递参数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    val = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">(<span class="keyword">int</span>&amp; val)</span><span class="comment">//按引用传递参数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    val = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">zoo</span><span class="params">(<span class="keyword">int</span> *pval)</span><span class="comment">//按指针传递参数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    *pval = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> b = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> c = <span class="number">1</span>;</span><br><span class="line">    foo(a);</span><br><span class="line">    bar(b);</span><br><span class="line">    zoo(&amp;c);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt;a &lt;&lt;<span class="string">"   "</span> &lt;&lt;b&lt;&lt;<span class="string">"   "</span>&lt;&lt;c &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="/2020/02/17/C-%EF%BD%9C%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/3.jpg"></p>
<p>分析：</p>
<ul>
<li><p>foo函数是按值传递，执行foo函数的执行，相当于对a的副本进行修改。在调用完foo函数后，a的值还是1，不会改变。</p></li>
<li><p>bar中value的传递是引用，对引用的修改和对原值修改是一样的。</p></li>
<li><p>zoo中value的传递是指针，指针对值得修改，原值也会修改。</p></li>
</ul>
<h3 id="stringstream">stringstream</h3>
<p>####1.声明一个stringstream对象</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="built_in">stringstream</span> ss;</span><br></pre></td></tr></table></figure>
<h4 id="常用函数">2.常用函数</h4>
<h5 id="ss.str">2.1.ss.str()</h5>
<p>str()函数的作用是：将ss中的对象转换成字符串输出。<code>ss.str("")</code>这段代码表示的是将ss流清零。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">stringstream</span> ss;</span><br><span class="line">ss &lt;&lt; <span class="string">"hello "</span>;</span><br><span class="line">ss &lt;&lt; <span class="string">"world!"</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;ss.str()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此外，stringstream最值得注意的是：<strong>从stringstream流中的数据输入字符串到一个变量里，是以遇到空格跳到下一个字符串的这样的形式连续读取的</strong>。这在刷题过程中，会体现在使用循环的过程中。如下：</p>
<h4 id="翻转字符串里的单词"><a href="https://leetcode-cn.com/problems/reverse-words-in-a-string/" target="_blank" rel="noopener">151. 翻转字符串里的单词</a></h4>
<p><br></p>
<p>给定一个字符串，逐个翻转字符串中的每个单词。</p>
<p>示例 1：</p>
<blockquote>
<p>输入: "the sky is blue" 输出: "blue is sky the"</p>
</blockquote>
<p>示例 2：</p>
<blockquote>
<p>输入: " hello world! " 输出: "world! hello" 解释: 输入字符串可以在前面或者后面包含多余的空格，但是反转后的字符不能包括。</p>
</blockquote>
<p>示例 3：</p>
<blockquote>
<p>输入: "a good example" 输出: "example good a" 解释: 如果两个单词间有多余的空格，将反转后单词间的空格减少到只含一个。</p>
</blockquote>
<p>AC代码：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>&#123;</span></span><br><span class="line">	<span class="keyword">public</span>:</span><br><span class="line">	<span class="function"><span class="built_in">string</span> <span class="title">reverseWords</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">		<span class="built_in">stringstream</span> ss;</span><br><span class="line">		ss&lt;&lt;s;</span><br><span class="line">		<span class="built_in">string</span> ans=<span class="string">""</span>;</span><br><span class="line">		<span class="built_in">string</span> temp;</span><br><span class="line">		<span class="keyword">while</span>(ss&gt;&gt;temp)&#123;</span><br><span class="line">			ans=<span class="string">" "</span>+temp+ans;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span>(ans!=<span class="string">""</span>)&#123;</span><br><span class="line">			ans.erase(ans.<span class="built_in">begin</span>());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> ans;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="built_in">string</span> s;</span><br><span class="line">	getline(<span class="built_in">cin</span>,s);<span class="comment">//cin遇到空格就会终止输入，索引需要使用getline函数。</span></span><br><span class="line">	Solution so1;</span><br><span class="line">	<span class="built_in">string</span> ans=so1.reverseWords(s);</span><br><span class="line">	<span class="built_in">cout</span>&lt;&lt;ans&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<p><img src="/2020/02/17/C-%EF%BD%9C%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/4.jpg"></p>
<p>over☕️～</p>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>stringstram</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|ELECTRA预训练模型</title>
    <url>/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>最近在看GAN相关的内容的时候，看到一篇关于预训练模型的新作：ELECTRA，核心是将GAN的思想引入到NLP中，非常地新颖。所以这篇文章将具体地讲解一下关于ELECTRA的内容，我个人是很喜欢这样精巧且优雅的文章的～🤩</p>
<a id="more"></a>
<p>ELECTRA模型是今年google在ICLR上放出的一篇论文。它的核心思想是：将GAN引入到NLP中，但是它所使用的Generator与Discriminator与GAN中的会稍微不同，通过设计新的预训练任务，从而大幅提升BERT的训练效率，节省算力。下面详细介绍一下～</p>
<h2 id="electra要解决的问题">ELECTRA要解决的问题</h2>
<p>BERT已经被证明效果在多个自然语言处理任务中表现良好，但是BERT也存在非常大的问题：需要大量的训练数据与算力。导致这一问题的原因在于：MLM任务。在MLM任务中，我们是去MASK掉15%的tokens，再利用上下文来预测被MASK掉的tokens，这样的话，只有15%的tokens被用来训练，导致了BERT的训练非常的慢，并且也需要大量的训练数据才能将BERT训练的非常好。此外，MASK标签，也会使得也正是这个原因，从BERT到T5，再到GPT3，随着预训练模型效果越来越好的同时，模型也越来越大。而作者希望能够在不降低模型效果的情况下，去减小模型的大小以及所需的算力。于是，便提出来了ELECTRA模型。</p>
<h2 id="electra模型架构">ELECTRA模型架构</h2>
<p>在ELECTRA模型中，针对上述问题的解决办法是：使用<strong><em>replaced token detection(RTD)预训练任务</em></strong>。具体来说，RTD预训练任务是去训练一个用来判别输入的token是原始文本的token还是被替换的token的Discriminator。不同于MLM任务，RTD不使用MASKING，而是用从一个给定的distribution中采样得到的一些tokens来替换掉原始的一些tokens，这个给定的distribution通常是一个小的MLM模型的输出(可以看作是Generator的输出)。这样的话，MLM只使用了被MASK掉tokens，而RTD则使用全部的tokens，做一个序列标注任务，所以RTD任务需要的算力就会小很多。最终我们预训练好之后，抛弃Generator，只使用Discriminator来进行fine-tune。RTD的架构如下：</p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/RTD.jpg"></p>
<p>从上图中，我们可以看到，RTD的整个架构与GAN非常的类似，但是又存在不同的地方。具体细节如下：</p>
<ul>
<li><strong>For Generator：</strong>在ELECTRA中，Generator一般是一个小型的MLM模型，其输入是tokens，其输出是替换过一些token之后的token。如果对GAN熟悉的话，我们就知道，input是满足某种的distribution的样本，通过generator之后，我们得到是另一种distribution的sample，尽管这种distribution我们不知道它的formulation，但是输出的tokens一定是满足这种distribution的。得到输出之后，我们并不将输出通过输入到DIscriminator中，来训练Generator。由于文本是离散的，GAN对于离散的数据是不好训练的，因为Discriminator的梯度无法反向传播到Generator中(其实也是有办法的，就是使用RL中的policy gradient来做，但是非常麻烦，并且后面的实验也表明，其结果并不如MLE)。那怎么训练Generator呢？在ELECTRA中，使用极大似然估计，来得到被MASK掉的token的概率，具体公式如下：</li>
</ul>
<p>对于输入序列：<span class="math inline">\(x=[x_1,x_2,...,x_n]\)</span>，通过Generator之后，得到<span class="math inline">\(h_G(x)=[h_1,h_2,...,h_n]\)</span>，对于被MASK的位置<span class="math inline">\(t\)</span>，Generator最后通过softmax来产生一个<span class="math inline">\(x_t\)</span>的概率是： <img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/MLM.jpg"></p>
<p>其中，<span class="math inline">\(e(x_t)\)</span>表示token embedding。</p>
<ul>
<li><strong>For Discriminator：</strong>在ELECTRA中，对于位置<span class="math inline">\(t\)</span>，Discriminator来判别token是原始的token还是来自于Generator产生的distribution的token。formulation如下：</li>
</ul>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/RTD%20formulation.jpg"></p>
<p>最终整个Generator与Discriminator用公式来表达其输入与loss的话，如下：</p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/loss.jpg"></p>
<p>loss function应该不难理解，我们最终需要minimize如下formulation：</p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/final%20loss.jpg"></p>
<p>训练好之后，抛弃Generator，使用Discriminator来进行下游任务的微调即可。</p>
<h2 id="electra模型优化与改进">ELECTRA模型优化与改进</h2>
<h3 id="weight-sharing">weight sharing</h3>
<p>作者想知道将Generator与Discriminator的参数共享是否会提升效果。作者设置了相同大小的Generator与Discriminator，在布宫喜那个参数的情况下，GLUE结果为83.6，只共享token embedding的结果为84.3，共享所有参数的结果是84.4。可以看到共享token embedding的效果很明显。这也说明Generator对于embedding有更好的学习能力。因为在MLM中，最后通过softmax，会去更新所有的vocab 的embedding；而在Discriminator中，只会更新输入的token的embedding。所以，最终作者选择了共享token embedding。</p>
<h3 id="smaller-generator">smaller Generator</h3>
<p>作者探索了Generator的大小是否会对结果有提升。结果如下：</p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/generator%20size.jpg"></p>
<p>从结果上可以看到，Generator的大小在Discriminator大小的1/4-1/2之间效果比较好。原因在于：Generator如果太强了，会导致Discriminator的效果下降。</p>
<h2 id="training-algorithms">training algorithms</h2>
<p>除了上述提及的训练策略，作者还试了其他两种策略，如下：</p>
<ul>
<li>Adversarial Contrastive Estimation：这实际上就是按照GAN的思路来做，即：将Generator的loss从MLM loss改为最大化被替换的token在Discriminator上的RTD loss，和GAN是一样的。但是一个很严重的问题是：Discriminator的梯度无法反向传播到Generator中，怎么解决呢？作者使用了RL中的policy gradient，使得梯度可以进行传导。最后的效果只有54%的准确率，而MLE的训练策略下，可以达到65%的准确率。</li>
<li>Two-stage training：也就是我们先只去训练Generator，然后freeze掉Generator，将Discriminator的参数使用Generator的参数来进行初始化，之后对Discriminator训练与Generator同样的步数。</li>
</ul>
<p>三种预训练策略结果对比如下：</p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/triple%20strategies.jpg"></p>
<p>从结果可以看到，MLE的方式结果最好。</p>
<h2 id="electra模型结果">ELECTRA模型结果</h2>
<p>在这里，作者疯狂地将ELECTRA与各种预训练模型进行对比，结果如下：</p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/small%20result.jpg"></p>
<p><img src="/2020/07/02/NLP-ELECTRA%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/large%20result.jpg"></p>
<p>可以看到效果可以说是非常优秀了～</p>
<h2 id="参考文献">参考文献</h2>
<p>《ELECTRA: P RE - TRAINING T EXT E NCODERS AS D ISCRIMINATORS R ATHER T HAN G ENERATORS》</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ELECTRA</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|NER-CGN模型</title>
    <url>/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解一下《Leverage Lexical Knowledge for Chinese Named Entity Recognition via Collaborative Graph Network》论文，即：CGN模型，个人认为这篇文章将GNN应用到NER任务中的方式非常的优雅，值得一读。</p>
<a id="more"></a>
<h2 id="cgn模型提出的背景">CGN模型提出的背景</h2>
<p>词汇增强是目前NER中提升模型效果最有效的方法之一。LatticeLSTM是词汇增强的开山之作。但是LatticeLSTM中，仍然有三个问题没有解决：<strong>无法并行化、没有集成self-matched lexicon words到character中、没有直接集成与当前character最接近的lexicon words的信息（会导致word conflicts问题）。</strong>CGN模型正是为了解决这些问题而被提出来的。</p>
<blockquote>
<p>ps：latticeLSTM模型一定要好好研读一下，对于词汇增强类NER模型，基本上都是基于latticeLSTM模型的缺陷改进而来。</p>
</blockquote>
<h2 id="cgn模型介绍">CGN模型介绍</h2>
<p>还是先放模型图，再逐步介绍～</p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/CGN.jpg"></p>
<p>整个模型分为四层：<strong>encoding layer、graph layer、fusion layer、decoding layer，</strong>下面详细介绍～</p>
<ol type="1">
<li><p><strong>encoding layer：</strong>模型的输入分为两部分：sentence以及与sentence相匹配的lexicon words，分别表示为：<span class="math inline">\(s=\{c_1,c_2,...,c_n\},l=\{l_1,l_2,...,l_m\}\)</span>，其中<span class="math inline">\(c_i\)</span>表示sentence的第i个character，<span class="math inline">\(l_i\)</span>表示与sentence相匹配的第i个lexicon word。通过使用pretrained char embedding table，我们可以得到每一个character的char embedding：<span class="math inline">\(x_i=e^c(c_i)\)</span>；得到char embeding，我们需要将输入到BILSTM进行编码，这一步主要是为了捕获句子的顺序关系，得到contextual representation；对于matched lexicon words，我们也是通过使用pretrained word embedding table，得到每一个word的embedding：<span class="math inline">\(wv_i=e^w(l_i)\)</span>。最终encoding layer的输出是对这两者进行concat的结果，表示为：<span class="math inline">\(Node_f=[h_1,h_2,...,h_n,wv_1,wv_2,...,wv_m]\)</span>，其维度是：<span class="math inline">\([batch\_size,n+m,hidden\_size]\)</span>，<strong>我认为这样融合lexicon word的方式是更加优雅的，因为这样不仅可以融合lexicon word information，同时还解决了latticeLSTM模型无法并行的问题。</strong></p></li>
<li><p><strong>graph layer：</strong>这一层就是使用GNN来解决后面的两个问题的。总共分为三个graph层：<strong>Containing graph、Transition graph、Lattice graph。</strong>下面依次介绍：图的构建以及aggregate。</p>
<ul>
<li><p>图的构建</p>
<ul>
<li><p><strong>Containing graph(C-graph)：</strong>这一层是为了集成self-matched words到character中而设计的，主要就是建模character与self-matched words的关系。那么怎么构建呢？既然是graph，就要首先确定node set与edge set，C-graph中的node就是所有的character与matched lexicon words；C-graph中的edge是：我们首先根据所有的character与matched lexicon words来构造邻接矩阵(行代表word，列代表character)，如果word <span class="math inline">\(i\)</span>包含character <span class="math inline">\(j\)</span>的话，那么邻接矩阵中的<span class="math inline">\((i,j)\)</span>的值为1，否则为0，对应于C-graph的话，就是word与character有连接，如下图所示：</p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/C-graph.jpg" style="zoom:50%;"></p></li>
<li><p><strong>Transition graph(T-graph)：</strong>这一层是为了直接集成与当前character最接近的lexicon word信息(preceding or following)。怎么构建图呢？node set还是有C-graph一样，但是edge set不一样，edge是：我们仍然是更加node set来构建邻接矩阵(行：words+character，列：word+character)，如果word <span class="math inline">\(i\)</span>或者character <span class="math inline">\(m\)</span>是 character <span class="math inline">\(j\)</span>的最接近的信息，那么邻接矩阵中的<span class="math inline">\((i,j) or (m,j)\)</span>的值为1，同时为了捕获word与word之间的关系，如果word <span class="math inline">\(i\)</span>是word <span class="math inline">\(k\)</span> 的preceding或者following的信息，那么<span class="math inline">\((i,k)\)</span>的值为1。对应于T-graph的话，如下图所示：</p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/t-graph.jpg" style="zoom:50%;"></p></li>
<li><p><strong>Lattice-graph(L-graph)：</strong>对应于LatticeLSTM中的Lattice结构，主要是能够捕获到self-matched word的局部信息以及隐式的的集成最接近的lexicon word(个人认为加这个其实就是为了融合更多信息，也没有什么特别的作用，就算不加也是OK的，逻辑上没有什么错误或者不通，但是效果可能就不会这么好了)。怎么构建图呢？node set与前面的一样，edge set1不一样，edge是：我们根据node set来构建邻接矩阵(行：word+character，列：word+character)，如果character <span class="math inline">\(m\)</span>是character <span class="math inline">\(j\)</span>最接近的character，那么<span class="math inline">\((m,j)\)</span>的值为，那其实相对的，<span class="math inline">\((j,m)\)</span>的值也为1，另外，如果character <span class="math inline">\(j\)</span>是word <span class="math inline">\(i\)</span>的首位或者尾位，那么<span class="math inline">\((i,j)\)</span>的值为1。对应到L-graph上的话，如下图所示：</p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/l-graph.jpg" style="zoom:50%;"></p></li>
</ul></li>
<li><p>aggeragte</p>
<p>构建完图之后，我们对每一个graph，都使用GAT模型来进行encoding(不知道可不可以这么说，我理解的aggregate就是encoding🧐)。假设有对于一个graph，有M层。第<span class="math inline">\(j\)</span>层的输入是node feature：<span class="math inline">\(NF^{j-1}=\{f_1,f_2,..,f_N\}\)</span>以及邻接矩阵<span class="math inline">\(A\)</span>，其中<span class="math inline">\(f_i\in R^F，A\in R^{N\times N}\)</span>，第<span class="math inline">\(j\)</span>层的输出是新的node feature：<span class="math inline">\(NF^{j+1}=\{f_1^{&#39;},f_2^{&#39;},...,f_N^{&#39;}\}\)</span>。具体怎么计算呢，其实也比较简单，如下： <span class="math display">\[
f_i^{&#39;}=||_{k=1}^{K}\sigma (\sum_{j\in \cal N_i}\alpha_{ij}^kW^kf_j) ,\ 其中\\
\alpha_{ij}^k=\frac {exp(LeakyRelu(a^T[W^kf_i||W^Kf_j]))}{\sum_{k\in \cal N_i}exp(LeakyRelu(a^T[W^kf_i||W^Kf_k]))}
\]</span> 其中，<span class="math inline">\(K\)</span>表示的是有<span class="math inline">\(K\)</span>个head，在GAT中，使用了multihead attention(但不是self-attention)，<span class="math inline">\(||\)</span>表示concat，<span class="math inline">\(\alpha_{ij}^k\)</span>表示的是第k个head中，邻域内第<span class="math inline">\(j\)</span>个node对<span class="math inline">\(i\)</span> node的重要性程度，<span class="math inline">\(\sigma\)</span>是激活函数，不是sigmoid函数，<span class="math inline">\(a^T\)</span>表示的是单层的FFN，激活函数选择leakyrelu。对于GAT的最后一层，则是先avg，然后在使用激活函数来激活，具体如下： <span class="math display">\[
f_i^{final}=\sigma(\frac{1}{K}\sum_{k=1}^K\sum_{j\in \cal N_i}\alpha_{ij}^kW^kf_j)
\]</span> 当然了，这是对于一个graph，我们构建了三个graph，所以三个graph的输出的node feature如下： <span class="math display">\[
G_1=GAT_1(Node_f,A^C) \\
G_2=GAT_2(Node_f,A^T) \\
G_3=GAT_3(Node_f,A^L)
\]</span> 其中，<span class="math inline">\(G_k\in R^{F^{&#39;} \times (n+m)},k=\{1,2,3\}\)</span>。在得到最终的输出的node feature之后，我们只取前<span class="math inline">\(n\)</span>列，后面的<span class="math inline">\(m\)</span>列不要，因为我们要得到的是character的representation，然后输送到CRF。如下： <span class="math display">\[
Q_k=G_k[:,0:n],k=\{1,2,3\}
\]</span></p></li>
</ul></li>
<li><p><strong>Fusion layer：</strong>这一层就是对上一层得到的各种node feature进行fusion。这里就是简单的使用了相加。如下： <span class="math display">\[
R=W_1H+W_2Q_1+W_3Q_2+W_4Q_3
\]</span> <span class="math inline">\(H\)</span>是最初的char embedding送入BILSTM之后，得出的contextual representation。</p></li>
<li><p><strong>Decoding layer：</strong>这一层就是使用标准的CRF来解码，加入了L2正则。</p></li>
</ol>
<h2 id="实验结果">实验结果</h2>
<ol type="1">
<li><p>数据集：OntoNotes、MSRA、Weibo</p></li>
<li><p>参数设置：batch size：64(MSRA)，20(OntoNotes)，10(Weibo);optimizer:Adam(MSRA)、GD(OntoNotes and Weibo)</p></li>
<li><p>实验结果：</p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/weibo.jpg"></p></li>
</ol>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/ontonotes.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/msra.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/18/NLP-NER-CGN%E6%A8%A1%E5%9E%8B/speed.jpg" style="zoom:50%;"></p>
<p>从结果来看，CGN还是不错的，尤其是在Weibo数据集上，提升非常大。另外一篇使用GNN来做的LGN模型，从paper中发布的结果来看，ontonotes以及msra的结果要比CGN好上一点点，但是我个人认为CGN融合lexicon的方式要更加优雅一些，而且扩展性比较强。当然了，FLAT出来之后，几乎秒杀CGN和LGN，不过要是还是想用GNN来做NER的话，CGN是一个可以follow的工作，个人想法🧐。</p>
<h2 id="参考文献">参考文献</h2>
<p>《A Lexicon-Based Graph Neural Network for Chinese NER》</p>
<p>code：https://github.com/DianboWork/Graph4CNER</p>
<p>GAT讲解：https://zhuanlan.zhihu.com/p/81350196?utm_source=wechat_session</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CGN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|NER-LGN模型</title>
    <url>/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解一下《A Lexicon-Based Graph Neural Network for Chinese NER》论文，即：LGN模型，有意思的是，这篇论文与LRCNN是同一个作者。</p>
<a id="more"></a>
<h2 id="lgn模型提出的背景">LGN模型提出的背景</h2>
<p>LGN着眼解决的问题与LRCNN模型是一样的（两个模型来自于同一作者），就是LatticeLSTM中存在着一个问题：lexicon conflicts。在LRCNN模型中，是通过rethinking mechanism来解决，而在LGN模型中，则是通过GNN来进行解决这个问题，那么为什么可以用GNN来解决呢？因为对于word conflicts问题，很重要的就是要利用句子的全局信息和高层特征来进行解决，而GNN对于全局信息的建模和聚合有着非常强大的能力。具体是怎么解决的将在下面几小节具体说明。</p>
<h2 id="lgn模型介绍">LGN模型介绍</h2>
<p>先放图～</p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/graph.jpg"></p>
<ol type="1">
<li><p><strong>图的构建及node与edge的初始化</strong>。既然使用了GNN来解决，那么首先需要解决的问题就是：<strong>如何构建图？</strong></p>
<ol type="1">
<li>node：在LGN模型中，sentence中的char当作node，node的初始化并不是直接使用char embedding，而是将其输入到LSTM当中，输出的结果当作node的初始化；</li>
<li>edge：lexicon中的word被当作edge，(edge的初始化直接使用word embedding即可)，edge的指向是从首个字指向尾个字，以此来构建一个有向图。</li>
<li>global relay node：此外，在LGN中，还设置了全局中继节点来获取全局信息来用以解决word conflicts问题。</li>
<li>使用公式来表示：假设句子表示为：<span class="math inline">\(s=\{c_1,c_2,...,c_n\}\)</span>，<span class="math inline">\(c_i\)</span>表示第<span class="math inline">\(i\)</span>个字，lexicon中的word表示为：<span class="math inline">\(w_{b,e}=\{c_b,c_{b+1},...,c_{e-1},c_e\}\)</span>，<span class="math inline">\(b,e\)</span>分别表示该词的首部与尾部的index，整个图表示为：<span class="math inline">\(\cal G=(V,E)\)</span>，<span class="math inline">\(c_i\in \cal V\)</span>，<span class="math inline">\(e_{b,e}\in \cal E\)</span>。除此之外，原图转置后的反向图表示为：<span class="math inline">\(\cal G^T\)</span>。我们最终的是</li>
</ol></li>
<li><p><strong>图的Local Aggregation。</strong>local aggregation分为两大部分：node aggregation和edge aggregation。下面依次介绍。</p>
<ol type="1">
<li>node aggregation其实和普通的GNN一样，就是聚合其邻域特征，在这里，使用transformer中的multihead attention来进行聚合，具体公式如下：</li>
</ol>
<p><span class="math display">\[
e\rightarrow c:\tilde c_i^t=MultiAtt(c^t_i,\{\forall [c^t_k,e^t_{k,i}]\})
\]</span></p>
<p>其中，<span class="math inline">\(\tilde c^t_i\)</span>表示第i个char的第t次step aggregate之后的结果(个人理解是为计算<span class="math inline">\(c^{t+1}_i\)</span>做准备)，<span class="math inline">\(c^t_i\)</span>表示第i个char的第t次step的结果，<span class="math inline">\(e^{t}_{k,i}\)</span>表示第t次step的index从k到i的edge的state。</p>
<ol start="2" type="1">
<li>edge aggregation也是一样，聚合其邻域特征。对于<span class="math inline">\(e^t_{b,e}\)</span>，它的邻域特征有index从b到e的所有char的信息。所以同样通过multihead attention来进行聚合，具体公式如下： <span class="math display">\[
c\rightarrow e:\tilde e^{t}_{b,e}=MultiAtt(e^t_{b,e},C^t_{b,e})
\]</span> 其中<span class="math inline">\(C^t_{b,e}\)</span>表示index从b到e的char sequence embedding。</li>
</ol></li>
<li><p><strong>Global aggregation。</strong>由于句子并不是完全的sequential的，所以为了捕获长期依赖和高层特征，在LGN当中设置了global relay node来aggregate node和edge。具体公式如下： <span class="math display">\[
g^t_c=MultiAtt(g^t,C^t_{1,n}) \\
g^t_e=MultiAtt(g^t,\{\forall e^t\in\cal E\}) \\
c,e\rightarrow g:\tilde g^t=[g^t_c;g^t_e]
\]</span> 其中，<span class="math inline">\(C^t_{1,n}\)</span>表示第t个step的整个句子的hidden state，<span class="math inline">\(g^t\)</span>表示的是第t个step整个图的全局信息。</p></li>
<li><p><strong>Node update。</strong>在进行聚合之后，就要开始对node进行更新。具体公司如下：</p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/node update.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(\cal X^t_i\)</span>表示的是node i与全局中继节点<span class="math inline">\(\tilde g^{t-1}\)</span>的concat，<span class="math inline">\(\xi ^t_i\)</span>表示的是bigram的concat，这个实际上和LSTM有点像。</p></li>
<li><p><strong>Edge update。</strong>这个与node update是一样的。具体公式如下：</p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/edge update.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(\cal X^t_{b,e}\)</span>表示的是aggregation edge <span class="math inline">\(\tilde e^{t-1}_{b,e}\)</span>与全局中继节点<span class="math inline">\(\tilde g^{t-1}\)</span>的concat。</p></li>
<li><p><strong>Global relay node update。</strong>与上述一样，不再赘述，具体公式如下：</p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/relay node update.jpg" style="zoom:50%;"></p></li>
<li><p>重复2-6步骤若干次，之后接入CRF，这没什么可以说的。</p></li>
</ol>
<h2 id="实验结果">实验结果</h2>
<ol type="1">
<li><p>数据集：OntoNotes、MSRA、Weibo、Resume</p></li>
<li><p>参数设置：Adam optimizer(learning rate=2e-5 on MSRA and OntoNotes, 2e-4 on Weibo and resume)；droput(0.5 on embedding ,0.2 on aggregation module)；embedding size and state size is 50；</p></li>
<li><p>结果</p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/msra_and_weibo.jpg"></p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/resume.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/15/NLP-NER-LGN%E6%A8%A1%E5%9E%8B/ontonotes.jpg" style="zoom:50%;"></p></li>
</ol>
<p>从结果来看，要比LatticeLSTM要好，但是有意思的是，这个结果要比这个作者提出的LRCNN要差一些，我查了一下，都是19年发的，一个在emnlp(this paper)，一个在ijcai，anyway，enjoy。</p>
<h2 id="参考文献">参考文献</h2>
<p>《A Lexicon-Based Graph Neural Network for Chinese NER》</p>
<p>code：https://github.com/RowitZou/LGN</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LGN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|NER-FLAT模型</title>
    <url>/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解一下《FLAT: Chinese NER Using Flat-Lattice Transformer》论文，即：FLAT模型，这是今年ACL2020上NER任务的SOTA，个人觉得模型设计上非常优雅，非常值得一读，预测以后会成为比赛的标配🧐。</p>
<a id="more"></a>
<h2 id="flat模型提出的背景">FLAT模型提出的背景</h2>
<p>对于中文NER任务，词汇增强是有效提升效果的方法之一。LatticeLSTM是词汇增强的典型模型。但是这种Lattice结构，其模型结构比较复杂，并且由于lexicon word插入位置的动态性，导致LatticeLSTM模型无法并行，所以LatticeLSTM无法很好的利用GPU加速，其training以及inference的速度非常慢。所以怎么既能够很好的融入lexicon信息，同时又能够在不降低效果甚至提升效果的同时，大幅提升training以及inference的速度，这就是FLAT模型提出的背景。目前在NER中，处理lattice结构的方式有两大类：1. 设计一个框架，能够兼容词汇信息的输入，典型的模型有：LatticeLSTM、LRCNN，这种模型的缺点是无法对长期依赖进行建模；2.将lattice结构转换为graph结构，典型的模型有：LGN、CGN，这种模型的缺点是：1.序列结构和graph结构还是有一定的差别；2.通常需要使用RNN来捕获顺序信息。而FLAT基于transformer，能够很好的解决上述的问题，但是vanilla transformer对于位置信息的捕捉其实是很弱的，很多实验表明transformer对于NER这种对于位置信息要求很高的任务，并不是很适合，所以怎么解决这个问题，具体的将在下面几节介绍。</p>
<h2 id="flat模型介绍">FLAT模型介绍</h2>
<p>先放模型图～</p>
<p><img src="/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/FLAT.jpg"></p>
<p>上图是FLAT模型的整体结构。整体结构就是transformer的encoder部分+CRF。主要的改进就是在position encoding部分，因为transformer使用的是self-attnetion，这种点积的方式，破坏了transformer的距离感知和方向感知性。具体怎么改呢？下面具体介绍。</p>
<ol type="1">
<li><p>在FLAT中，将lattice结构转换为flat 结构。怎么做呢？每一个character以及mathced lexicon word都有head index与tail index。这样的话，我们就将lattice转换为flat结构，而且由于有了head index与tail index，所以转换是可以逆转的。</p></li>
<li><p>在FLAT中，定义了新的相对位置编码。对于span <span class="math inline">\(x_i\)</span>与<span class="math inline">\(x_j\)</span>，它的head与tail分别表示为：<span class="math inline">\(head[i],tail[i]\)</span>与<span class="math inline">\(head[j],tail[j]\)</span>。<span class="math inline">\(x_i\)</span>与<span class="math inline">\(x_j\)</span>的关系可以用以下四个相对距离来表示： <span class="math display">\[
d_{ij}^{(hh)}=head[i]-head[j] \\
d_{ij}^{(ht)}=head[i]-tail[j] \\
d_{ij}^{(th)}=tail[i]-head[j] \\
d_{ij}^{(tt)}=tail[i]-tail[j] \\
\]</span> 最终，相对位置编码计算如下： <span class="math display">\[
R_{ij}=RELU(W_r(p_{ d_{ij}^{(hh)}}\bigoplus p_{d_{ij}^{(hh)}} \bigoplus p_{d_{ij}^{(th)}} \bigoplus p_{d_{ij}^{(tt)}} )  ) \\
p_d^{2k}=sin(\frac {d}{10000^{2k/d_{model}}}) \\
p_d^{2k+1}=cos(\frac {d}{10000^{2k/d_{model}}}) \\
d\in \{d_{ij}^{hh},d_{ij}^{ht},d_{ij}^{th},d_{ij}^{tt}\}
\]</span> 融合后的<span class="math inline">\(R\)</span>的维度是：<span class="math inline">\([batch\_size,hidden\_size,seq\_len,seq\_len]\)</span>。当然了，既然使用了相对位置编码，那么vanilla transformer中的self attention的方式就不能用了(适用于绝对位置编码)，所以在FLAT中，采用的是transformer-xl中的variant self attention，公式如下： $$ A_{i,j}^{*}=W_{q}^{T} E_{x_i}^TE_{x_j}W_{k,E}+ W_q<sup>⊤E_{x_i}</sup>⊤R_{ij}W_{k,R}</p>
<p>+u^⊤E_{x_j}W_{k,E} + v^⊤R_{ij}W_{k,R} $$ 关于这个是怎么来的，可以参考transformer-xl原始论文或者<a href="https://zhuanlan.zhihu.com/p/84159401" target="_blank" rel="noopener">link</a>。</p></li>
</ol>
<h2 id="实验结果">实验结果</h2>
<ol type="1">
<li><p>数据集：OntoNotes、MSRA、Weibo、Resume</p></li>
<li><p>参数设置：</p>
<p><img src="/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/params_MSRA+ontonotes.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/prams_weibo+resume.jpg" style="zoom:50%;"></p></li>
<li><p>实验结果</p>
<p>我们首先来看一下FLAT以及其他的模型在四个数据集上的表现，如下图所示。从结果可以看到，FLAT的结果都超过了之前提出的模型。</p>
<p><img src="/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/result.jpg"></p>
<p>我们看一下加上了BERT之后的结果，如下图所示。NB!以后比赛可以搞起来了就不一定是BERT+BILSTM+CRF了，也可以是是BERT+FLAT+CRF，不过词汇表在其中还是占了大部分作用，以后要用的好的话，就得制作词汇表了。🧐</p>
<p><img src="/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/bert.jpg" style="zoom:80%;"></p>
<p>我们再来看一下推理速度，如下图所示。在batch的情况下，inferance的速度比其他模型快上好几倍，FLAT nb！这个实验另外让我非常惊奇的是，原始的LatticeLSTM的code是不能batch训练的，但是这篇paper的作者自己手撸了一个batch的LatticeLSTM，向大佬低头，有空去看看具体是怎么做到的。<a href="https://github.com/LeeSureman/Batch_Parallel_LatticeLSTM" target="_blank" rel="noopener">code</a></p>
<p><img src="/2020/10/18/NLP-NER-FLAT%E6%A8%A1%E5%9E%8B/speed.jpg"></p></li>
</ol>
<h2 id="参考文献">参考文献</h2>
<p>《FLAT: Chinese NER Using Flat-Lattice Transformer》</p>
<p>《Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context》</p>
<p>code：https://github.com/LeeSureman/Flat-Lattice-Transformer</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>FLAT</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|NER-LRCNN模型</title>
    <url>/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解一下《CNN-Based Chinese NER with Lexicon Rethinking》论文，即：LRCNN模型。</p>
<a id="more"></a>
<h2 id="lrcnn模型提出的背景">LRCNN模型提出的背景</h2>
<p>词汇增强被证明是有效提升NER性能的方式。其中最具开创性的工作就是LatticeLSTM，通过设计巧妙的gate机制将词汇信息融入到char level model中，大幅提高了效果。但是LatticeLSTM存在着两个非常严重的缺陷：</p>
<p><strong>1. 无法并行化。</strong>在LatticeLSTM中，引入的词汇信息的位置与数量是不固定的，所以无法使用batch来加速训练；</p>
<p><strong>2. 无法有效处理词汇信息冲突的问题</strong>。譬如：广州市长隆公园，在lexicon中有：<code>广州市</code>，<code>市长</code>，<code>长隆</code>，由于 <code>长</code> 是 <code>市长</code> 与 <code>长隆</code>的组成部分，所以模型对于<code>长</code>这个字的标记就会出现偏差，因为模型不知道要怎么对其进行标记，从而很可能标记出错，换句话说，词汇中的某些词会对模型中的字的标记进行误导。为了解决这两个问题，就有了LRCNN模型。</p>
<h2 id="lrcnn模型介绍">LRCNN模型介绍</h2>
<p>在LRCNN模型中，<strong>对于第一个问题：</strong>LRCNN采用了CNN而不是RNN来对句子进行编码，这样的话，在不同的cnn layer，不同长度的词汇将被融入模型中；<strong>对于第二个问题：</strong>LRCNN模型使用了rethinking mechanism。先放图～</p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/LRCNN.jpg"></p>
<p>具体的LRCNN模型描述如下：</p>
<ol type="1">
<li><p>假设输入的句子表示：<span class="math inline">\(C=\{c_1,c_2,...,c_M\}\in\cal R^{M \times d}\)</span>，基于字的词汇表为<span class="math inline">\(\cal V\)</span>，其中<span class="math inline">\(c_i\)</span>是第<span class="math inline">\(i\)</span>个字的char embedding，且<span class="math inline">\(c_i\in \cal R^d\)</span>，句子中与lexicon相匹配的每一个词语表示为：<span class="math inline">\(w_m^l=\{c_m,...,c_{m+l-1}\}\)</span>，其中<span class="math inline">\(c_m\)</span>表示这个词语的首个字，<span class="math inline">\(l\)</span>是这个词语的长度。</p></li>
<li><p>在句子编码层，LRCNN模型stack多层的cnn layer，主要目的就是：不仅能够很好的融入词汇信息，同时还能够并行化加速训练。具体来说：采用window size为2的一维卷积，以上图为例，我们将char embedding当作第一层，那么使用一次卷积后，可以得到第二层，这一层中每一个token都是一个bigram，如果在此基础上，在使用一次卷积，就可以得到trigram，以此类推，我们就可以不断地扩大感受野。除此之外，我们可以将词汇信息融入其中，具体来说：譬如<code>广州</code>是lexicon中的词，也是卷积后的bigram，我们可以使用badanan attention，从而将bigram信息与词汇信息进行融合。具体公式如下：</p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/cnn_attention.jpg" style="zoom: 33%;"></p>
<p>其中<span class="math inline">\(X_m^l\)</span>表示的是融合之后的结果。</p></li>
<li><p>由于CNN的分层结构，低层的词汇无法影响到高层的词汇，所以为了解决第二个问题，LRCNN模型中应用了rethinking mechanism。所谓地rethinking mechanism指的是：使用最顶层的CNN的feature map来指导低层的lexicon conflicts问题。具体做法是：向每一层CNN添加一个feedback layer来调整lexicon的权重。举个例子，如果高层特征中没有得到<code>广州市</code>与<code>长隆</code>，那么低层的<code>市长</code>这一词汇将会对最终的标注产生误导，所以需要<code>广州市</code>来降低<code>市长</code>一词在输出特征中的权重。具体公式如下：</p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/rethinking.jpg" style="zoom:50%;"></p>
<p>需要注意的是，其中<span class="math inline">\(W,U,b\)</span>都是reuse attention module中的参数，为了避免过拟合。</p></li>
<li><p>经过<span class="math inline">\(L\)</span>层，我们可以得到<span class="math inline">\(L\)</span>个multigram的信息，其维度都是<span class="math inline">\([batch\_size,hidden\_size,seq\_length]\)</span>。接下来，为了让句子的每一个字能够自适应的选择不同gram的feature map，LRCNN借鉴了《Densely Connected CNN with Multi-scale Feature Attention for Text Classification》<a href="https://blog.csdn.net/guoyuhaoaaa/article/details/81842767" target="_blank" rel="noopener">link</a> paper中的multi-scale attention机制，具体如下：</p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/multi-scale attention.jpg" style="zoom: 33%;"></p>
<p>其中，<span class="math inline">\(s_m^l\)</span>表示的是第L层的feature的第m个字对其D个特征进行求和得到的scalar，我们最终输入到CRF层的是：<span class="math inline">\(X^{att}=\{X_1^{att},X_2^{att},...,X_M^{att}\}\)</span>。</p></li>
</ol>
<h2 id="实验结果">实验结果</h2>
<ol type="1">
<li><p>数据集仍然是那四个：MSRA、OntoNotes、Weibo、Resume</p></li>
<li><p>参数设定：initial learning rate：0.0015 with decay rate：0.05，dropout rate：0.5 on char embedding,lexicon embedding,cnn layers; char embedding(50) and lexicon embedding(50) were initialized by pretained embedding and fine_tuned during training; four cnn layer with 128 output channels for other datasets except MSRA(5 cnn layer with 300 channels);use earlystopping.</p></li>
<li><p>结果</p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/msra.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/ontonotes.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/weibo.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/13/NLP-NER-LRCNN%E6%A8%A1%E5%9E%8B/resume.jpg" style="zoom:50%;"></p></li>
</ol>
<p>从实验结果来看，LRCNN还是不错的，不过我个人的感觉就是还是太过于复杂了，主要是rethinking那块，应该不是很好迁移到其他模型上。</p>
<h2 id="参考文献">参考文献</h2>
<p>《CNN-Based Chinese NER with Lexicon Rethinking》</p>
<p>《Densely Connected CNN with Multi-scale Feature Attention for Text Classification》</p>
<p>LRCNN-code:https://github.com/gungunXD/LR-CNN</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LRCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>REFLECTION ON DRE</title>
    <url>/2020/11/07/NLP-RE-Reflection-on-DRE/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="Oh, these decrypted content cannot be verified, but you can still have a look."><div class="hbe-input-container"><input type="password" id="hbePass" placeholder="Hey, password is required here." /><label>Hey, password is required here.</label><div class="bottom-line"></div></div><script id="hbeData" type="hbeData" data-hmacdigest="d84ca00ffe484527369c0de3d58c4fe4bef7f13574270161773060a99b73a58f">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</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>ReflectionofPaper</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|TST-DualRL model</title>
    <url>/2020/10/28/NLP-TST-DualRL-model/</url>
    <content><![CDATA[<p>这篇博客讲解一下IJCAI2019的一篇关于text style transfer的论文：《A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer》。</p>
<a id="more"></a>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本风格转换</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>DualRL</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|RE-Integrate GNN into sentence-level RE task</title>
    <url>/2020/10/24/NLP-RE-Integrate-GNN-into-sentence-level-RE-task/</url>
    <content><![CDATA[<p>这篇博客主要讲解图神经网络在sentence-level的关系抽取任务中的应用，共讲解三篇paper：《Graph Neural Networks with Generated Parameters for Relation Extraction》、《GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》、《A General Framework for Information Extraction using Dynamic Span Graphs》。</p>
<a id="more"></a>
<h2 id="graph-neural-networks-with-generated-parameters-for-relation-extractionacl2019">Graph Neural Networks with Generated Parameters for Relation Extraction(ACL2019)</h2>
<h3 id="background">background</h3>
<p>图神经网络在图关系推理中的效果非常强大，但是目前以后的GNN只能在预定义好的图上进行，无法直接在非结构化的文本中直接应用。所以在这篇paper中，就是采用 spectral GCN来进行aggegate。但是要使用GCN的话，有这么几个条件：1.最好是无向图，如果是有向图的话，则需要再根据问题进行调整，很麻烦，一般是要求无向图；2.边的类型必须一样，也就是同质图。所以问题就在于：怎么去构建一个无向图？怎么得到邻接矩阵？这就是GPGNN所要解决的问题。</p>
<h3 id="model">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-sentence-level-RE-task/GPGNN_model.jpg"></p>
<p>整体模型架构分为三部分：<strong>encoding module、propagation module、classification module。</strong></p>
<ul>
<li><p><strong>task definition：</strong>给定一个sentence：<span class="math inline">\(s=(x_0,x_1,...,x_{l-1})\)</span>，relation的集合：<span class="math inline">\(\cal R\)</span>，以及sentence中的entites：<span class="math inline">\({\cal V}=\{v_1,v_2,...,v_{|\cal V|}\}\)</span>，其中<span class="math inline">\(v_i\)</span>由单个或者多个token构成。我们要做的是：对每一个entity pair <span class="math inline">\((v_i,v_j)\)</span>，我们去得出它的rleation <span class="math inline">\(r_{v_i,v_j}\)</span>。</p></li>
<li><p><strong>encoding module：</strong>首先根据text来构建全连接的GNN：<span class="math inline">\(\cal G=(V,E)\)</span>，所有的entity就是node。然后我们对entity pair进行encoding，得到邻接矩阵。具体如下： <span class="math display">\[
A_{i,j}^{(n)}=[MLP_n(BILSTM_n((E(x_0^{i,j}),E(x_1^{i,j}),...,E(x_{l-1}^{i,j}))))] \\
E(x_t^{i,j})=[x_t;p_t^{i,j}] \\
\]</span> 其中，<span class="math inline">\(x_t\)</span>表示word <span class="math inline">\(x_t\)</span>的word embedding(采用GloVe)，<span class="math inline">\(p_t^{i,j}\)</span>表示word <span class="math inline">\(x_t\)</span>相对于entity pair <span class="math inline">\((v_i,v_j)\)</span>的position embedding(sentnece中的每一个token要么属于entity <span class="math inline">\(v_i\)</span>，要么属于entity <span class="math inline">\(v_j\)</span>，要么都不属于)，我们这两者进行concat，得出的<span class="math inline">\(E(x_t^{i,j})\)</span>表示相对于entity pair <span class="math inline">\((v_i,v_j)\)</span>的word <span class="math inline">\(x_t\)</span>的embedding；<span class="math inline">\(A_{i,j}^{(n)}\)</span>表示的是第<span class="math inline">\(n\)</span>层的entity pair <span class="math inline">\(v_i,v_j\)</span>的边的参数，从本质上来说，就是邻接矩阵。</p></li>
<li><p><strong>propagation module：</strong>这一部分就是传统的aggregate。从第<span class="math inline">\(n\)</span>层更新到第<span class="math inline">\(n+1\)</span>层的公式如下： <span class="math display">\[
h_i^{(n+1)}=\sum_{v_j\in {\cal N_{(v_i)}}}\sigma(A^{(n)}_{i,j}h^{(n)}_{j})
\]</span> 其中，<span class="math inline">\(\sigma\)</span>表示某一种激活函数。但是这里有一个疑问：<span class="math inline">\(h^{(0)}_i\)</span>怎么得到的？论文里的没看懂。。。</p></li>
<li><p><strong>classification module：</strong>concat所有layer的<span class="math inline">\(v_i\)</span>与<span class="math inline">\(v_j\)</span>的表示，然后使用softmax，如下： <span class="math display">\[
r_{v_i,v_j}=[[h^1_{v_i}\odot h^1_{v_j}];...;h^K_{v_i}\odot h^K_{v_j}]] \\
{\cal P}(r_{v_i,v_j}|h,t,s)=softmax(MLP(r_{v_i,v_j})) \\
\]</span> 最终的loss使用CE loss，如下： <span class="math display">\[
L=\sum_{s\in S}\sum_{i\not=j}log{\cal P}(r_{v_i,v_j}|h,t,s)
\]</span></p></li>
</ul>
<h2 id="graphrel-modeling-text-as-relational-graphs-for-joint-entity-and-relation-extractionacl2019">GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction(ACL2019)</h2>
<h3 id="background-1">background</h3>
<h3 id="model-1">model</h3>
<h3 id="experiment">experiment</h3>
<h2 id="a-general-framework-for-information-extraction-using-dynamic-span-graphsnaacl2019">A General Framework for Information Extraction using Dynamic Span Graphs(NAACL2019)</h2>
<h3 id="background-2">background</h3>
<h3 id="model-2">model</h3>
<h3 id="experiment-1">experiment</h3>
<h2 id="references">references</h2>
<p>《Graph Neural Networks with Generated Parameters for Relation Extraction》</p>
<p>《GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction》</p>
<p>《A General Framework for Information Extraction using Dynamic Span Graphs》</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>关系抽取</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Sentence-level RE</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|TST-Paper Reading Notes</title>
    <url>/2020/11/01/NLP-TST-Paper-Reading-Notes/</url>
    <content><![CDATA[<p>这篇博客主要记录一下阅读过的重要的文本风格转换的paper，可能不会非常详细，但是会记录核心内容，遇到自己非常喜欢的或者真的特别新颖的，可能会详细的写吧，主要是paper太多了，全部详细的写没时间～</p>
<a id="more"></a>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本风格转换</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|chatbot系列-DAM</title>
    <url>/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/</url>
    <content><![CDATA[<p>正式入对话的坑啦🤣对话是目前NLP技术重要的落地场景之一，但是相比于其他的方向，对话目前的应用还不算成熟，也远远没有产生它应有的巨大商业价值，但是随着物联网与5G等的发展，对话的应用前景是非常光明的。此外，既然对话的坑还有很多，这也意味着总需要人去填满这些坑，对NLPer来说，是挑战也是机会。其他的不多说，本篇着重讲讲检索式对话的经典模型DAM。</p>
<a id="more"></a>
<h2 id="dam模型介绍">DAM模型介绍</h2>
<p>DAM模型是2018年百度发表的论文，思路非常清晰。由于传统的RNN并行效率低，所以DAM模型中全部使用了self-attention机制；除此之外，在多个粒度上去匹配文本，能够得到更加高层的语义特征，对结果有好处，这个在SSE模型上证明了这一点。下面就具体地来讲解这些DAM模型细节～(ps：这篇论文还总结了检索式对话与生成式对话的区别，非常值得一读)</p>
<h3 id="dam模型架构">DAM模型架构</h3>
<p>首先给出一些定义：给定训练集：<span class="math inline">\(\cal D=\{c,r,y\}_{z=1}^{N}\)</span>，其中，<span class="math inline">\(c=\{u_0,u_1,...,u_{n-1}\}\)</span>，<span class="math inline">\(y=\{0,1\}\)</span>，c表示context，r表示response，y表示标签，目标是：衡量与c与r之间的匹配度。下面直接给出模型图吧～</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/DAM.jpg"></p>
<p>DAM模型大致遵循：representation-matching-aggregation框架，下面一一介绍～</p>
<h4 id="input-layer">Input layer</h4>
<p>c与r共享word embedding，对于第i个utterance in context，它的表示为：<span class="math inline">\(U_{i}^{0}=\{e_{u_i,0}^{0},e_{u_i,1}^{0}...,e_{u_i,n_{u_i}-1}^{0}\}\)</span>，对于response，表示为：<span class="math inline">\(R^0=\{e_{r,0}^{0},e_{r,1}^{0},...,e_{r,n_r-1}^{0}\}\)</span>。</p>
<h4 id="representation-layer-and-matching-layer">representation layer and matching layer</h4>
<p>这一层主要是叠加多个self attention，从而提取出多个粒度的语义表示。假设有L层，那么每一层的结果都被保存下来，即：<span class="math inline">\(\{U_{i}^{0},U_{i}^{1},...,U_{i}^{L}\}_{i=0}^{n-1}\)</span>，<span class="math inline">\(\{R^{0},R^{1},...,R^{L}\}\)</span>。计算公式如下： <span class="math display">\[
U_{i}^{l+1}=attentiveModule(U_{i}^{l},U_{i}^{l},U_{i}^{l}) \\
R^{l+1}=attentiveModule(R^{l},R^{l},R^{l})
\]</span> 然后，根据这些结果，来构建self attention match matrix <span class="math inline">\(M_{self}^{u_i,r,l}\)</span>与cross attention match matrix <span class="math inline">\(M_{cross}^{u_i,r,l}\)</span>。那么，具体怎么来构建呢？如下：</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/self attention.jpg" style="zoom:50%;"></p>
<p>要计算<span class="math inline">\(M_{cross}^{u_i,r,l}\)</span>，需要计算cross attention的值，公式如下： <span class="math display">\[
U_{i}^{l}=attentiveModule(U_{i}^{l},R^{l},R^{l}) \\
R^{l}=attentiveModule(R^{l},U_{i}^{l},U_{i}^{l})
\]</span> <img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/cross attention.jpg" style="zoom:50%;"></p>
<p>从公式可以看到，两种matrix的构建实际上就是dot product。<strong>最终，对于每一个utterance in context的每一层来说，每一个矩阵的维度是：(batch_size,max_length_utterance,max_length_response)。</strong></p>
<h4 id="aggregation-layer">aggregation layer</h4>
<p>aggregation layer的输入维度是：(batch_size,num_utterance,max_length_utterance,max_length_response,2(L+1))，这是一个五维向量。我们记做：<span class="math inline">\(Q=\{\cal Q_{i,k,t}\}_{n\times n_{u_i},r}\)</span>。它是self attention与cross attention矩阵concat之后的结果。然后对其使用两层的三维卷积（每一层都接上最大池化），第二个最大池化是全局最大池化，得到的输出维度是：(batch_size,filters)，最终将输出送入一层preceptron中，公式如下：</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/preceptron.jpg" style="zoom:50%;"></p>
<h3 id="details">details</h3>
<ul>
<li><p>数据集：Ubuntu Corpus、Douban Conversation Corpus</p></li>
<li><p>评价指标：<span class="math inline">\(R_n@k\)</span>，MRR，MAP</p></li>
<li><p>loss function：</p></li>
</ul>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/loss function.jpg" style="zoom:50%;"></p>
<ul>
<li>每一个context最多有9个utterance，每一个utterance(reponse)最多有50个word</li>
<li>word embedding使用word2vec得到的词向量，维度是200</li>
<li>FFN的维度为200，self attention层最大是7层，最后是5层最好</li>
<li>第一个卷积层有32个[3,3,3]的filters，stride为[1,1,1]，最大池化大小为[3,3,3]，stride为[3,3,3]；第二个卷积层有16个[3,3,3]filters，stride为[1,1,1]，最大池化大小为[3,3,3]，stride为[3,3,3]</li>
<li>batch szie 为256</li>
</ul>
<h3 id="结果">结果</h3>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DAM/结果.jpg"></p>
<p>从结果来看，DAM模型效果还是不错的～</p>
<h2 id="dam模型实现">DAM模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>DAM</tag>
        <tag>tensorflow2</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|chatbot系列-IMN</title>
    <url>/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-IMN/</url>
    <content><![CDATA[<p>正式入对话的坑啦🤣对话是目前NLP技术重要的落地场景之一，但是相比于其他的方向，对话目前的应用还不算成熟，也远远没有产生它应有的巨大商业价值，但是随着物联网与5G等的发展，对话的应用前景是非常光明的。此外，既然对话的坑还有很多，这也意味着总需要人去填满这些坑，对NLPer来说，是挑战也是机会。其他的不多说，本篇着重讲讲检索式对话的经典模型IMN。</p>
<a id="more"></a>
<h2 id="imn模型介绍">IMN模型介绍</h2>
<p>IMN是2019年中科大发表在CIKM上的一篇论文，整个模型与SMN比较像，但是做了很多的改进，实验也做的比较扎实。总的来看：IMN模型的优势在于：1.使用char embedding来解决OOV问题，同时也加强了word的表示；2.使用了类似于ELMo的结构来做sentence encoder，从而增强sentence的表示；3.使用全局的方式来让context与response进行交互，而不是以往的那种每一个utterance in context分别与response进行交互，从而能够挑选出context中与response最相关的部分(有点pooling的味道？🧐)。下面直接介绍吧～</p>
<h3 id="imn模型架构">IMN模型架构</h3>
<p>首先给出定义：给出数据集：<span class="math inline">\(\cal D=\{c,r,y\}_1^N\)</span>，其中<span class="math inline">\(c=\{u_1,u_2,...,u_n\}\)</span>，表示context；r表示response，<span class="math inline">\(y=\{0,1\}\)</span>，表示标签，目标是：得到context与response的匹配度。下面给出架构图～</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-IMN/IMN.jpg"></p>
<p>IMN模型总共分为5部分，下面一一介绍～</p>
<h4 id="word-representation-layer">word representation layer</h4>
<p>这一部分是模型最初的输入，分为三部分：pre-train word embedding、training set word embedding、character embedding，分别对应于：斯坦福发布的GloVe/Google的word2vec词向量、在训练集上使用word2vec得到的词向量、字向量(随机初始化)。最终的输出是这三个embedding的concat，表示为：<span class="math inline">\(U_{k}^{0}=\{u_{k,i}^{0}\}_{i=1}^{l_{u_k}}\)</span>、<span class="math inline">\(R^0=\{r_{j}^{0}\}_{j=1}^{l_r}\)</span>，<span class="math inline">\(u_{k,i}^{0}\in R^d、r_{j}^{0}\in R^d\)</span>。</p>
<blockquote>
<p>注意：当一个模型的输入既有word embedding又有char embedding的时候，那么对于一个sentence来说，word embedding一定是3维的，char embedding一定是4维的，这个时候我们是无法直接将word embedding与char embedding进行concat或者是作为两个channel，那么怎么办呢？从我目前看到的模型中，基本上有两种解决办法：</p>
<p>1.对char embedding进行reshape，变成3维的，然后输入一个lstm中，然后将lstm的输出再次进行reshape，将结果与word embedding进行concat/channel；</p>
<p>2.对char embedding进行reshape，变成3维的，然后输入到一个cnn中，然后将结果使用reduce_mean,reduce_sum等等操作(或者使用全局池化)，使得结果变成2维的，然后再次进行reshape，将结果与word embedding进行concat/channel。</p>
<p>在IMN模型中，使用的是第二种方法，论文中没有具体说，但是如果去看它的开源代码的话，可以看到就是这样实现的。另外，第一种方法在BIMPM模型中有用到，感兴趣的可以去翻翻我之前关于BIMPM模型的解读～</p>
</blockquote>
<h4 id="sentence-encoding-layer">sentence encoding layer</h4>
<p>这一层主要是提出了一个sentence encoder，从而来得到一个好的sentence embedding。那具体怎么做的呢？在IMN模型中，它借鉴了ELMo模型中的结构。与以往的模型一样，它也是堆叠多层的BILSTM（论文里是三层），但是不同的是，以往的很多模型只使用最顶层的BILSTM的输出，而忽略了中间层的输出结果。实际上，中间层的输出结果同样也表示了很多的信息，只是相对于高层的输出而言，它表示的是较为低层的语义特征。所以，为了得到更好的sentence embedding，IMN模型中将所有层的输出都保留下来，并且还训练了softmax，得到了所有层的权重，最终的输出是所有层的加权求和，它把这种结构叫做：<strong>attentive hierarchical recurrent encoder (AHRE)</strong>。下面用公式来表达：</p>
<p>有M个层，对于第k个utterance，表示为：<span class="math inline">\(\{U_k^1,U_k^2,...,U_k^M\}\)</span>，response表示为：<span class="math inline">\(\{R^1,R^2,...,R^M\}\)</span>；最终context的输出表示为：<span class="math inline">\(U_K^{enc}=\{u_{k,i}^{enc}\}_{i=1}^{l_{u_k}}\)</span>，response的输出为：<span class="math inline">\(R^{enc}=\{r_{j}^{enc}\}_{j=1}^{l_r}\)</span>，其中：</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-IMN/ahe.jpg" style="zoom:50%;"></p>
<h4 id="matching-layer">matching layer</h4>
<p>在这块，IMN模型中采用的是将整个context与response进行match，而不是每一个utterance分别与response进行match。那么，context表示为：<span class="math inline">\(C^{enc}=\{c_i^{enc}\}_{i=1}^{l_c} \ where \ \ l_c=\sum_{k=1}^{n}l_{u_k}\)</span>。然后将context与response进行dot preduct，即：<span class="math inline">\(e_{ij}=(c_i^{enc})^T r_j^{enc}\)</span>。然后在此基础上，计算context-to-response representation <span class="math inline">\(\overline R^{enc}=\{\overline r_j^{enc}\}_{j=1}^{l_r}\)</span>与response-to-context representation <span class="math inline">\(\overline C^{enc}=\{\overline c_i^{enc}\}_{i=1}^{l_c}\)</span>，依次如下：</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-IMN/rc.jpg" style="zoom:50%;"></p>
<p><span class="math inline">\(\overline c_i^{enc}\)</span>的计算方法与<span class="math inline">\(\overline r_j^{enc}\)</span>一样。然后在此基础上，形成两个矩阵，如下：</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-IMN/matrix.jpg" style="zoom:50%;"></p>
<p>另外，得到<span class="math inline">\(C^{mat}\)</span>之后，需要进行seperate，因为一开始是进行了flatten，得到<span class="math inline">\(\{U_k^{mat}\}_{k=1}^{n}\)</span>。</p>
<h4 id="aggregation-layer">aggregation layer</h4>
<p>这一层就是将上一层的输出：<span class="math inline">\(\{U_k^{mat}\}_{k=1}^{n}\)</span>以及<span class="math inline">\(R^{mat}\)</span>作为输入，然后经过BILSTM，得到：<span class="math inline">\(U^{agr}=\{u_k^{agr}\}_{k=1}^{n}\)</span>与<span class="math inline">\(r^{agr}\)</span>。由于得到的<span class="math inline">\(U^{agr}=\{u_k^{agr}\}_{k=1}^{n}\)</span>是一个三维的向量，所以再经过一层BILSTM，得到<span class="math inline">\(c^{agr}\)</span>。最终的输出是：<span class="math inline">\(m=[c^{agr};r^{agr}]\)</span>。</p>
<h4 id="prediction-layer">prediction layer</h4>
<p>这一层是将<span class="math inline">\(m\)</span>输入到一个MLP中，激活函数为relu，然后再经过线性层，使用sigmoid，得到最终的输出结果。最终的损失函数为cross entropy。</p>
<h3 id="details">details</h3>
<ul>
<li>数据集：UDC V1，UDC V2，douban，e-commerce dialogue corpus</li>
<li>评价指标：<span class="math inline">\(R_n@k\)</span>、MAP、MRR、<span class="math inline">\(P@1\)</span></li>
<li>优化器：adam</li>
<li>对于中文数据集，batch size为128，英文数据集为96</li>
<li>初始学习率为0.001，每5000step指数衰减0.96</li>
<li>dropout应用与word embedding与所有的隐藏层，dropout为0.2</li>
<li>使用300维的GloVe词向量，100维的使用word2vec训练训练集得到的词向量，使用150维的char embedding(使用了3个卷积层[3,4,5]，每一次卷积有50个filters)</li>
</ul>
<h3 id="结果">结果</h3>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-IMN/结果.jpg"></p>
<p>从结果上看，IMN表现还是不错的，但是有一个疑问，CompAgg以及BIMPM也可以用于多轮对话吗？🧐</p>
<h2 id="imn模型实现">IMN模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>IMN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|chatbot系列-DUA</title>
    <url>/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/</url>
    <content><![CDATA[<p>正式入对话的坑啦🤣对话是目前NLP技术重要的落地场景之一，但是相比于其他的方向，对话目前的应用还不算成熟，也远远没有产生它应有的巨大商业价值，但是随着物联网与5G等的发展，对话的应用前景是非常光明的。此外，既然对话的坑还有很多，这也意味着总需要人去填满这些坑，对NLPer来说，是挑战也是机会。其他的不多说，本篇着重讲讲检索式对话的经典模型DUA。</p>
<a id="more"></a>
<h2 id="dua模型介绍">DUA模型介绍</h2>
<p>DUA模型是2018年SJTU所发表的一篇论文，在检索式对话当中也是非常有影响力的一片论文。不管是Multi-view还是SMN模型，其实并没有对utterances之间的关系进行很好的建模(即使是在SMN中，也只是对utterances使用简单的GRU就完事了)，而且每一个utterance对于得到合理response的作用是不一样的，所以不应该享有同样的权重(attention)，所以，DUA模型针对于这一点做出了改进，对utterance也进行了深度的encoding。下面将介绍DUA模型的详细细节，<strong>注意，DUA模型相对来说，个人认为有些细节还是挺难理解de1，符号定义得乱七八糟的，而且该讲清楚的没讲清楚，所以一定要仔细地下面的讲解(或者原始论文)～</strong></p>
<blockquote>
<p>回过头来看，我还是要吐槽一下，符号定义的真的乱七八糟，这是我读过的符号使用最混乱的paper。。。</p>
</blockquote>
<h3 id="dua模型架构">DUA模型架构</h3>
<p>首先给出一些定义：训练集的一个sample表示为：[C,R,Y]，其中$ C={U_1,U_2,...,U_t}<span class="math inline">\(， 另外\)</span>U_k$表示第k个utterance，R表示一个response，Y表示label(合理回复/不合理回复)。目标是：衡量context于response的匹配度。先给出模型图吧～</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/DUA模型架构.jpg"></p>
<p>模型架构和SMN挺像的，但是更复杂了，有些细节还是挺难懂的，下面依次讲解～</p>
<h4 id="utterance-representation">utterance representation</h4>
<ul>
<li>这一层首先是将每一个utterance in context以及response进行encoding，得到相应的utterance embedding或response embedding。输入是：<span class="math inline">\(C=\{U_1,U_2,...,U_t\}、R=[r_1,r_2,...,r_{n_r}]\)</span>，<span class="math inline">\(U_k\)</span>表示第k个utterance，且：<span class="math inline">\(U_k=[u_1,u_2,...,u_{n_u}]\)</span>，其中<span class="math inline">\(u_i,r_i\)</span>分别表示utterance in context于response的第i个word的向量表示。</li>
<li>然后将每个utterance embedding与response embedding输入到GRU中，得到编码后的utterance与response向量，表示为：<span class="math inline">\(S=[S_1,S_2,...,S_t]\)</span>与<span class="math inline">\(S_r\)</span>，分别表示utterance in context 与response，<span class="math inline">\(S_i\in R^n\)</span>。</li>
</ul>
<h4 id="turns-aware-aggregation">turns-aware aggregation</h4>
<p><strong>这是DUA模型中最重要的地方，也是创新的地方。</strong>如果仅仅只有上面那一步对utterance的处理的话，会存在一个缺点：也就是所有的utterance都被一样的处理了，也就是说我们认为所有的utterance对得到合理的response都有同样的作用，但这是不对的。由于最后一个utterance对于得到合理response来说，是最重要的，所以在DUA中，我们将每一个utterance in context与其他的utterance(包括 response)进行融合，从而挖掘最后一个utterance与其他utterance(包括response)之间的关系。具体如下：</p>
<p>输入是上一层的输出：<span class="math inline">\(S=[S_1,S_2,...,S_t]\)</span>与<span class="math inline">\(S_r\)</span>，输出表示为：<span class="math inline">\(F=[F_1,F_2,...,F_t,F_r]\)</span>。其中，<span class="math inline">\(F_j=S_j\Diamond S_t\)</span>，<span class="math inline">\(j\in \{1,...,t,r\}\)</span>。<span class="math inline">\(\Diamond\)</span>运算符表示aggregation运算，<strong>在论文中，采取了concatenation的方式。</strong></p>
<h4 id="matching-attention-flow">matching attention flow</h4>
<p><strong>这是DUA模型中最重要的地方，也是创新的地方。</strong>通过turns-aware aggregation，所有的utterance得到了再一次的encoding。但是此时的utterance过于冗长，且并不是所有的word都发挥了作用，所以我们要从中提取出有用且重要的信息。为了解决这个问题，<strong>DUA模型采用了R-net中的self-matching attention机制</strong>(所以MRC中的模型最好去看看，VQA最好也看看(co-attention)，这些都是不分家的，莫得办法，keep learning🤷‍♂️)。具体如下：</p>
<p>输入是上一层的输出：<span class="math inline">\(F=[F_1,F_2,...,F_t,F_r]\)</span>；对于任意的<span class="math inline">\(\hat F=[f_1,f_2,...,f_n]\in F\)</span>，输出表示为：<span class="math inline">\(P=[p_1,p_2,...,p_n]\)</span>。(这里我认为论文中写错了，并不是只有response这样处理，应该是所有的utterances(包括response)都这样处理才对。)注意：这里的<span class="math inline">\(n\)</span>与utterance representation中的n是一样的。其中，<span class="math inline">\(p_t\)</span>的计算公式如下： <span class="math display">\[
p_t=GRU(p_{t-1},[f_t,c_t]) 
\]</span> 其中，<span class="math inline">\(c_t\)</span>是self matching attention机制得到的结果，具体计算过程如下： <span class="math display">\[
c_t=attn(\hat F,f_t) \\
s_{j}^{t}=v^Ttanh(W_vf_j+W_{\hat v}f_t+b_r) \\
a_{i}^{t}=\frac {exp(s_{i}^{t})}{\sum _{j=1}^{n}exp(s_{j}^{t})} \\
c_t=\sum_{i=1}^{n}a_{i}^{t}f_i
\]</span> <strong>这一层的输出是：<span class="math inline">\(P=[P_1,P_2,...,P_t,P_r]，where \ P_i=[p_1,p_2,...,p_n]\)</span>。</strong></p>
<blockquote>
<p>注意：这一层之所以要在self matching attention之后加GRU，是因为self matching attention会丢失utterance内的顺序信息，所以将attention后的向量与之前的向量进行拼接，然后输入到GRU中，一方面可以进行再一次的encoding，另一方面，也可以把utterance中的无用信息给过滤掉。</p>
</blockquote>
<h4 id="response-matching">response matching</h4>
<p>经过matching attention flow之后，我们就已经对utterance进行了很好的encoding。这一层的处理和SMN就很像了，我们利用word level与utterance level 的utterance的表示，来构建word-word similarity matrix(<span class="math inline">\(M_1\)</span>)与sequence-sequence similarity matrix(<span class="math inline">\(M_2\)</span>)。 <span class="math display">\[
e_{1,i,j}=u_i^Tr_j \\
e_{2,i,j}=P_{u_i}^TAP_{r_j}
\]</span> 其中，<span class="math inline">\(P_{u_i},P_{r_j}\)</span>是matching attention flow的关于utterance与response的输出，并且，<span class="math inline">\(P_{u_i},P_{r_j}\in R^c\)</span>。对于<span class="math inline">\(M_1\)</span>与<span class="math inline">\(M_2\)</span>，分别输入到卷积层中，做二维卷积(是不是二维卷积还值得商榷)得到<span class="math inline">\(M_{1c}\)</span>与<span class="math inline">\(M_{2c}\)</span>。具体计算如下：</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/卷积.jpg" style="zoom:50%;"></p>
<p>紧接着，接上一个池化，并对其进行flatten与concat。具体如下：</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/flatten.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(m_p\)</span>表示第p个utterance的表示。</p>
<h4 id="attentive-turns-aggregation">attentive turns aggregation</h4>
<p>这一层对所有的信息进行聚合，从而得到最终的结果。上一层的输出表示为：<span class="math inline">\(M=[m_1,m_2,...,m_t]\)</span>(这里我认为原始论文写错了，通过CNN之后，得到的<span class="math inline">\(m_p\)</span>表示的是第p个utterance in context的向量化表示，所以t个utterance，<span class="math inline">\(M\)</span>中应该有t个元素)，将其输入到GRU中，得到<span class="math inline">\(H_m=[h_{m_1},h_{m_2},...,h_{m_t}]\)</span>，然后采用attention机制，得到attention值：<span class="math inline">\(v_f=L(H_m)\)</span>。如下：</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/attention.jpg" style="zoom:50%;"></p>
<blockquote>
<p>稍稍吐槽一下，这里符号定义得真是乱七八糟，和其他小节的符号冲突了，也不说明，关键是根本不对🤷‍♂️有点失望。而且每一步输出的维度也不讲清楚，虽然不用每一步都写，但是至少关键的地方，向量的维度要讲清楚，这样也方便读者阅读啊。我以后写论文，一定要把向量的维度说清楚。🧐</p>
</blockquote>
<p>最后将结果通过一个softmax，得到最终的score。公式如下： <span class="math display">\[
{\cal F}(U,R)=softmax(W_sv_f)
\]</span></p>
<h3 id="details">details</h3>
<ul>
<li><p>数据集：UDC、douban、E-commerce dialogue corpus(本文自己所release的corpus，ECD)</p></li>
<li><p>loss function：CE</p></li>
<li><p>评价指标：MAP、MRR、Rn@k、P@1</p></li>
<li><p>优化器：Adam</p></li>
<li><p>utterance的最大数量为10，每一个utterance的最大单词数目为50</p></li>
<li><p>batch size为200，初始学习率为0.001</p></li>
<li><p>卷积与池化的window size为（3，3）</p></li>
<li><p>GRU的维度为200</p></li>
<li><p>词向量使用word2vec在训练集训练得到的词向量，维度为200</p></li>
<li><p>训练5个epcoh收敛</p></li>
</ul>
<h3 id="结果">结果</h3>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/结果1.jpg"></p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-DUA/结果二.jpg"></p>
<blockquote>
<p>第二张图片是在ECD数据集上的结果。</p>
</blockquote>
<p>从结果来看，DUA的效果确实不错～</p>
<h2 id="dua模型实现">DUA模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Modeling Multi-turn Conversation with Deep Utterance Aggregation</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>DUA</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|chatbot系列-MRFN</title>
    <url>/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-MRFN/</url>
    <content><![CDATA[<p>正式入对话的坑啦🤣对话是目前NLP技术重要的落地场景之一，但是相比于其他的方向，对话目前的应用还不算成熟，也远远没有产生它应有的巨大商业价值，但是随着物联网与5G等的发展，对话的应用前景是非常光明的。此外，既然对话的坑还有很多，这也意味着总需要人去填满这些坑，对NLPer来说，是挑战也是机会。其他的不多说，本篇着重讲讲检索式对话的经典模型MRFN。</p>
<a id="more"></a>
<h2 id="mrfn模型介绍">MRFN模型介绍</h2>
<p>MRFN模型是2019年PKU与MSRA在WSDM顶会上所发表的关于检索式对话的论文。个人认为这是一篇不可多得的好论文，实验做的很详细，而且在representation或者说是granularities上做到了极致，综合了目前几乎所有的表示方法。MRFN模型解决的是两个问题：<strong>1.怎么对多个representation进行融合？2.不同的representation对最终matching performance的贡献是怎么样的？</strong>下面就来一一介绍～</p>
<h3 id="mrfn模型架构">MRFN模型架构</h3>
<p>首先给出一些定义：给定数据集：<span class="math inline">\(\cal D=\{c_i,r_i,y\}_{i=1}^{N}\)</span>，其中，<span class="math inline">\(c_i=\{u_{i,1},u_{i,2},..,u_{i,m_i}\}\)</span>，<span class="math inline">\(u_{i,j}\)</span>表示第i个样本的第j个utterance in context，<span class="math inline">\(y_i\in\{0,1\}\)</span>，目标是：衡量<span class="math inline">\(c_i\)</span>与<span class="math inline">\(r_i\)</span>之间的匹配度。下面直接模型结构图～</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-MRFN/MRFN.jpg"></p>
<p>MRFN模型主要分为两大部分：<strong>Representations与Fusion strategies。</strong>下面依次介绍～</p>
<h4 id="representations">Representations</h4>
<p>在MRFN模型中，有三种类型的representation：<strong>word representations, contextual representations, and attention-based representations。</strong></p>
<ul>
<li><p>word representation分为两种：<strong>Character-based Word Embedding与Word2Vec。</strong>前者能够将subword的信息考虑进去，后者能够考虑corpus中的共现信息，为其他的表示打下基础。</p>
<ul>
<li>Character-based Word Embedding指的是将文本按照character来划分，得到的是4维向量，将这四维向量进行reshape，输入到激活函数为tanh的一维卷积中，再经过全局池化，得到二维向量，再经过reshape得到最终的三维向量；</li>
<li>Word2Vec指的是使用word2vec训练得到的词向量作为初始化，并在训练过程中进行微调。</li>
</ul></li>
<li><p>contextual representation分为两种：<strong>Sequential Representation与Local Representation。</strong>前者编码的是子序列的语义信息，后者编码的是n-gram的语义信息，两种都是旨在捕获单词之间的短期依赖关系。</p>
<ul>
<li>Sequential Representation指的是将输入的三维向量(是使用word2vec初始化的)经过GRU编码，<strong>return_sequence=True</strong>；</li>
<li>Local Representation指的是使用4种不同卷积核尺寸的[1,2,3,4]卷积层对输入的三维向量(是使用word2vec初始化的)进行<strong>same卷积</strong>，各个卷积核的数目均一致，为:<span class="math inline">\(n_c\)</span>，论文里使用的是50，最终对4个结果进行concat，得到最终的结果。</li>
</ul></li>
<li><p>Attention-based Representations分为两种：<strong>self attention与cross attention。</strong>这种representation能够对单词之间的长期以来进行建模，以及对单词之间的重要性进行建模。</p>
<ul>
<li>self attention指的就是transformer中的attention机制，用公式来表达utterance <span class="math inline">\(u\)</span> 经过attention之后，得到的表示如下：</li>
</ul>
<p><span class="math display">\[
\cal O=MultiHeadAttention(U,U,U)
\]</span></p>
<ul>
<li>self attention与self attentio就在输入不一样，用公式来表达utterance <span class="math inline">\(u_i\)</span>经过attention之后，得到的表示如下：</li>
</ul>
<p><span class="math display">\[
\cal O=MultiHeadAttention(U_i,R,R) \\
\cal O=MultiHeadAttention(R,U_i,U_i)
\]</span></p></li>
</ul>
<blockquote>
<p>注意：attention的输出是word2vec初始化的三维向量！</p>
</blockquote>
<h4 id="fusion-strategies">Fusion Strategies</h4>
<p>首先明确一下，经过上一步，我们现在得到的是：对于utterance <span class="math inline">\(u_i\)</span> in context，表示为：<span class="math inline">\(\{U_i^k\}_{k=1}^K,K=6\)</span>；对于reponse r，表示为：<span class="math inline">\(\{R^k\}_{k=1}^K,K=6\)</span>。也就是说，现在对于网络的输入是：context：<span class="math inline">\(\{U_i^k\}_{k=1}^K,1&lt;=i&lt;=m\)</span>，response：<span class="math inline">\(\{R^k\}_{k=1}^K\)</span>。OK，接着往下走～</p>
<p>我们得到了这么多的表示后，要得到最终的matching score，就必须要对这些表示进行融合，那么怎么融合呢？论文里考虑了三种融合策略：<strong>FES(Fusing at an early stage)、FIS(Fusion at an intermediate stage)、FLS(Fusion at the last stage)</strong>。下面依次来看～</p>
<h5 id="fes">FES</h5>
<p>所谓的FES指的是，在utterance与response进行交互之前，就将各种表示进行融合。具体来说，对于utterance <span class="math inline">\(u_i\)</span>来说，它的6种表示可以表述为：<span class="math inline">\(\{U_i^k\}_{k=1}^K,K=6\)</span>，由于这6种表示都是三维的，且batch_size维度与seq_lengt维度相同，所以一个做法就是将这6种表示在最后一个维度上进行concat，最后得到的向量为：U_i*，其维度是：(batch_size,seq_length_u_i,hidden_units)；同样的，得到的reponse的向量为：R*，其维度是：(batch_size,seq_length_r,hidden_units)。</p>
<p>得到每一个utterance与response的表示之后，就需要对utterance in context与response进行交互。如下：</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-MRFN/FES.jpg" style="zoom:50%;"></p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-MRFN/Xnip2020-06-08_16-04-22.jpg" style="zoom:50%;"></p>
<p>然后将 U_i* 与 <span class="math inline">\(\overline e_{i,j}\)</span> 一起送入CompAgg模型中提出的SUBMULT+NN方法中，从而得到 <span class="math inline">\(T_i^*\in R^{n_i\times d}\)</span>。具体 <span class="math inline">\(t_{i,j}\)</span> 的计算如下：</p>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-MRFN/Xnip2020-06-08_16-08-07.jpg" style="zoom:50%;"></p>
<p>然后在此基础上，使用GRU+MLP，得到最终的matching score。</p>
<h5 id="fis">FIS</h5>
<p>在FIS中，对每种表示形式都执行utterance与response交互。然后对结果进行fusion，在经过GRU+MLP，得到matching score。</p>
<h5 id="fls">FLS</h5>
<p>在FLS中，对每种表示形式都执行utterance与response交互，经过GRU，然后对结果进行fusion，在经过GRU+MLP，得到matching score。</p>
<p>结果证明，FLS效果最好。</p>
<h3 id="details">details</h3>
<ul>
<li>数据集：UDC、douban</li>
<li>评价指标：MAP、MRR、<span class="math inline">\(P@1\)</span>、<span class="math inline">\(R_n@k\)</span></li>
<li>sequential representation使用的GRU的维度为200，其他部分使用的GRU的最佳维度是1000</li>
<li>batch size为100，学习率为0.001，采用earlystoping</li>
</ul>
<h2 id="结果">结果</h2>
<p><img src="/2020/06/04/NLP-chatbot%E7%B3%BB%E5%88%97-MRFN/Xnip2020-06-08_16-30-41.jpg"></p>
<p>结果不错～</p>
<p>最后说一点点叭，关于检索式对话近期可能不会再看了。看了这么多篇paper，其实都差不多，基本上就是representation-matching-aggregation这样一个框架，但是每一个部分都能够去做修改和探索。之后可能会去看看生成式对话的东西（不知道为什么，总是对生成式任务有着谜一般的执念🤣），然后可能再去看看强化学习的东西吧，毕竟对话要离开DRL的东西很难。其实我还是蛮想做pretraining的东西，或者MRC/VQA也行啊(如果有人带我的话)，但是摸得GPU啊，要是intern能做preretraining这块的话，就灰常好了(洗洗睡吧)。然后主要就是去做实现这块吧，感觉代码能力还是蛮重要的。就是这样了，keep learning，加油～</p>
<h2 id="mrfn模型实现">MRFN模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Multi-Representation Fusion Network for Multi-turn Response Selection in Retrieval-based Chatbots</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>MRFN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|chatbot系列-SMN</title>
    <url>/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-SMN/</url>
    <content><![CDATA[<p>正式入对话的坑啦🤣对话是目前NLP技术重要的落地场景之一，但是相比于其他的方向，对话目前的应用还不算成熟，也远远没有产生它应有的巨大商业价值，但是随着物联网与5G等的发展，对话的应用前景是非常光明的。此外，既然对话的坑还有很多，这也意味着总需要人去填满这些坑，对NLPer来说，是挑战也是机会。其他的不多说，本篇着重讲讲检索式对话的经典模型SMN。</p>
<a id="more"></a>
<h2 id="smn模型介绍">SMN模型介绍</h2>
<p>SMN模型是检索式对话中一篇非常有影响力的工作。由于attention机制在2016年后大幅兴起，所以SMN模型的思路也是基于交互的方式，来做context与response的match。首先讲讲SMN所要解决的问题：<strong>1.怎么在context中辨别出对选出合理response的有用的重要信息？2.怎么对context中的utterances之间的关系进行建模？</strong>针对于第一个问题，SMN模型使用基于交互的方式，从一开始就让context中每一个utterance与respons进行不同粒度的matching，并且使用卷积和池化来提取出重要的信息；针对于第二个问题，对得到utterance embedding，使用GRU来进行encoder，最终通过accumulation得到context embedding。明确了SMN所要解决的问题后，下面就具体讲讲具体的SMN模型架构与细节～</p>
<h3 id="smn模型架构">SMN模型架构</h3>
<p>首先定义一些符号：数据集：<span class="math inline">\(\{(y_i,s_i,r_i)\}_{i=1}^{N}，s_i=\{u_{i,1},u_{i,2},u_{i,3},...,u_{i,n_i}\}，y_i\in \{0,1\}\)</span>，其中，<span class="math inline">\(y_i\)</span>表示第i个样本的label，<span class="math inline">\(s_i\)</span>表示第i个样本中的context(包括多个utterances)，<span class="math inline">\(r_i\)</span>表示第i个样本的一个候选回复(合理回复/不合理回复)。我们的目标是：对于一对<span class="math inline">\((s,r)\)</span>，得出其匹配度，对于chatbot来说，我们要从candidate responses中选出匹配度最高的回复。SMN架构还是挺好懂的，直接放图吧～</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-SMN/SMN架构.jpg"></p>
<p>SMN模型分为三个部分：utterance-response matching layer、matching accumulation layer、matching prediction layer。下面依次介绍～</p>
<h4 id="utterance-response-matching-layer">utterance-response matching layer</h4>
<p>这一层是让context中的每一个utterance与response进行matching，输出是word-word similarity matrix与sequence-sequence similarity matrix。具体来说，对于一个给定的utterance：<span class="math inline">\(U=[e_{u,1},...,e_{u,n_u}]\)</span>与一个response:<span class="math inline">\(R=[e_{r,1},...,e_{r,n_r}]，U\in R^{d\times n_u}，R\in R^{d\times n_r}\)</span>。我们使用<span class="math inline">\(U\)</span>与<span class="math inline">\(R\)</span>来构建word-word similarity matrix(<span class="math inline">\(M_1\)</span>)与sequence-sequence similarity matrix(<span class="math inline">\(M_2\)</span>)。</p>
<ul>
<li><span class="math inline">\(M_1\)</span>的构建相对简单，直接使用utterance与response的word embedding sequence来进行dot即可，得到的结果的维度：<span class="math inline">\(M_1\in R^{n_u\times n_r}\)</span>。</li>
<li><span class="math inline">\(M_2\)</span>的构建相对复杂一点。我们需要将utterance与response的word embedding sequence输入到GRU层中，得到编码过之后的word embedding sequence：<span class="math inline">\(H_u=[h_{u,1},h_{u,2},...,h_{u,n_u}]\)</span>、<span class="math inline">\(H_r=[h_{r,1},h_{r,2},...,h_{r,n_r}]\)</span>，然后在将<span class="math inline">\(H_u\)</span>与<span class="math inline">\(H_r\)</span>进行dot，即：<span class="math inline">\(e_{2,i,j}=h_{u,i}^{T}Ah_{r,j}\)</span>。其中，<span class="math inline">\(A\in R^{m\times m}\)</span>。得到的结果的维度是：<span class="math inline">\(M_2\in R^{n_u\times n_r}\)</span>。</li>
</ul>
<p>得到<span class="math inline">\(M_1\)</span>与<span class="math inline">\(M_2\)</span>之后，将其作为CNN的两个channel，输入到CNN当中，做二维卷积，然后再使用全局二维池化。注意：CNN的输入维度是：<span class="math inline">\((seqLengthUtterance,seqLengthResponse,2)\)</span>，最终通过全局池化的输出的维度是：<span class="math inline">\((numFilters,)\)</span>。注意，对于一个utterance与response，经过全局池化的输出的维度是：<span class="math inline">\((numFilters,)\)</span>，由于有n个utterance，所以有n个<span class="math inline">\((numFilters,)\)</span>，维度是：<span class="math inline">\((n,numFilters)\)</span>。</p>
<blockquote>
<p>Note：这里没有算上batch_size维度，也没有算上num_utterance这个维度，针对的就是一个utterance的计算！具体复现论文的时候，需要注意一下tensor的维度。</p>
</blockquote>
<h4 id="matching-accumulation-layer">matching accumulation layer</h4>
<p>经过utterance-response matching layer，每一个向量都表示一个utterance embedding，然后将其输入到GRU中，进行编码。得到<span class="math inline">\([h_1^{ &#39;},h_2^{ &#39;}...,h_n^{ &#39;}]\)</span>。然后使用<span class="math inline">\(L\)</span>函数，得到最终的context embedding。</p>
<h4 id="matching-prediction-layer">matching prediction layer</h4>
<p>通过matching accumulation layer，我们可以得到<span class="math inline">\([h_1^{ &#39;},h_2^{ &#39;}...,h_n^{ &#39;}]\)</span>，在此基础上，使用<span class="math inline">\(L\)</span>函数，得到最终的context embedding。在论文中，<span class="math inline">\(L\)</span>有三种方式。</p>
<ul>
<li><span class="math inline">\(SMN_{last}\)</span>：<span class="math inline">\(L([h_1^{ &#39;},h_2^{ &#39;}...,h_n^{ &#39;}])=h_n^{ &#39;}\)</span>；</li>
<li><span class="math inline">\(SMN_{static}\)</span>：<span class="math inline">\(L([h_1^{ &#39;},h_2^{ &#39;}...,h_n^{ &#39;}])=\sum_{i=1}^{n}w_ih_i^{ &#39;}\)</span>；</li>
<li><span class="math inline">\(SMN_{dynamic}\)</span>：使用与HAN模型类似的attention机制，如下：</li>
</ul>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-SMN/attention.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(h_{u_i,n_u}\)</span>是第i个utterance通过GRU之后的最后一个位置的向量。</p>
<p>得到最终的context embedding之后，然后送入softmax中，得到最终的结果。如下：</p>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-SMN/softmax.jpg" style="zoom:50%;"></p>
<h3 id="details">details</h3>
<ul>
<li>损失函数：cross entropy。如下：</li>
</ul>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-SMN/CE.jpg" style="zoom:50%;"></p>
<ul>
<li>数据集：Douban conversation corpus</li>
<li>评价指标：<span class="math inline">\(R_n@k\)</span>，MAP，MRR</li>
<li>word embedding：使用word2vec预训练得到的200维向量</li>
<li>GRU维度：200</li>
<li>卷积和池化的window大小：（3，3）</li>
<li>第二层的GRU维度：50</li>
<li>batch_size：200</li>
<li>每一个utterance的最大长度为50，每一个context的最大的utterance的数目为10。如果context中utterance的数目大于10，那么取最近的前10个，否则，padding</li>
<li>初始学习率：0.001，应用earlystopping</li>
</ul>
<h3 id="结果">结果</h3>
<p><img src="/2020/05/19/NLP-chatbot%E7%B3%BB%E5%88%97-SMN/结果.jpg"></p>
<p>从结果来看，SMN模型要好于Multi-view模型，还是非常不错的。从这也说明，基于交互的匹配方式要比基于表示的好很多，并且多粒度表示对于性能的提升也是很有帮助的~</p>
<h2 id="smn模型实现">SMN模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>SMN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|chatbot系列-Multi-view</title>
    <url>/2020/05/15/NLP-chatbot%E7%B3%BB%E5%88%97-Multi-view/</url>
    <content><![CDATA[<p>正式入对话的坑啦🤣对话是目前NLP技术重要的落地场景之一，但是相比于其他的方向，对话目前的应用还不算成熟，也远远没有产生它应有的巨大商业价值，但是随着物联网与5G等的发展，对话的应用前景是非常光明的。此外，既然对话的坑还有很多，这也意味着总需要人去填满这些坑，对NLPer来说，是挑战也是机会。其他的不多说，本篇着重讲讲检索式对话的经典模型Multi-view。</p>
<a id="more"></a>
<h2 id="multi-view模型介绍">Multi-view模型介绍</h2>
<p>首先不得不说，这是一片非常好的文章，非常适合用来做对话入门🤩首先来说说为什么要有Multi-view模型。最先要说明的是，这篇文章是于2016年发表的，这时候匹配是以基于表示的模型为主，而基于交互的模型还为兴起。所以，在response selection任务中，大部分模型都是将context与response视为word的集合，然后对其进行embedding，论文中称其为word sequence model。这样的方法的缺点在于：<strong>其忽略了utterances之间的关系。</strong>譬如说：一个utterance可以是对上一个utterance的肯定、否定，或者是开启一个新的topic，但是word sequence model无法捕捉到这样的信息。再说明白点，word sequence model无法很好地处理多轮对话。<strong>所以，Multi-view模型通过在utterance level进行建模，并与word level进行combine，从而大幅提升在respone selection任务中的表现。</strong></p>
<h3 id="模型结构">模型结构</h3>
<p>首先介绍传统的word sequence model处理多轮对话的思路：将多轮对话首尾连接成一条长长的文本，从而输送到网络中。就是下面这个样子：</p>
<p><img src="/2020/05/15/NLP-chatbot%E7%B3%BB%E5%88%97-Multi-view/word sequence modle.jpg" style="zoom: 67%;"></p>
<p>其中，<code>_sos_</code>是两个utterance之间的连接符。再将句子使用GRU进行encoder之后，我们使用GRU的最后一个hidden state来计算confidence，即匹配度。公式如下： <span class="math display">\[
p(y=1|c,r)=\sigma (cWr+b)
\]</span> 其中，<span class="math inline">\(c，r\)</span>表示context1与response，<span class="math inline">\(\sigma\)</span>是sigmoid函数。但是仅仅在word level进行建模是不够的，Multi-view模型更是在utterance level进进行建模，从而期望得到更高层次的语义表示。如下</p>
<p><img src="/2020/05/15/NLP-chatbot%E7%B3%BB%E5%88%97-Multi-view/multi-view.jpg" style="zoom:67%;"></p>
<p>我在图上标记了一下，因为这个图要分成两部分来看(一开始我看的时候，有点模糊😷)。下面依次解析～</p>
<ul>
<li>红框部分是word sequence model，这个没有可说的，与上面说的一样；红框下面的是utterance sequence model部分。</li>
<li>绿色部分是word embedding部分，可以使用word2vec、Glove或者是随机初始化。注意：word level与utterance level是共用word embedding的。</li>
<li>黄色部分是一维卷积层，与textCNN中的卷积一样。注意，这里使用的是等长卷积，并且每一个utterance产生一个输出，但是kernel部分是共享的。</li>
<li>红色部分是全局池化层，提取出context与response中的核心意思，这样就把对轮对话变成utterance embedding sequence了。</li>
<li>蓝色部分是GRU层，并利用最后一个hidden state来计算置信度。</li>
</ul>
<blockquote>
<p>注意：word level的GRU是context与reponse共享的；utterance level的GRU是context与response共享的，但是两个level之间的GRU不共享参数。</p>
</blockquote>
<h3 id="损失函数">损失函数</h3>
<p>Multi-view的损失函数是：likehood loss、disagreement loss与 regularization loss。</p>
<p><img src="/2020/05/15/NLP-chatbot%E7%B3%BB%E5%88%97-Multi-view/loss.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(p_w(l_i)、p_u(l_i)\)</span>分别表示word level与utterance level的是第<span class="math inline">\(i\)</span>个label的概率，label取<span class="math inline">\(\{0,1\}\)</span>，<span class="math inline">\(\hat p_w(l_i)=1-p_w(l_i)、\hat p_u(l_i)=1-p_u(l_i)\)</span>，<span class="math inline">\(\cal L_D、\cal L_L\)</span>分别表示disagreement loss与likehood loss。likehood loss很好理解，不过关于disagreement loss，就不是很理解为什么要这么定义了，可能需要再看看其他论文。</p>
<p>在预测的时候，我们对每一个response的置信度的公式如下：</p>
<p><img src="/2020/05/15/NLP-chatbot%E7%B3%BB%E5%88%97-Multi-view/预测.jpg" style="zoom:50%;"></p>
<p>最后，我们从candidate中选出置信度最大的response，即可。</p>
<h3 id="details">details</h3>
<ul>
<li><p>数据集：UDC(ubuntu corpus)</p></li>
<li><p>评价指标：1 in m Recall@k，在m个候选回复中，合理回复出现在前k个就算成功。1 in 2 Recall@1，就是二分类中的recall。</p></li>
<li><p>training details：采用Glove词向量，GRU的hidden units为200，kernel数目为200，初始学习率为0.01，batch size为32，4-5个epochs就收敛了。</p></li>
</ul>
<h3 id="结果">结果</h3>
<p><img src="/2020/05/15/NLP-chatbot%E7%B3%BB%E5%88%97-Multi-view/结果1.jpg" style="zoom:50%;"></p>
<p>可以看到，结果确实非常好～论文中还去探索了<code>_sos_</code>tag的作用，也探索了不同数目的utterance对结果的影响，感兴趣的童鞋可以看原始论文，个人认为这篇论文写的非常好，非常值得一读～</p>
<h2 id="multi-view模型实现">Multi-view模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Multi-view Response Selection for Human-Computer Conversation</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>chatbot</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>Multi-view</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|gensim boy的修炼之路</title>
    <url>/2020/05/02/NLP-gensim-boy%E7%9A%84%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AF/</url>
    <content><![CDATA[<p>我最近在复现一些模型的时候，发现这些模型基本上都使用了word2vec或者是Glove等word embedding模型的pre-train的词向量来初始化，然后在此基础上进行fine tune。这些word embedding模型实现起来也不是很难，但是我的主要目的又不是去实践这些模型🥺，关键是自己实现的还不如开源的效果好，毕竟别人的代码做了很多很多的优化啦。所以，想要好的效果并且又不想花费太多时间在pre-train词向量上面的话，学习gensim就显得非常重要了。所以，这篇文章主要是记录gensim的基本使用方法，帮助大家快速的构建vocab，得到pre-train的词向量。</p>
<a id="more"></a>
<h2 id="gensim要求的数据格式">gensim要求的数据格式</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">texts = [[<span class="string">'human'</span>, <span class="string">'interface'</span>, <span class="string">'computer'</span>],</span><br><span class="line">[<span class="string">'survey'</span>, <span class="string">'user'</span>, <span class="string">'computer'</span>, <span class="string">'system'</span>, <span class="string">'response'</span>, <span class="string">'time'</span>],</span><br><span class="line">[<span class="string">'eps'</span>, <span class="string">'user'</span>, <span class="string">'interface'</span>, <span class="string">'system'</span>],</span><br><span class="line">[<span class="string">'system'</span>, <span class="string">'human'</span>, <span class="string">'system'</span>, <span class="string">'eps'</span>],</span><br><span class="line">[<span class="string">'user'</span>, <span class="string">'response'</span>, <span class="string">'time'</span>],</span><br><span class="line">[<span class="string">'trees'</span>],</span><br><span class="line">[<span class="string">'graph'</span>, <span class="string">'trees'</span>],</span><br><span class="line">[<span class="string">'graph'</span>, <span class="string">'minors'</span>, <span class="string">'trees'</span>],</span><br><span class="line">[<span class="string">'graph'</span>, <span class="string">'minors'</span>, <span class="string">'survey'</span>]]</span><br></pre></td></tr></table></figure>
<p>如上所示，也就是说，我们在将数据输入到gensim的模型进行训练之前，要对corpus进行分词、去除停用词等操作，总之要变成上述的格式。当然啦。如果要处理超大的文件的话，是无法一次性全部加载进入内存的，所以gensim也支持流式处理，也就是说将其变成一个python生成器就可以。如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySentence</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,file)</span>:</span></span><br><span class="line">		self.file=file</span><br><span class="line">   </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file,<span class="string">"r"</span>,encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">      <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines();</span><br><span class="line">      	<span class="comment">#淡当然中间可以执行很多操作，如分词、去除停用词等等</span></span><br><span class="line">      	yiled list(jieba.cut(line.strip()))</span><br><span class="line"></span><br><span class="line">my_sentence=MySentence(file)</span><br><span class="line">w2v_model=Word2Vec(my_sentence)</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问token的词向量</span></span><br><span class="line">w2v_model[<span class="string">"a"</span>]</span><br></pre></td></tr></table></figure>
<p>除了上述方法外，还有一种方法，就是使用LineSentence，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim,models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="comment">#注意：a.txt必须是已经分好词的文件！</span></span><br><span class="line">sentences=LineSentence(<span class="string">"a.txt"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="word2vec模型参数">Word2Vec模型参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"></span><br><span class="line"><span class="comment">#默认构建出来的词汇表，是按照频率降序排列。</span></span><br><span class="line">model=Word2Vec(sentences=sentence,\<span class="comment">#输入的数据</span></span><br><span class="line">              min_count=<span class="number">5</span>,\<span class="comment">#低于min_count的频率的token将被抛弃</span></span><br><span class="line">              size=<span class="number">100</span>,\<span class="comment">#embedding的维度</span></span><br><span class="line">              workers=<span class="number">4</span>,\<span class="comment">#用于训练的线程数目</span></span><br><span class="line">              sg=<span class="number">0</span>,\<span class="comment">#为0表示使用CBOW，为1表示使用skip-gram</span></span><br><span class="line">              )</span><br></pre></td></tr></table></figure>
<h2 id="词汇表保存与加载">词汇表保存与加载</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#这种方式可以加载后增加训练</span></span><br><span class="line">model.save(<span class="string">"vocab.w2v"</span>)</span><br><span class="line">model=Word2Vec.load(<span class="string">"vocab.w2v"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#这种方式无法增加训练</span></span><br><span class="line">model.wv.save_word2vec_format(<span class="string">"wde.txt"</span>,binary=<span class="literal">False</span>)</span><br><span class="line">model.wv.save_word2vec_format(<span class="string">"wd.bin"</span>,binary=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line">model=KeyedVectors.load_word2vec_format(<span class="string">"ds.txt"</span>,binary=<span class="literal">False</span>)</span><br><span class="line">model=KeyedVectors.load_word2vec_format(<span class="string">"ds.bin"</span>,binary=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>目前大致能用到的就是这样，其他的等以后用到的话，再更新吧～🥰</p>
<h2 id="参考文献">参考文献</h2>
<p>https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|分类与匹配的各类评价指标</title>
    <url>/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<a id="more"></a>
<h2 id="各类指标">各类指标</h2>
<p>首先记录一下符号：TP(true positive)、FP(false positive)、FN(false nergative)、TN(true negative)。表格如下：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">pred</th>
<th style="text-align: center;">True</th>
<th style="text-align: center;">False</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">True</td>
<td style="text-align: center;">TP</td>
<td style="text-align: center;">FP</td>
</tr>
<tr class="even">
<td style="text-align: center;">False</td>
<td style="text-align: center;">FN</td>
<td style="text-align: center;">TN</td>
</tr>
</tbody>
</table>
<h3 id="accuracy与error-rate">accuracy与error rate</h3>
<p><span class="math display">\[
accuracy=\frac {TP+TN}{N} \\
error \ rate=\frac {FP+FN}{N}
\]</span></p>
<h3 id="precisionrecallf1-score">precision/recall/F1 score</h3>
<p>precision表示在预测为正的样本中，真的为正的样本的比例；</p>
<p>recall表示在原本真的为正的样本中，预测为正的样本的比例。</p>
<p>F1-score是精确率与召回率的权衡。 <span class="math display">\[
precision=\frac {TP}{TP+FP} \\
recall=\frac {TP}{TP+FN} \\
F1-score=\frac {2}{\frac{1}{P}+\frac {1}{R}}
\]</span></p>
<blockquote>
<p>注意：precison、recall、F1score仅使用于二分类问题。对于多分类问题，使用的是micro-P、micro-R、micro-F1以及macro-P、macro-R、macro-F1。</p>
<p>micro： "Micro"是通过先计算总体的TP, FP和FN的数量，然后计算PRF。</p>
<p>macro：“Macro”是分别计算每个类别的PRF，然后分别求平均得到PRF。</p>
<p>链接：<a href="https://www.cnblogs.com/nana-zhang/p/11496496.html" target="_blank" rel="noopener">micro与macro</a></p>
<p><img src="/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/宏平均与微平均对比.jpg"> 如何每个class的样本数量相差不大，那么宏平均和微平均差异也不大 如果每个class的相差较大并且你想： 更注重样本量多的class：使用微平均 更注重样本量少的class：使用宏平均 如果微平均远低于宏平均，则应该去检查样本量多的class 如果宏平均远低于微平均，则应该去检查样本量少的class</p>
</blockquote>
<h3 id="em">EM</h3>
<p>EM，全称exact match。EM度量是在问答系统中广泛运用的指标。它衡量的是与任何一个真实答案完全匹配的预测的比例。是SQuAD的主要衡量指标。</p>
<h3 id="mrr">MRR</h3>
<p>MRR，全称mean reciprocal rank，广泛用于评估NLP中需要排序的模型与算法中，常见于信息检索与问答任务中。具体公式如下： <span class="math display">\[
MRR=\frac {1}{Q} \sum_{i=1}^{Q}\frac {1}{rank_i}
\]</span> 其中，<span class="math inline">\(Q\)</span>表示query的个数，<span class="math inline">\(rank_i\)</span>表示第i个query在真实答案集中出现的位置。举个🌰（来源于知乎）：</p>
<p><img src="/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/MRR.jpg"></p>
<p>可计算这个系统的MRR值为：(1/3 + 1/2 + 1)/3 = 11/18=0.61。</p>
<h3 id="map">MAP</h3>
<p>mAP，顾名思义，即AP的平均值，那么就需要先计算AP，然后再对其进行平均。</p>
<p>在信息检索中，AP指的是不同召回率上的正确率的平均值，而现在的有些检索系统为了能够快速返回结果，在计算AP时就不再考虑召回率。换句话说，如果数据库中和查询信息相关的5条信息，分别出现在查询结果中的第1、3、6、9、10位，那么这次查询的AP就是： <img src="https://www.zhihu.com/equation?tex=AP+%3D+%EF%BC%881%2F1%2B2%2F3%2B3%2F6%2B4%2F9%2B5%2F10%EF%BC%89%2F5%3D0.62" alt="[公式]"></p>
<p>得到多条查询的AP值，对其进行平均，就得到了mAP。<a href="https://zhuanlan.zhihu.com/p/35983818" target="_blank" rel="noopener">来源</a></p>
<p><img src="/2020/05/12/NLP-%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8C%B9%E9%85%8D%E7%9A%84%E5%90%84%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/MAP.jpg"></p>
<h3 id="ndcg">NDCG</h3>
<p><a href="https://zhuanlan.zhihu.com/p/38850753" target="_blank" rel="noopener">链接</a></p>
<h2 id="指标在文本匹配中的应用">指标在文本匹配中的应用</h2>
<p><strong>相似度计算与复述识别</strong></p>
<p>这个任务是给出两句话，判断两者是不是表达了同样的含义，或者两句话的相似度有多高，一般都是当成分类问题来对待。常用的公开数据集有：STS、QQP、MSRP，当然啦，还有PPDB，但是数量太大，我没有跑过这个数据集，前面三个数据集就已经很好啦。由于是分类问题(pointwise)，所以评价指标一般使用Accuracy或者F1 score。</p>
<p><strong>问答匹配(answer selection)</strong></p>
<p>这个任务指的是：给定问题与问题的候选答案池，从中选择出最好的答案，如果说选择的答案在真实答案集中，说明问题被很好的回答了，否则的话，问题就没有被很好的回答。一般来说，问答匹配会采用pairwise的学习方式，即同question的一对正负样本作为一个训练样本，当然也会使用listwise。常用的数据集有：TrecQA、insuranceQA、WikiQA，还有QNLI，我一般会使用前三个，QNLI是很大的数据集，还没尝试过(关键是么得算力😫)。评价指标使用mAP与MRR等指标。</p>
<p><strong>对话匹配(response selection)</strong></p>
<p>对话匹配任务还没有试过，相比问答匹配来说，对话会更加难一些，因为它的回复空间非常大。常见的数据集：UDC、Douban conversation Corpus。评价指标使用Recall_n@k，也就是n个候选回复中，合理回复出现在前k个位置就算成功。当然也会使用mAP与MRR。</p>
<p><strong>自然语言推理/文本蕴含识别</strong></p>
<p>NLI/RTE任务指的是：给定句子A(premise)的前提下，如果句子B(hypothesis)为真，那么就是A蕴含了B；如果B为假，那么A与B互相矛盾；否则A与B相互独立。一般来说，NLI/RTE被视为三分类问题(pointwise)。常见的数据集“SNLI、XNLI、MNLI。评价指标使用micro-PRF与macro-PRF。</p>
<p><strong>信息检索(Information Retrieval)</strong></p>
<p>信息检索，一般都是先进行相关项的找回，然后再对其进行rank。评价指标会使用mAP与MRR。</p>
<p><strong>机器阅读理解(Machine Reading Comprehension)</strong></p>
<p>MRC任务就是在文本段中找寻答案的过程。常用数据集有：SQuAD、MS MARCO、CoQA等等。评价指标主要是EM。MRC任务比较难，而且需要大量的算力，所以不建议算力不充足的童鞋自己搞，容易走很多弯路。</p>
<p>大致就是这样啦，梳理完这些评价指标后，舒服啦🤩</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本分类模型-RCNN</title>
    <url>/2020/04/25/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-RCNN/</url>
    <content><![CDATA[<p>最近在看文本分类系列模型，应该会陆续更新几篇有关文本分类模型的博客，大概一周内更新完成，之后开始做文本匹配的东西，虽然这些模型网上有很多解读了，但是只有自己写出来了，才算是自己的呀。本篇博客讲解最为经典的RCNN模型，并采用tensorflow2来进行实现。</p>
<a id="more"></a>
<h2 id="rcnn模型简介">RCNN模型简介</h2>
<p>RCNN是2015年由中科院所提出的用于文本分类的模型。一句话概括就是：RCNN的优点在于：能够尽可能的通过捕捉上下文信息来表示单词，同时又能够以较低的时间复杂度完成训练。模型整体比较简单，直接上图吧。👇</p>
<p><img src="/2020/04/25/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-RCNN/RCNN模型.jpg"></p>
<ul>
<li>模型的第一层是一个biRNN架构，其中<span class="math inline">\(c_l(w_3)\)</span>表示单词<span class="math inline">\(w_3\)</span>的左侧所有的上下文的向量化表示；<span class="math inline">\(c_r(w_3)\)</span>表示单词<span class="math inline">\(w_3\)</span>的右侧所有的上下文的向量化表示。<span class="math inline">\(e(w_3)\)</span>表示单词<span class="math inline">\(w_3\)</span>的词向量。那么，第3个位置的向量话表示是三者的concatenate，即：<span class="math inline">\(x_3=[c_l(w_3),e(w_3),c_r(w_3)]\)</span>。这样，能够充分地利用单词的上下文信息，从而能够更好地表达单词的在上下文的含义。</li>
<li>模型的第二层使用激活函数tanh，来对得到的<span class="math inline">\(x_i\)</span>，进行转换。那么，第二层的输出是：<span class="math inline">\(y^{(2)}_i=tanh(W^{(2)}x_i+b^{(2)})\)</span>。</li>
<li>模型的第三层是最大池化层。用公式来表示就是：<span class="math inline">\(y^{(3)}=max_{i=1}^{n}y_{i}^{(2)}\)</span>。</li>
<li>模型的最后一层就是softmax层。这个没有什么好说的。</li>
</ul>
<p>论文提到的模型训练的trick：使用SGD、使用word2vec得到的词向量来初始化、均匀分布初始化所有参数。</p>
<p>整个模型就是这样，挺简单的，从现在的角度看，创新性真的不大，不过在当时应该有创新性的吧。</p>
<h2 id="rcnn模型实现">RCNN模型实现</h2>
<p>暂无。</p>
<h2 id="参考文献">参考文献</h2>
<p>Recurrent Convolutional Neural Networks for Text Classiﬁcation</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本分类模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本分类模型-HAN</title>
    <url>/2020/04/25/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-HAN/</url>
    <content><![CDATA[<p>最近在看文本分类系列模型，应该会陆续更新几篇有关文本分类模型的博客，大概一周内更新完成，之后开始做文本匹配的东西。虽然这些模型网上有很多解读了，但是只有自己写出来了，才算是自己的呀。本篇博客讲解最为经典的HAN模型，并采用tensorflow2来进行实现。</p>
<a id="more"></a>
<h2 id="han模型简介">HAN模型简介</h2>
<p>HAN模型大体的思想是：将word embedding通过attention机制得到sentence embedding，再通过sentence embedding 得到document embedding，最后加上一个分类层，得到最终的文本分类结果。模型架构如下：👇</p>
<p><img src="/2020/04/25/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-HAN/HAN模型架构jpg.jpg" style="zoom: 67%;"></p>
<p>整体架构比较简单，在这里，说几点需要注意的地方：</p>
<ul>
<li><p>模型的输入是：(batch_size,max_sentence,max_word)，这点非常重要。</p></li>
<li><p><span class="math display">\[
word \ encoder+word \ attention部分：\\
u_w=tanh(Wh_{i,j}+b_w) \\
\alpha_{it}=softmax(u_{it}^Tu_{w})\\
s_i=\sum_{t=1}^{T}\alpha_{it}h_{it}\\
\\
\\
sentence \ encoder+sentence \ attention部分：\\
u_s=tanh(W_sh_{i}+b_s)\\
\alpha_i=softmax(u_i^Tu_s)\\
v=\sum_{l=1}^{L}\alpha_ih_i
\]</span></p></li>
</ul>
<h2 id="han模型实现">HAN模型实现</h2>
<p>暂无。</p>
<h2 id="参考文献">参考文献</h2>
<p>Hierarchical Attention Networks for Document Classiﬁcation</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本分类模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>HAN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本分类模型-DPCNN</title>
    <url>/2020/04/25/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-DPCNN/</url>
    <content><![CDATA[<p>最近在看文本分类系列模型，应该会陆续更新几篇有关文本分类模型的博客，大概一周内更新完成，之后开始做文本匹配的东西。虽然这些模型网上有很多解读了，但是只有自己写出来了，才算是自己的呀。本篇博客讲解最为经典的DPCNN模型，并采用tensorflow2来进行实现。</p>
<a id="more"></a>
<h2 id="dpcnn模型简介">DPCNN模型简介</h2>
<p>DPCNN模型是张潼博士在2017年提出的模型。这个模型可以说是非常的出色，是第一个word level的深层文本分类卷积神经网络，优点在于：能够非常长的文本依赖的同时，时间复杂度又相对较低，废话不多说，直接上图。👇</p>
<p><img src="/2020/04/25/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-DPCNN/DPCNN模型.jpg"></p>
<p>DPCNN(深度金字塔卷积神经网络)之所以被提出，主要有这么几点：1.CNN比RNN更有优势，这在于CNN良好的并行计算效率；2.作者发现word level的CNN比深层的character level CNN效果要更好，所以word level的CNN有进一步的挖掘的潜力；3.但是如果只是单纯的加深网络，得到的效果反而不好，所以需要进行设计，就提出了DPCNN。</p>
<p>整个网络架构还是挺好懂的，下面主要介绍一下DPCNN模型几个重要的部分。</p>
<p><strong>text region embedding</strong></p>
<p>正如字面意思，text region embedding是针对一个文本区域进行卷积操作，得到的embedding。需要注意的是，这里使用的是等长卷积。为什么要使用等长卷积呢？原因在于：<strong>通过等长卷积，我们能够克服textCNN模型中缺点，从而能够捕获长距离信息，从而提高embedding的表示的丰富性。</strong></p>
<p><strong>固定feature map的数量</strong></p>
<p>为什么要这么做呢？原因在于：通过固定feature map的数量，实际上就可以得到更高层次的语义信息。</p>
<p><strong>1/2池化层</strong></p>
<p>我们通过使用池化层，就可以将模型感受文本的范围扩大一倍。在论文中，使用的是stride=2，size=3的池化层。</p>
<p><strong>残差连接</strong></p>
<p>当然了，非常深的网络，非常容易导致梯度消失问题。在DPCNN中，借鉴了ResNet的思想，使用残差连接。</p>
<h2 id="dpcnn模型实现">DPCNN模型实现</h2>
<p>暂无。</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>Deep Pyramid Convolutional Neural Networks for Text Categorization</li>
<li>https://zhuanlan.zhihu.com/p/35457093</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本分类模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>DPCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本分类模型-textCNN</title>
    <url>/2020/04/24/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-textCNN/</url>
    <content><![CDATA[<p>最近在看文本分类系列模型，应该会陆续更新几篇有关文本分类模型的博客，大概一周内更新完成，之后开始做文本匹配的东西，虽然这些模型网上有很多解读了，但是只有自己写出来了，才算是自己的呀。本篇博客讲解最为经典的textCNN模型，并采用tensorflow2来进行实现。</p>
<a id="more"></a>
<h2 id="textcnn模型简介">textCNN模型简介</h2>
<p>textCNN是2014年所提出的一个模型，可以说是非常早了。它的模型也非常简单，直接放图👇。</p>
<p><img src="/2020/04/24/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-textCNN/textCNN模型结构.jpg"></p>
<ul>
<li>首先是输入层，是单词的embedding得到的矩阵，维度是(batch_size,seq_length,embedding_size)。在论文中，作者实验了好几种策略：1.<strong>CNN-rand</strong>：对单词的词向量随机初始化；2.<strong>CNN-static</strong>：使用word2vec得到的word embedding，并且在训练过程中，不更新embedding；3.<strong>CNN-non-static</strong>：使用word2vec得到的word embedding，但是在训练过程中，更新word embedding；4.<strong>CNN-multichannels</strong>：使用两个channel，两个channel都是使用word2vec使用的word embedding，但是一个不随着训练过程更新，一个随着训练训练过程更新。(这样做的目的主要是防止过拟合)结果证明，第三种效果最好。</li>
<li>然后是卷积层，类似于n-gram，可以使用多个不同尺寸的卷积核，在论文中，使用(3,4,5)三种卷积核，激活函数为relu，每一种卷积核都有100个，即每一种卷积核得到的feature map都有100个。</li>
<li>之后是全局最大池化层(max-over-time pooling layer)，也就是对于每一个feature map，我们从中得到一个数值最大的feature。譬如：3种卷积核，每种100个，那么得到300个feature map(3*100)，经过max-over-time pooling之后，得到的结果是300维的向量。</li>
<li>最后是一个softmax层。在论文中，对经过全局最大池化层之后的输出，进行了dropout操作，防止过拟合。</li>
</ul>
<p>这就是整个textCNN模型，可以说是非常简单的思路了～</p>
<p>（放一张卷积操作的过程图～<a href="https://blog.csdn.net/The_lastest/article/details/81460424" target="_blank" rel="noopener">图片来源</a>）</p>
<p><img src="/2020/04/24/NLP-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B-textCNN/卷积计算过程.jpg" style="zoom: 50%;"></p>
<h2 id="textcnn模型实现">textCNN模型实现</h2>
<p>暂无。</p>
<h2 id="参考文献">参考文献</h2>
<p>Convolutional Neural Networks for Sentence Classiﬁcation</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本分类模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>textCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-BIMPM</title>
    <url>/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的BIMPM模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="bimpm模型的介绍">BIMPM模型的介绍</h2>
<p>BIMPM模型来源于2017年的《Bilateral Multi-Perspective Matching for Natural Language Sentences》论文。这篇文章总结了Siamese架构与matching-aggregate架构的优缺点，非常值得一读，有时间的童鞋一定要去读读原始论文🤩</p>
<h3 id="bimpm模型提出的原因">BIMPM模型提出的原因</h3>
<p>在文本匹配模型中，大致可以分为两大类：基于表示的文本匹配模型，即Siamese架构，以及基于交互的文本匹配模型，即matching-aggregate架构(有时候也称作compare-aggregate架构)。两种结构的优缺点如下：</p>
<ul>
<li>Siamese架构的优点在于两个句子可以共享参数，所以模型容量比较小，也容易训练；但是其缺点也很明显，由于在encoder部分没有明显的交互，所以会导致失去某些重要的信息，即没法捕获到更加丰富的语义特征。</li>
<li>matching-aggregate架构的优点可以及早地捕获更为丰富的交互特征，因此效果通常来说会更好一些。但是其缺点在于：一方面，很多模型只考虑了单个细粒度的匹配，比如word by word，而没有考虑其他细粒度的匹配；另一方面，匹配往往是单向的，即只有P对Q的匹配，或者是Q对P的匹配。</li>
</ul>
<p>知道了两种架构的优缺点后，作者提出了BIMPM模型。该模型本质上还是属于matching-aggregate架构，但是与其他模型不同的地方在于：其考虑了两个方向的匹配，即P对Q与Q对P。这样的话，就可以得到更为丰富的语义特征。事实证明，其确实取得了比较好的效果。</p>
<h3 id="bimpm模型的架构">BIMPM模型的架构</h3>
<p>首先给出相关定义。假设sample的形式为三元组：<span class="math inline">\((P,Q,y)\)</span>，其中<span class="math inline">\(P=\{p_1,p_2,p_3,...,p_M\}，Q=\{q_1,q_2,q_3,...,q_N\}\)</span>。其中，每一个token的维度均是d维的；<span class="math inline">\(y\)</span>是label。从这里可以看出，BIMPM模型属于pointwise的学习方式。</p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/BIMPM.jpg" style="zoom: 67%;"></p>
<p>上图就是BIMPM模型的整体架构。可以看到，BIMPM模型分为五层，下面一一介绍。</p>
<h4 id="word-representation-layer">word representation layer</h4>
<p>这一层包含两个部分：一部分是固定的词向量，比如使用Google的300维的word2vec词向量或者Glove词向量；另一部分是将每一个单词的每一个输入到LSTM网络中，得到的词向量，这一层最终的输出是将这两部分的词向量进行concat之后的结果。</p>
<h4 id="context-representation-layer">context representation layer</h4>
<p>这一层是对P和Q进行进一步的encoding，将上下文信息融入到每一个time step。具体来说，是使用一层BILSTM。注意：P与Q使用同样的BILSTM，即共享参数。</p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/BILSTM1.jpg" style="zoom:50%;"></p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/BILSTM2.jpg" style="zoom:50%;"></p>
<h4 id="matching-layer">matching layer</h4>
<p>这一部分是BIMPM模型真正创新的地方。它采用了双向的matching方法，即：比较P的每一个timestep的向量与Q的所有timestep向量，比较Q的每一个timestep的向量与P的所有timestep向量。在论文中，作者设计了一种multi-perspective的方法。整个multi-perspective方法包含两步：第一步：给出cosine matching function的定义；第二步，在给出的函数的基础上，使用四种匹配策略（full-matching、Maxpooling-matching、attentive-matching、max-attentive-matching），从而得到最终的输出。</p>
<p><strong>step1：</strong>首先要明白给出的cosine matching function的定义，如下：</p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/cosine.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(v_1,v_2\)</span>是两个向量，假设其维度是<span class="math inline">\(d\)</span>维；<span class="math inline">\(W\)</span>是参数，其维度是<span class="math inline">\(R^{l\times d}\)</span>，<span class="math inline">\(l\)</span>是perspective的数量；得到的输出<span class="math inline">\(m=\{m_1,m_2,m_3,...,m_l\},m\in R^l\)</span>。具体来说，$ m_k=cosine(W_k<em>v_1,W_k</em>v_2) $。</p>
<p><strong>step2：</strong>在第一步的基础上，给出了四种匹配策略。下面演示的是P对Q的matching，即P的每一个timestep对Q进行matching。</p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/matching strategiesjpg.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>full matching：</strong>对于P的一个timestep的向量，只与Q的最后一个状态相比较(前向的或者后向的)。对应上图的a，具体公式如下：</li>
</ul>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/full-matching.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>maxpooling matching：</strong>对于P的一个timestep的向量，分别与Q的每一个timestep的向量进行比较，最后选择结果最大的输出。对应于上图的b，具体公式如下：</li>
</ul>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/Maxpooling-matching.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>attentive matching：</strong>对于P的每一个timestep的向量，分别与Q的每一个timestep进行余弦相似度的计算，最后得到加权后的向量。对应于上图的c，具体公式如下：</li>
</ul>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/attentive-matching1.jpg" style="zoom:50%;"></p>
<p>得到余弦相似度，然后得到进行加权，如下：</p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/attentive-matching2.jpg" style="zoom:50%;"></p>
<p><img src="/2020/05/15/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-BIMPM/attentive-matching3.jpg" style="zoom:50%;"></p>
<ul>
<li><strong>max-attentive matching：</strong>与attentive matching不同·的地方在于，注意力向量并不是加权得到的，而是从中选取最大的作为注意力向量，启宇的与attentive matching一样。</li>
</ul>
<p>最后，这一层的输出是上面四种策略的concat之后的结果。<strong>注意，因为是双向的BLSTM，所以一个句子最后聚合的是8个向量！！！</strong></p>
<h4 id="aggregate-layer">aggregate layer</h4>
<p>这一层主要是去聚合两个句子。具体来说，使用的是一个BILSTM，我们取前向的LSTM与后向的LSTM的最后一个状态，由于是两个句子，所以最后是concat4个向量。</p>
<h4 id="prediction-layer">prediction layer</h4>
<p>这一层就是要得到最后的分类结果啦。具体来说，就是使用两层的MLP，最后使用softmax，得到最终的输出。</p>
<p>上述就是BIMPM模型的全部内容啦🎉</p>
<blockquote>
<p>稍微啰嗦一下吧，目前基本的文本匹配模型就看完了，总的来说，还是比较简单，实现起来也不难。要说难的地方，就是pairwise的训练方式与listwise的训练方式了吧，这个对于数据的处理是需要下一点功夫的，最好去借鉴一下别人的代码是怎么写的。pointwise是比较简单的，这个不用说什么。接下来的话，如果还需要接触文本匹配的模型的话，那就是MRC了吧。说实话，这个单独搞真不好搞，但是MRC毕竟是目前研究的热点方向，经典的BIDAF、R-net等模型还是需要去看的，VQA也需要去接触一下。不过，主线还是要放在检索式对话这块，先把检索式对话看完，然后再去熟悉生成式对话。毕竟生成式任务真的不好弄，而且还是对话这块，还是需要有人带。</p>
</blockquote>
<h2 id="bimpm模型的实现">BIMPM模型的实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Bilateral Multi-Perspective Matching for Natural Language Sentences</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>BIMPM</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-CompAgg</title>
    <url>/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的CompAgg模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="compagg模型介绍">CompAgg模型介绍</h2>
<p>CompAgg模型来源于《A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES》论文，于2016年提出。CompAgg模型并不是非常新的模型，它是对之前的模型进行了一个总结，提出了Compare-Aggregate模型框架，并对compare function进行了探索。与DecAtt模型类似。总而言之，CompAgg模型提出的原因在于：<strong>1.之前的模型只在一两个task上测试，泛化性不强。(换个数据集可能表现就比较差) 2.没有关注于comparison function。</strong></p>
<p>首先直接上模型架构图吧～</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/CompAgg模型架构.jpg" style="zoom: 50%;"></p>
<h3 id="compagg模型架构">CompAgg模型架构</h3>
<p>首先，定义一下训练集<span class="math inline">\(\{Q,A,y\}_{1}^{N}\)</span>，（pointwise）其中，<span class="math inline">\(Q、A\)</span>分别表示两个句子的向量化表示，并且：<span class="math inline">\(Q\in R^{d\times Q},A\in R^{d\times A}\)</span>，其中，<span class="math inline">\(d\)</span>表示每一个token的维度，也就是说，每一列表示一个token的embedding。CompAgg模型共分为：<strong>Preprocessing、Attention、Comparison、Aggregation</strong>这四个部分，下面详细的讲一下。</p>
<h4 id="preprocessing">Preprocessing</h4>
<p>这一步是对原始的<span class="math inline">\(Q\)</span>与<span class="math inline">\(A\)</span>进行一些处理，从而使得每一个词能够获取上下文的信息，从而让每一个token的embedding能够表示更加丰富的语义信息。(直接放图片吧，公式太难敲了😩)</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/preprocessing.jpg" style="zoom: 67%;"></p>
<p>注意，得到的<span class="math inline">\(\hat Q\)</span>与<span class="math inline">\(\hat A\)</span>的维度分别是：<span class="math inline">\(R^{l\times Q}、R^{l\times A}\)</span>。</p>
<h4 id="attention">Attention</h4>
<p>这里就是传统的Attention机制。用Q对A进行attention，然后得到加权后的Q，即H。公式如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/attention.jpg" style="zoom:67%;"></p>
<p>注意，G与H的维度分别是：<span class="math inline">\(R^{Q\times A}、R^{l\times A}\)</span>。</p>
<h4 id="comparison">Comparison</h4>
<p>这一步主要是matc上一步得到的<span class="math inline">\(\hat A\)</span>与<span class="math inline">\(H\)</span>。在这一步，作者实验了6中compare function：NN、NTN、EucCos、SUB、MULT、SUBMULT+NN。</p>
<ul>
<li>NN：实际上就是将两个向量concat，然后经过一层神经网络。</li>
</ul>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/NN.jpg" style="zoom:50%;"></p>
<ul>
<li>NTN：实际上就是使用NTN来concat两个向量，然后经过一层神经网络。</li>
</ul>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/NTN.jpg" style="zoom:50%;"></p>
<ul>
<li>EucCos：实际上就是计算两个向量的欧式距离和余弦相似度，然后将结果concat。</li>
</ul>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/EUcCos.jpg" style="zoom:50%;"></p>
<ul>
<li>SUB与MULT：实际上就是两种基于element-wise的方式来对向量进行比较。</li>
</ul>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/SUB+MULT.jpg" style="zoom:50%;"></p>
<ul>
<li>SUBMULT+NN：实际上就是将上面的结果concat，然后经过一层神经网络。</li>
</ul>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/SUBMULT+NN.jpg" style="zoom:50%;"></p>
<p>注意，上面的方法，都是两个句子对应位置单词之间的compare，所以compare之后，得到的向量的长度食欲A一样的。</p>
<h4 id="aggregation">Aggregation</h4>
<p>这一步就是将comparison得到的结果，经过一层CNN，得到最后的结果。</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/CNN.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(r\)</span>的维度是：<span class="math inline">\(R^{nl}\)</span>，<span class="math inline">\(n\)</span>表示filter的数量。注意：<span class="math inline">\([t_1,t_2,t_3,...,t_A]\)</span>的维度是：<span class="math inline">\(R^{l\times A}\)</span>。</p>
<h4 id="final-classification">final classification</h4>
<p>对于NLI问题，实际上就是一个三分类问题，接一个softmax层就可以啦！对于问答匹配问题来说，假设有K个candidate answer，我们要从中选择出一个answer，我们可以先计算出<span class="math inline">\([r_1,r_2,r_3,...,r_k]\)</span>,然后使用下属公式来预测：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-CompAgg/as.jpg" style="zoom:50%;"></p>
<p>以上就是CompAgg模型的内容，还是蛮好理解的。最后作者进行了实验，发现结果比Siamese NN要好一些，说明基于交互的SiameseNN在分别encode两个句子的过程中，丢失了一些信息，没有充分利用，整个论文还是值得一读的！</p>
<h2 id="compagg模型实现">CompAgg模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>CompAgg</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-DSSM/CDSSM</title>
    <url>/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-DSSM-CDSSM/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的DSSM/CDSSM模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="dssm模型介绍">DSSM模型介绍</h2>
<p>DSSM模型来源于2013年《Learning Deep Structured Semantic Models for Web Search using Clickthrough Data》论文。由于是13年的论文，所以其实整体思想还是很简单的，直接放图吧～</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-DSSM-CDSSM/DSSM.jpg"></p>
<p><span class="math inline">\(x\)</span>表示term vector， 表示输出向量，<span class="math inline">\(l_i,i=1,...,N\)</span> 表示隐藏层， <span class="math inline">\(W_i\)</span> 表示第<span class="math inline">\(i\)</span>层的参数矩阵，<span class="math inline">\(b_i\)</span>表示 第<span class="math inline">\(i\)</span>个偏置项。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=l_%7B1%7D%3DW_%7B1%7Dx+%5C%5C+l_%7Bi%7D%3Df%28W_%7Bi%7Dl_%7Bi-1%7D%2Bb_%7Bi%7D%29%2Ci%3D2%2C...%2CN-1+%5C%5C+y+%3D+f%28W_%7BN%7Dl_%7BN-1%7D%2Bb_%7BN%7D%29+%5C%5C+" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中，使用 <span class="math inline">\(tanh\)</span> 作为输出层和隐藏层的激活函数。在检索中，使用<span class="math inline">\(Q\)</span>表示query，<span class="math inline">\(D\)</span>表示doc，，那么它们之间的相关度可以使用余弦相似度来衡量。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=R%28Q%2CD%29%3Dcosine%28y_%7BQ%7D%2Cy_%7BD%7D%29%3D%5Cfrac%7By_%7BQ%7D%5E%7BT%7Dy_%7BD%7D%7D%7B%5Cleft+%5C%7C+y_%7BQ%7D%5Cright%5C%7C+%5Cleft+%5C%7C+y_%7BD%7D%5Cright%5C%7C%7D+%5C%5C" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>在·word2vec还没有兴起之前，一般都是采用BOW构建词汇表，然后使用one-hot进行编码，但是时间上，词汇表会非常大，在DSSM中，使用word hashing的方法，作为模型的第一层。和word2vec比较像，都是将高维稀疏向量转换为低维稠密向量。</p>
<p><code>Word Hashing</code>是paper非常重要的一个 <code>trick</code>，以英文单词来说，比如 <code>good</code>，他可以写成 <code>#good#</code>，然后按tri-grams来进行分解为 <code>#go goo ood od#</code>，再将这个tri-grams灌入到 <code>bag-of-word</code>中，这种方式可以非常有效的解决 <code>vocabulary</code>太大的问题(因为在真实的web search中vocabulary就是异常的大)，另外也不会出现 <code>oov</code>问题，因此英文单词才26个，3个字母的组合都是有限的，很容易枚举光。</p>
<p>在得到余弦相似度之后，再使用softmax，通过softmax 函数可以把query 与样本 doc 的语义相似性转化为一个后验概率：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=P%28D%7CQ%29%3D%5Cfrac%7Bexp%28%5Cgamma+R%28Q%2CD%29%29%7D%7B%5Csum_%7BD%5E%5Cprime%5Cin+%5Cbold+D%7D%7Bexp%28%5Cgamma+R%28Q%2C+D%5E%5Cprime%29%29%7D%7D+%5C%5C" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中<span class="math inline">\(\gamma\)</span>是一个softmax函数的平滑因子， <img src="https://www.zhihu.com/equation?tex=%5Cbold+D" alt="[公式]"> 表示被排序的候选文档集合，在实际中，对于正样本，每一个（query， 点击doc）对，使用 <span class="math inline">\((Q,D^+)\)</span>表示；对于负样本，随机选择4个曝光但未点击的doc，用 <img src="https://www.zhihu.com/equation?tex=%5Cleft%5C%7BD_%7Bj%7D%5E%7B-%7D%3B+j%3D1%2C...%2C4%5Cright%5C%7D" alt="[公式]"> 来表示。在训练阶段，通过极大似然估计来最小化损失函数：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=L%28%5CLambda%29+%3D+-log%5Cprod_%7B%28Q%2CD%5E%7B%2B%7D%29%7D%5E%7B%7D+P%28D%5E%7B%2B%7D%7CQ%29+%5C%5C" alt="[公式]"><figcaption aria-hidden="true">[公式]</figcaption>
</figure>
<p>其中<span class="math inline">\(\Lambda\)</span>表示神经网络的参数。模型通过随机梯度下降（SGD）来进行优化，最终可以得到各网络层的参数 <span class="math inline">\(W_i,b_i\)</span>。</p>
<p>DSSM模型的缺点在于：使用word hashing会造成冲突，因为不同的单词有可能产生同样的n-gram；采用词袋模型，损失了上下文结构信息。</p>
<h2 id="cdssm模型介绍">CDSSM模型介绍</h2>
<p>CDSSM是DSSM的改进，在DSSM中，由于采用词袋模型来表示输入的query与document，完全丧失了句子本身应有的上下文结构信息，而CDSSM采用词n-gram与卷积池化操作，捕获上下文关系。模型图如下：👇</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-DSSM-CDSSM/CDSSM.jpg" style="zoom: 45%;"></p>
<p><strong>Query/document：</strong>输入的query与doc句子，并且使用了<code>&lt;s&gt;</code>进行填充，保证进行卷积的时候，前后都有词。</p>
<p><strong>Sliding window</strong> 是定义滑动窗口对输入的句子做n-gram,来获取word-n-gram，文章用的tri-gram。</p>
<p><strong>word-n-gram layer</strong> 是经过滑窗处理后的word-n-gram 数据。</p>
<p><strong>Letter-trigram layer</strong> 是对word-n-gram 按原始dssm说的那个把单词用n-gram进行切割的方式进行处理，不过在进行词袋模型统计的时候统计的是word-n-gram了，不再是整个句子。</p>
<p><strong>卷积层：</strong>一维卷积。</p>
<p><strong>max-pooling ：</strong>池化层也是经常和卷积一起配合使用的操作了，这里之所以选择max-pooling是因为，语义匹配的目的是为了找到query和doc之间的相似度，那么就需要去找到两者相似的点，max-pooling则可以找到整个特征图中最重要的点，而avg-pooling则容易把重要的信息平均掉。 之后加一层全连接，损失函数什么的都与DSSM一样。</p>
<h2 id="cdssm模型实现">CDSSM模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data</p>
<p>A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval</p>
<p>https://zhuanlan.zhihu.com/p/53326791</p>
<p>https://www.cnblogs.com/wmx24/p/10157154.html</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>DSSM</tag>
        <tag>CDSSM</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-DecAtt</title>
    <url>/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-DecAtt/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的DecAtt模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="decatt模型介绍">DecAtt模型介绍</h2>
<p>DecAtt模型来源于《A Decomposable Attention Model for Natural Language Inference》论文，于2016年由google提出。主要是用于解决NLI（自然语言推理/文本蕴含）问题中。首先介绍一下什么叫NLI问题，如下：</p>
<p><strong>给定一个premise A与一个hypothesis B，如果给定A的前提下，B为真，我们就说A蕴含(entailment)了B，或者说能从A推理出B；如果B为假，那么就说A与B互相矛盾(contradiction)；如果无法根据A得出B是否为真还是假，就说A与B是互相独立的(neutral)。</strong>从这个定义中，我们可以知道NLI/TE是一个三分类的问题。</p>
<h3 id="decatt模型架构">DecAtt模型架构</h3>
<p>首先，定义训练集<span class="math inline">\(\{a^{(n)},b^{(n)},y^{(n)}\}_{n=1}^{N}\)</span>，其中<span class="math inline">\(a^{(n)}\)</span>表示第n个样本的premise，即：<span class="math inline">\(a^{(n)}=(a_3^{(n)},a_1^{(n)},a_3^{(n)},...,a_{l_a}^{(n)})\)</span>，<span class="math inline">\(l_a\)</span>表示是A句子的长度，其中每一个token的维度为<span class="math inline">\(d\)</span>；<span class="math inline">\(b^{(n)}\)</span>表示第n个样本的hypothesis，即<span class="math inline">\(b^{(n)}=(b_1^{(n)},b_2^{(n)},b_3^{(n)},...,b_{l_b}^{(n)})\)</span>，<span class="math inline">\(l_b\)</span>表示是B句子的长度，其中每一个token的维度为<span class="math inline">\(d\)</span>；<span class="math inline">\(y^{(n)}\)</span>表示第n个样本的标签向量，即<span class="math inline">\(y^{(n)}=(y_1^{(n)},y_2^{(n)},y_3^{(n)}...,y_C^{(n)})\)</span>，所以<span class="math inline">\(y^{(n)}\)</span>是一个C维的one-hot向量。</p>
<p>DecAtt模型总共分为三部分：<strong>Attend、Compare、Aggregate</strong>。模型结构图如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-DecAtt/decAtt模型架构.jpg" style="zoom: 67%;"></p>
<h4 id="attend">Attend</h4>
<p>对A与B句子的token，计算它们之间的attention weights。公式如下： <span class="math display">\[
e_{ij}=F^·(\hat a_i,\hat b_j)=F(\hat a_i)^TF(\hat b_j)
\]</span> 得到attention weights之后，我们在加权，得到新的向量，如下： <span class="math display">\[
\beta_i=\sum_{j=1}^{l_b} \frac {e_{ij}}{\sum_{k=1}^{l_b}e_{ik}}\hat b_j \\
\alpha_j=\sum_{i=1}^{l_a} \frac {e_{ij}}{\sum_{k=1}^{l_b}e_{kj}}\hat a_i
\]</span> 需要注意的是：<strong><span class="math inline">\(\beta\)</span>与<span class="math inline">\(\hat a\)</span>对齐，<span class="math inline">\(\alpha\)</span>与<span class="math inline">\(\hat b\)</span>对齐；</strong>另外，其中<span class="math inline">\(F\)</span>表示Relu函数。为什么要这么做呢？因为如果直接使用<span class="math inline">\(F^·\)</span>的话，由于<span class="math inline">\(\hat a_i,\hat b_j\)</span>均是d维的向量，并且句子长度分别是<span class="math inline">\(l_a,l_b\)</span>，那么，要算出全部的attention weigthts的话，要计算<span class="math inline">\(l_a\times l_b\)</span>次的<span class="math inline">\(F^·\)</span>计算；如果先使用relu函数先计算的话，那么就只需要<span class="math inline">\(l_a+l_b\)</span>次的relu计算，大大降低了计算复杂度。这也就是为什么被称作<strong>Decomposable Attention</strong>的原因。</p>
<h4 id="compare">Compare</h4>
<p>对加权后的一个句子与另一个原始句子相比较。 <span class="math display">\[
v_{1,i}=G([\hat a_i,\beta_i]) \\
v_{2,j}=G([\hat b_j,\alpha_j])
\]</span></p>
<h4 id="aggregate">Aggregate</h4>
<p>通过Compare，我们得到了两个向量：<span class="math inline">\(\{v_{1,i}\}_{i=1}^{l_a}、\{v_{2,j}\}_{j=1}^{l_b}\)</span>，它们的维度分别是<span class="math inline">\(l_a\times d、l_b\times d\)</span>。首先对两个向量集合进行求和。如下： <span class="math display">\[
v_1=\sum_{i=1}^{l_a}v_{1,i}\\
v_2=\sum_{j=1}^{l_b}v_{2,j}
\]</span></p>
<p>然后进行concatenate，输入到softmax中，得到最终的结果。 <span class="math display">\[
\hat y=H([v_1,v_2])
\]</span> loss function采用crossentropy。</p>
<h4 id="intra-sentence-attentionoptional">intra-sentence attention(optional)</h4>
<p>Attend、Compare、Aggregate三步是必不可少的。除此之外，我们还可以考虑句子内token之间的相关性，从而增强句子表示，我们称作intra-sentence attention。 <span class="math display">\[
f_{ij}=F_{intra}(a_i)^TF_{intra}(b_j) \\
a_{i}^{&#39;}=\sum_{j=1}^{l_a}\frac {exp(f_{ij}+d_{i-j})}{\sum_{k=1}^{l_a}exp(f_{ik}+d_{i-k})}a_j \\
\hat a=[a_i,a_{i}^{ &#39;}]
\]</span> 模型训练的细节：参数初始化使用高斯分布、使用预训练的word embedding(Glove)、对于所有的relu层均适用droput(ratio=0.2)、FFN层的维度为200。</p>
<h2 id="decatt模型实现">DecAtt模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>A Decomposable Attention Model for Natural Language Inference</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>DecAtt</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-ESIM</title>
    <url>/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的ESIM模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="esim模型介绍">ESIM模型介绍</h2>
<p>ESIM模型来源于2017年的论文《Enhanced LSTM for Natural Language Inference》。ESIM模型是文本匹配领域非常有影响力的工作，主要用来解决NLI/RTE问题。直接放图吧～</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/ESIM模型架构.jpg" style="zoom: 50%;"></p>
<p>ESIM模型架构总共分为三个部分：<strong>input encoding、local inference、inference composition。</strong>下面一一介绍～不过在此之前，给定一些符号：两个句子<span class="math inline">\(a=(a_1,a_2,a_3,...,a_{l_a})、b=(b_1,b_2,b_3,...,b_{l_b})\)</span>。其中，<span class="math inline">\(a_i\in R^l、b_j\in R^l\)</span>。</p>
<h3 id="input-encoding">input Encoding</h3>
<p>这一部分就是通过word embedding，得到整个句子的向量化表示，然后输入到BILSTM中(当然也可以使用tree LSTM)，得到encode之后的向量。具体公式如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/input encoding.jpg" style="zoom:50%;"></p>
<p>另外，在这里，之所以没有用更加简单方便的BIGRU，是因为作者发现结果不如BILSTM来的更好。</p>
<h3 id="local-inference">local inference</h3>
<p>在得到encode之后的新向量之后，我们需要对两个句子进行局部推理建模，也就是利用attention让两个句子发生交互。具体计算方法DecAtt模型一模一样，只不过去掉了FFN，直接计算Attention weights。具体公式如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/local inference 0.jpg" style="zoom:50%;"></p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/local inference.jpg" style="zoom:50%;"></p>
<p>注意：加权得到新的向量的长度没有发生变化，仍然是<span class="math inline">\(l_a、l_b\)</span>。在得到新向量之后，再做了local inference information的enhancement，这里的enhancement指的是：原来的向量与加权之后的向量的差与点积。然后将这些向量都进行concat。如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/enhancement.jpg" style="zoom:50%;"></p>
<p>这样做的目的，实际上就是希望能够捕捉到更多的high level的交互信息。</p>
<h3 id="inference-composition">inference composition</h3>
<p>最后一步就是inference composition。仍然是将local inference得到的结果输入到BILSTM中，提取上下文信息，然后同时使用averagepooling与maxpooling进行池化，最后使用tanh进行激活，最后加上softmax层，得到最终的分类结果。</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-ESIM/inference composition.jpg" style="zoom:50%;"></p>
<p>注意，这里，<span class="math inline">\(v_{a,i}、v_{b,j}\)</span>是经过BILSTM之后得到的输出。</p>
<p>training details：使用Adam，两个参数为0.9，0.999；初始学习率为0.0004；batch size为32；word embedding为300，使用Glove词向量，并且在训练的时候微调word embedding；对于所有FFN，使用dropout ，rate为0.5。以上就是ESIM模型的全部内容啦！不过说实话，ESIM模型架构上没有啥太新颖的地方，但是效果是真的好！🥰</p>
<h2 id="esim模型实现">ESIM模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Enhanced LSTM for Natural Language Inference</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>ESIM</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-HCAN</title>
    <url>/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的HCAN模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="hcan模型介绍">HCAN模型介绍</h2>
<p>HCAN模型来源于2019年的《Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling》论文。整体来说不算太难。</p>
<h3 id="hcan模型提出的原因">HCAN模型提出的原因</h3>
<p>在NLP中，存在着两大任务：relevance matching(RM)与semantic matching(SM)。所谓的RM常见于信息检索中，其目的在于根据query与documents的relevance，对documents进行排序，其核心是关键字匹配；而SM，其目的在于计算两个句子的相似度或者匹配度，其核心是句子语义上的匹配。从这个定义中，可以看出RM本质上也是语义匹配，所以RM与SM有一定的相似性。那么问题来了：<strong>能不能提出一种模型，在RM与SM上都能取得非常好的效果呢？</strong>这就是HCAN模型提出的原因。</p>
<h3 id="hcan模型架构">HCAN模型架构</h3>
<p>整个架构比较简单：word embedding层、hybrid encoder层、relevance matching与semantic matching层、最后的分类层。直接放图吧～</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/HCAN模型架构.jpg" style="zoom:50%;"></p>
<h4 id="embedding-layer">Embedding layer</h4>
<p>输入句子对，记作<span class="math inline">\((q,c)\)</span>，在QA任务中，其表示question与answer，在信息检索中，其表示query与document。其中，<span class="math inline">\(q、c\)</span>均表示句子的token的集合，它们的embedding记作：<span class="math inline">\(\{w_1^q,w_2^q,w_3^q,...,w_n^q\}\)</span>与<span class="math inline">\(\{w_1^c,w_2^c,w_3^c,...,w_m^c\}\)</span>，embeddding的维度为<span class="math inline">\(L\)</span>，<span class="math inline">\(n、m\)</span>是句子的长度，所以，<span class="math inline">\(q\in R^{n\times L}、c\in R^{m\times L}\)</span>。</p>
<h4 id="hybrid-encoders">Hybrid Encoders</h4>
<p>在得到句子的embedding后，为了进一步得到更加丰富的表示，在HCAN中，使用了三种不同类型的encoders：<strong>deep、wide、contextual。</strong></p>
<p><strong>Deep Encoder：</strong>所谓的Deep encoder，就是叠加多个卷积层，从而得到更加丰富的语义表示，在论文中，称作phrase-level representation，第<span class="math inline">\(h\)</span>个卷积层的输出记作：<span class="math inline">\(U^h\)</span>。这个和DPCNN中的region embedding有点相似，不熟悉DPCNN的，可以移步我的文章：<a href="https://codewithzichao.github.io/2020/04/25/NLP-文本分类模型-DPCNN/#more">DPCNN</a></p>
<blockquote>
<p>注意，q与c是共享这些参数的！</p>
</blockquote>
<p><strong>Wide encoder：</strong>wide encoder则是换了一种思路，并联多个卷积层，也就是“变宽”。给定<span class="math inline">\(N\)</span>个卷积层，假设第一个卷积层的窗口大小为k，那么N个卷积层的窗口大小依次是：<span class="math inline">\([k,k+1,k+2,k+3,..,k+N-1]\)</span>.</p>
<p><strong>Contextual encoder</strong>：contextual encoder不使用卷积层，转而使用BILSTM来提取更为丰富的语义特征。给定<span class="math inline">\(N\)</span>个BILSTM层，第<span class="math inline">\(h\)</span>个BILSTM层的输出是：<span class="math inline">\(U^h\)</span>。</p>
<p><strong>三种encoder的比较</strong></p>
<blockquote>
<ol type="1">
<li>deep与wide由于使用CNN，所以并行计算效率比contextual要高，训练要快；此外，使用CNN的话，我们可以控制窗口大小，能够得到不同phrase level的语义表示。同时，这样对于relevance matching也是非常有用的。</li>
<li>deep的参数数量更少，因为deep中所有层的filter是一样的，所以可以实现参数共享。</li>
</ol>
</blockquote>
<h4 id="relevance-matching">Relevance Matching</h4>
<blockquote>
<p>注意：下面所示的维度均没有带上batch_size！</p>
</blockquote>
<p>通过hybrid encoders之后，我们可以得到q与c的深层次的向量化表示：$ U_qR<sup>{nF}、U_cR</sup>{mF}$。在此基础上，我们首先要去得到relevance matching。采用的方式就是dot。公式如下： <span class="math display">\[
S=U_qU_c^T, \ \ \ \ \ \ \ \ \ \ S\in R^{n\times m} 
\]</span> 然后，在此基础上，在context列上使用softmax，公式如下： <span class="math display">\[
\tilde S=softmax(S)
\]</span> 这一步的意义在于让得到的similarity score的范围处于[0,1]，此外，softmax也能够让score之间的区分度变大。</p>
<p>然后再对其使用最大池化和平均池化（注意，是全局池化，也就是说，池化后，会减少一个维度），论文中指出，平均池化要好一点。公式如下：</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/RM.jpg" style="zoom:50%;"></p>
<p>接下来，作者还引入了IDF来作为不同的query term与phrase的权重，衡量其重要性程度。</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/IDF.jpg" style="zoom:50%;"></p>
<p>这样加权的方法也可以减小得分比较大的token(譬如停用词)的影响。</p>
<h4 id="semantic-matching">Semantic Matching</h4>
<p>Semantic Matching的输出仍然采用hybird encoders的输出，即q与c的深层次的向量化表示：<span class="math inline">\(U_q\in R^{n\times F}、U_c\in R^{m\times F}\)</span>。在此基础上，使用co-attention机制，不熟悉co-attention机制的童鞋，可以参看论文《BIDIRECTIONAL ATTENTION F LOW FOR MACHINE COMPREHENSION》，这是在MRC任务中大名鼎鼎的BIDAF模型，非常值得一看🤩。使用co-attention机制的公式如下：</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/co-attention.jpg" style="zoom:50%;"></p>
<p>其中，REP表示将input转化为n✖️m的矩阵，<span class="math inline">\(W_q,W_c\in R^{F}，W_b\in R^{F\times F}\)</span>。</p>
<p>在两个方向上使用co-attention机制：query-to-context和context-to-query。</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/co-ten.jpg" style="zoom:50%;"></p>
<p>然后对得到的向量进行concat，再使用BILSTM来进行编码，得到输出。公式如下：</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/bilstm.jpg" style="zoom:50%;"></p>
<p>注意，我们只使用BILSTM的最后一个输出。</p>
<h4 id="final-classification">Final classification</h4>
<p>对relevance matching与semantic matching得到的输出进行concat，然后使用两层的MLP，从而得到最终的输出。</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/final.jpg" style="zoom:50%;"></p>
<p>HCAN在answer selection(使用TrecQA数据集、评价指标MAP与MRR)、paraphrase identification(使用TwitterURL数据集、评价指标macro-F1)、semantic textual similarity(使用Quora数据集，评价指标准确率)、Tweet Search(TREC Microblog数据集、评价指标P@30)。结果如下：</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/结果1.jpg" style="zoom:50%;"></p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-HCAN/jieguo2.jpg" style="zoom:50%;"></p>
<p>从结果可以看到，HCAN模型吊打了DecAtt、ESIM、InferSent等一众模型，但是仍然打不过BERT。所以，要在文本匹配任务重取得更好的结果，目前还是离不开预训练模型。</p>
<h2 id="hcan模型实现">HCAN模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>HCAN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-InferSent</title>
    <url>/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-InferSent/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的InferSent模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="infersent模型介绍">InferSent模型介绍</h2>
<p>InferSent模型来源于2018年Facebook发表的《Supervised Learning of Universal Sentence Representations from Natural Language Inference Data》论文。这是一篇非常有影响力的工作。首先谈谈InferSent提出的原因吧：<strong>目前word embedding已经被广泛应用了，但是大多数时候，我们想要得到的是sentence embedding，即句子的向量化表示(甚至是段落、文章的向量化表示)，只是单纯对所有word embedding相加取平均，无法提取到有意义并且丰富的语义信息，所以怎么才能提取出有意义并且丰富的sentence embedding，并且提取的方法要非常的generalize，能在各种task中得到sentence embedding，成为了一个亟待解决的问题。</strong>从结构上来讲，InferSent是一个基于表示的文本匹配模型，但是它的真正目的是提出一种有监督的sentence embedding的学习方法，从而完成迁移学习。废话不多说，直接上图吧～</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-InferSent/InferSent架构.jpg" style="zoom:50%;"></p>
<p>由于InferSent仍然是一种基于表示的文本匹配模型，所以整个模型架构分为两个部分，下面一一介绍～</p>
<h3 id="sentence-encoder">sentence encoder</h3>
<p>这一步就是对premise与hypothesis两个句子进行encoder。作者尝试很多的方法，最后发现使用BILSTM+maxpooling的方式效果最好。</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-InferSent/architetute.jpg" style="zoom:50%;"></p>
<p>公式如下：</p>
<p><img src="/2020/05/01/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-InferSent/biLstm+max.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(w_i\)</span>表示premise或者是hypothesis的第<span class="math inline">\(i\)</span>个位置的word embedding。然后再对<span class="math inline">\(h\)</span>进行max-pooling，得到premise与hypothesis的sentence embedding：<span class="math inline">\(u,v\)</span>，其中<span class="math inline">\(u,v\in R^{2d_m}\)</span>，<span class="math inline">\(d_m\)</span>是前向LSTM或者后向LSTM输出的维度。具体可以参见我的关于SSE的文章<a href="https://codewithzichao.github.io/2020/04/29/NLP-文本匹配模型-SSE/#more">SSE模型</a></p>
<h3 id="classifier">classifier</h3>
<p>这一步就是将得到的<span class="math inline">\(u,v\)</span>以及它们之间element-wise的difference与product进行concat，得到向量<span class="math inline">\(m\)</span>。即： <span class="math display">\[
m=[u,v,|u-v|,u\bigotimes v]
\]</span> 然后将<span class="math inline">\(m\)</span>输入到dense层，激活函数为relu，最后送入softmax层中，得到最终的结果。</p>
<p>training details：BILSTM的输出维度为4096，使用Adam。</p>
<p><strong>Noooooooote：像word embedding，我们可以存储下来，等到要用的时候，直接查询embedding lookup table即可，但是sentence embedding不行，那么sentence embedding的本质是什么呢？其实没有所谓的sentence embedding算法，因为我们没有用来评价得到的sentence embedding的方法，我们只有将sentence embedding嵌入到下游任务中，通过下游任务的好坏，来评价得到的sentence embedding的质量。所以，我们追求的通用的sentence embedding，实际上我们是要得到一个网络架构，或者称作encoder，在InferSent中，指的就是sentence encoder模块。我们输入文本序列，能够通过encoder，得到文本序列的向量化表示。这才是sentence embedding的本质，或者说其与word embedding的不同之处<a href="https://www.zhihu.com/question/299549788" target="_blank" rel="noopener">链接</a>。🥰</strong></p>
<h3 id="简单唠叨一下sentence-embedding的发展">简单唠叨一下sentence embedding的发展</h3>
<p>目前来说，得到sentence embedding的方法分为监督学习与无监督学习两大类。一般来说，使用无监督学习就可以得到比较好的sentence embedding，代表性模型有SkipThought以及FastSent。监督学习的代表性模型就是InferSent，所以InferSent真的是一项非常有影响力的工作。而目前，sentence embedding的热点是多任务学习，即在一次训练中，组合不同的训练目标。代表性模型有：Universal Sentence Encoder(Google)。大致就是这样，之后有空再更新一篇关于sentence embedding的文章吧🥰～</p>
<h2 id="infersent模型实现">InferSent模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>InferSent</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-MatchPyramid</title>
    <url>/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-MatchPyramid/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的MatchPyramid模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="matchpyramid模型介绍">MatchPyramid模型介绍</h2>
<p>MatchPyramid模型来源于2016年的《Text Matching as Image Recognition》论文，用来解决文本匹配问题，即计算句子的相似度。论文的思想比较有意思，它借鉴了图像识别的思想。在图像识别中，我们通过CNN，先得到最为基本的特征，譬如：线、角、点等等，然后在这些low level的特征上，构建出更加高级的特征，譬如：眼睛、鼻子、嘴巴等等，之后再构建出整个图像。那么在文本中，是不是也可以这样来做呢？答案是肯定的！我们可以计算word level的matching signals，在此基础上，我们可以计算phrase level的matching signals，最后得到sentence level的matching signals。但是，图像一般是2D的，而文本是1D的，这个怎么解决呢？<strong>在MatchPyramid模型中，使用token与token之间的相似度当作图像中的像素，从而构建出一个matching matrix <span class="math inline">\(M\)</span>，</strong>之后使用卷积层来提取出更加丰富的matching pattern，最后得到句子之间的相似度，就解决啦！思想是不是nice～直接放图吧～</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-MatchPyramid/MatchPyramid模型.jpg" style="zoom: 50%;"></p>
<h3 id="matching-matrix">Matching Matrix</h3>
<p>从上图中可以得知，matching matrix的构建对于MP模型来说至关重要。在论文中，提到了三种构建matching matrix的方法：<strong>indicator function、cosine、dot product。</strong>如下：</p>
<ul>
<li><strong>indicator function：</strong><span class="math inline">\(M_{ij}=\left\{ \begin{matrix} 1，w_i=v_j \\ 0， otherwise \end{matrix} \right.\)</span>。其中，<span class="math inline">\(w_i、v_j\)</span> 分别表示两个句子的第 <span class="math inline">\(i\)</span> 个位置与第 <span class="math inline">\(j\)</span> 个位置的token。这种方法的缺点在于：无法捕捉到更加丰富的语义信息。</li>
<li><strong>Cosine：</strong> <span class="math inline">\(M_{ij}=\frac {\alpha_i\beta_j}{||\alpha_i||·||\beta_j||}\)</span>，其中，<span class="math inline">\(\alpha_i、\beta_j\)</span> 分别是两个句子第 <span class="math inline">\(i\)</span> 个位置与第 <span class="math inline">\(j\)</span> 个位置的token的word embedding(可以由word2vec、glove等得到)。</li>
<li><strong>dot product：</strong> <span class="math inline">\(M_{ij}=\alpha_i^{T}\beta_j\)</span>，这个没有什么可说的，就是向量相乘。</li>
</ul>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-MatchPyramid/matching matrix.jpg" style="zoom:50%;"></p>
<h3 id="hierarchical-convolution">Hierarchical Convolution</h3>
<p>在得到matching matrix之后，我们只是得到的word level的matching pattern，这远远不够。所以，我们要搞事情啦！由于卷积层提取特征的能力远远超过分类层，所以使用卷积层能够提取到更为丰富的matching pattern。具体如下：</p>
<ul>
<li>在模型的第一层，<strong>我们使用一个2维卷积，</strong>注意：<strong>这个2维卷积层采用的是方核，并且激活函数使用relu。</strong></li>
<li>在模型的第二层，我们使用动态的pooling。pooling kernel的维度根据文本长度会发生变化。</li>
<li>当然啦，我们可以复用上述两个block，从而得到更加高层次语义特征。</li>
<li>最后，我们加一个MLP层，得到最终的每一个class(0,1)的matching score。对于得到的matching score，我们使用softmax作为分类器，cross entropy作为loss function。 训练时，我们使用SGD，优化器使用AdaDelta。</li>
</ul>
<p>作者在Paraphrase Identiﬁcation（MSRP datasets）与Paper Citation Matching两个task上进行了实验，都取得了非常的效果。<strong>发现使用dot product构建的Matching matrix得到的结果最好。</strong>如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-MatchPyramid/结果.jpg" style="zoom:50%;"></p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-MatchPyramid/结果1.jpg" style="zoom:50%;"></p>
<p><strong>另外提一句，MatchPyramid也有局限性，在于：MP模型只适合于相似度主要在结构上，并且数据量较少的情况，像这个case：我写信写了一个晚上/这封信写了我一个晚上，是不适合MP模型的。</strong></p>
<p>以上就是MatchPyramid模型的全部内容啦～</p>
<h2 id="matchpyramid模型实现">MatchPyramid模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Deep Pyramid Convolutional Neural Networks for Text Categorization</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>MatchPyramid</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-SSE</title>
    <url>/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的SSE模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="sse模型介绍">SSE模型介绍</h2>
<p>SSE模型来源于2017年《Shortcut-Stacked Sentence Encoders for Multi-Domain Inference》论文，用于解决NLI问题。SSE模型主要是基于InferSent模型，然后借鉴了resNet的残差连接的思想，从而在SNLI上取得了SOTA。直接放图吧～</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/SSE模型架构.jpg" style="zoom:50%;"></p>
<p>SSE模型是基于表示的文本匹配模型，所以大体上分为两步：<strong>sentence encoder、entailment classifier。</strong>下面一一介绍～</p>
<h3 id="sentence-encoder">sentence encoder</h3>
<p>这一步，SSE模型主要是通过stack多层的biLSTM，从而得到更加深层语义信息。这其实没有什么新奇的，在SiamLSTM、ESIM、InferSent上都用到了，但是不同的在于：<strong>第 <span class="math inline">\(i\)</span> 个blLSTM层的输入是：前 <span class="math inline">\(i-1\)</span> 个biLSTM层的输出与最初的word embedding的concat。这也是SSE模型的改进的地方。</strong>公式如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/bilstm.jpg" style="zoom:50%;"></p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/bilstm2.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(h_t^i\)</span> 表示第<span class="math inline">\(i\)</span>个biLSTM层的第<span class="math inline">\(t\)</span>个位置的输出；<span class="math inline">\(x_t^i\)</span>表示第<span class="math inline">\(i\)</span>个biLSTM层的第<span class="math inline">\(t\)</span>时刻的输入；<span class="math inline">\(w_t\)</span>是最初token的word embedding。假设有m层，那么最后一层的输出如下：</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/finalbiLSTM.jpg" style="zoom:50%;"></p>
<p>其中，<span class="math inline">\(H^m\)</span>表示第m层的输出，长度与原语句长度一致，也就是说，stack这么多层的biLSTM，就是为了能够提取出更加深层的语义信息；<span class="math inline">\(v\)</span>表示对<span class="math inline">\(H^m\)</span>进行maxpooling之后得到的结果。<strong>在这里需要理清一下各个向量之间的维度关系：<span class="math inline">\(h_i^m\in R^{2d_m}，H^m\in R^{2d_m\times n}，v\in R^{2d_m}\)</span>。</strong></p>
<p><strong>Noooooote：premise <span class="math inline">\(p\)</span> 与hypothesis <span class="math inline">\(h\)</span> 都是经过上述的网络结构，得到结果向量 <span class="math inline">\(v_p\)</span>与<span class="math inline">\(v_h\)</span>，也就是说共享参数！这个在《APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK》论文里有讨论过！</strong></p>
<h3 id="entailment-classifier">entailment classifier</h3>
<p>通过sentence encoder，我们得到两个句子的向量化表示<span class="math inline">\(v_p\)</span>与<span class="math inline">\(v_h\)</span>，首先要对其进行concat，得到matching vector <span class="math inline">\(m\)</span>，公式如下： <span class="math display">\[
m=[v_p,v_h,|v_p-v_h|,v_p\bigotimes v_h]
\]</span> 然后将concat之后<span class="math inline">\(m\)</span>输入一个dense层，使用relu进行激活，然后输入到softmax，得到最终的结果。</p>
<p><strong>training details：</strong>数据集使用Multi-SNLI；优化器为Adam；batch size为32；初始学习率为0.0002，每两个epoch衰减一半；dense层的输出维度为1600，对dense层输出的结果使用dropout，rate为0.1；使用Glove词向量，并在训练过程中微调；<strong>实验证明，使用3层的biLSTM的效果最好，每一层的输出维度依次是：512，1024，2048。</strong>下面是实验结果。</p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/Xnip2020-05-02_16-00-48.jpg" style="zoom:50%;"></p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/Xnip2020-05-02_16-01-39.jpg" style="zoom:50%;"></p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/Xnip2020-05-02_16-01-47.jpg" style="zoom:50%;"></p>
<p><img src="/2020/04/29/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SSE/Xnip2020-05-02_16-01-53.jpg" style="zoom:50%;"></p>
<p>以上就是SSE模型的全部内容啦！效果是比InferSent好，但是估计训练时间要多得多。🥰</p>
<h2 id="sse模型实现">SSE模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Shortcut-Stacked Sentence Encoders for Multi-Domain Inference</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>SSE</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-SiamCNN</title>
    <url>/2020/04/27/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SiamCNN/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的SiamCNN模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="siamcnn模型介绍">SiamCNN模型介绍</h2>
<p>SiamCNN模型来源于《APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK》(后来查了一下，SiameseCNN最初用来解决图像问题，anyway，都是相通的🤩)。这个模型要解决的就是<strong>answer selction(问答匹配)问题</strong>。什么是问题匹配问题呢？如下：</p>
<p>给定一个问题 <span class="math inline">\(q\)</span> 以及问题 <span class="math inline">\(q\)</span> 的答案候选池<span class="math inline">\(\{a_1,a_2,...,a_s\}\)</span>，通过模型，从中选出最好的答案<span class="math inline">\(a_k\)</span>，如果说选出的答案 <span class="math inline">\(a_k\)</span> 在问题 <span class="math inline">\(q\)</span> 的truth set里面的的话，那么，就说问题<span class="math inline">\(q\)</span>被正确的回答了，否则就说明问题 <span class="math inline">\(q\)</span> 没有被很好地回答。这就是问答匹配问题。</p>
<p>定义问题之后，接下来就是给出模型架构。SiamCNN模型的架构比较简单：<strong>首先学习得到给定的问题及其候选答案池的向量化表示，然后使用相似性度量方法来衡量匹配度。</strong>模型的训练实例的构造方式或者说是学习策略，是<strong>pairwise</strong>。即：我们构建的训练实例是：<span class="math inline">\((q_i,c_i^+,c_i^-)\)</span>，其中 <span class="math inline">\(q_i\)</span> 表示问题，<span class="math inline">\(c_i^+\)</span> 表示给定问题的一个正确答案，<span class="math inline">\(c_i^-\)</span> 表示给定问题的一个错误答案。损失函数为hinge loss，公式是：<span class="math inline">\(L=max\{0,m-h_\theta(q_i,c_i^+)+h_\theta(q_i,c_i^-)\}\)</span>，其中，<span class="math inline">\(m\)</span>表示边界阈值，如果说<span class="math inline">\(L\)</span>大于0，那么说明模型把错误答案排在正确答案之前；如果说<span class="math inline">\(L\)</span>等于0，那么说明模型把正确答案排在错误答案之前。所以hingle loss的目的就是要促使正确答案的得分比错误答案的得分高m。在预测阶段，得分最高的候选答案被当作正确答案。</p>
<p><img src="/2020/04/27/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SiamCNN/simaCNN模型.jpg"></p>
<p>上面这张图就是作者给出的SiamCNN模型的结构。作者做了很多实验，对架构做了很多改进。最后发现第二中架构是最好的，即：<strong>首先是tanh层，也就是对Q、A的向量化表示进行激活，但是两者共享权重参数；第二层是卷积层，同样也是共享权重参数；第三层是max ppoling层，一般是使用1 max pooling，选出与input最相关的特征；第四层是tanh层；最后根据相似度衡量方法来得到结果。</strong></p>
<p><img src="/2020/04/27/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SiamCNN/实验结果.jpg"></p>
<p>这是作者的实现结果，第10行就是第二个架构的结果。经过实验之后，作者给出了几个有意思的结论：</p>
<ul>
<li>参数共享层的使用比参数分开的情况，效果要好得多；</li>
<li>不需要在卷积层之后，再添加hidden layer，因为卷积层已经能够很好的提取出有用的特征，而加入hidden layer反而会降低效果；</li>
<li>提高filter的数量会提高效果；</li>
<li>多层的卷积层能够提取更加高层次的语义特征，也可以提高效果；</li>
<li>不同的相似度衡量方法会对结果产生很大的影响，所以要慎重选取相似度衡量方法。</li>
</ul>
<p>以上就是SiamCNN模型的内容，我个人觉得这是一篇非常好的文章，虽然内容较为简单，但是给了我们很多启发，之后，我们在做文本匹配的时候，也可以参考。当然啦，在BERT面前，这些模型都显得逊色许多，不过仍然非常具有启发性。🤩</p>
<h2 id="siamcnn模型实现">SiamCNN模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>SiamCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|文本匹配模型-SiamLSTM</title>
    <url>/2020/04/28/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SiamLSTM/</url>
    <content><![CDATA[<p>正式开始看文本匹配的东西啦！文本匹配对NLPer来说是很重要的，不管是最后是做对话、推荐、搜索，文本匹配都是必不可少的。当然啦，BERT系列的模型出来之后，其实传统的深度学习模型效果是远远比不上的。不过这些预训练模型效果好是好，但是训练代价昂贵，当然啦，有人会说，现在已经有剪枝、量化、蒸馏这样的方法来减小预训练模型的大小，从而降低训练所需的代价(所以说模型压缩、加速这个方向还是很有前景的🤩咦，好像跑偏了，anyway)，但是这仍然远远不够，所以熟悉传统的文本匹配模型是非常有必要的。本篇博客讲解经典的SiamLSTM模型，并采用tensorflow2实现。</p>
<a id="more"></a>
<h2 id="siamlstm模型介绍">SiamLSTM模型介绍</h2>
<p>SiamLSTM模型来源于《Siamese Recurrent Architectures for Learning Sentence Similarity》论文，是在2016年提出的。SiamLSTM模型所要解决的是<strong>预测句子之间的相似度</strong>。SiamLSTM模型的架构和SiamCNN非常地类似，毕竟都是Siamese神经网络嘛🥰。直接放图吧～</p>
<p><img src="/2020/04/28/NLP-%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D%E6%A8%A1%E5%9E%8B-SiamLSTM/Siam.jpg" style="zoom: 67%;"></p>
<p>模型本身并不是很难，不过有两个地方需要注意：第一个是，两个句子最后使用的是只有最后一个step的hidden state；第二个是，SiamLSTM所使用的相似度的计算方法（1范数），如下： <span class="math display">\[
g(h_{T_a}^{(a)},h_{T_b}^{(b)})=exp(-||h_{T_a}^{(a)},h_{T_b}^{(b)}||_1)\in[0,1]
\]</span> 最后，SiamLSTM模型使用的是SemEval 2014数据集。训练集：SemEval 2014 train data，共5000条 测试集：SemEval 2014 test data， 共4927条。每条数据的形式是：<code>[“I like playing basketball.”,”My favourite sports is basketball.”, 3.4]</code>。由于SiamLSTM使用的相似度计算方法，导致结果是在<span class="math inline">\([0,1]\)</span>区间的，所以最后需要对结果进行缩放，从而在<span class="math inline">\([1,5]\)</span>之间。SiamLSTM模型使用的损失函数是MSE。</p>
<p>论文提到的训练的一些details：<strong>使用AdaDelta，使用梯度裁剪，使用SGD，LSTM的输出维度是50。</strong></p>
<p>以上就是SiamLSTM模型模型的全部内容啦🥰（说实话，最近看文本匹配的论文，真的感觉不难，比起之前看reformer、BERT等的文章，真的是太轻松啦，希望之后不要打脸🥰）</p>
<h2 id="siamlstm模型实现">SiamLSTM模型实现</h2>
<h2 id="参考文献">参考文献</h2>
<p>Siamese Recurrent Architectures for Learning Sentence Similarity</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>文本匹配模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>tensorflow2</tag>
        <tag>SiamLSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|预训练模型-SentenceBERT</title>
    <url>/2020/08/04/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-SentenceBERT/</url>
    <content><![CDATA[<p>这篇文章主要是记录一下SBERT模型，其实这个模型思路很简单，基本上就是将InferSent中的BILSTM换成了BERT。但是呢，这篇文章对sentence embedding做了较为详细的介绍，读完之后，对sentence embedding有了较为详细的了解，所以，记录一下～</p>
<a id="more"></a>
<h2 id="sentence-bert模型">Sentence-BERT模型</h2>
<p>BERT模型在各种NLU任务中基本上是都远超传统的深度学习模型，但是仍然有不足。譬如在文本匹配(or文本相似度计算)任务中，BERT的做法是：将两个句子用[SEP]token连接起来作为一个句子，再输入到BERT中。但是这么做的缺点是：当我们的数据量特别大的时候，论文说的是从10000个句子中找到与目标句子最为相似的句子，花费的时间为65小时，这显然是非常低效的。另外，也有研究人员将单个句子输入到BERT当中，对最后一个transformer layer进行平均或者只使用[CLS]token的张量，但是效果还不如GloVe embedding。于是，为了降低文本匹配的计算量与时间，并得到好的sentence embedding，就有了Sentence-BERT模型。</p>
<h3 id="sentence-bert模型架构">Sentence-BERT模型架构</h3>
<p>根据不同的数据组织形式，SBERT有着不同的结构。在论文当中，提出了三种情况下的模型结构，如下：</p>
<ul>
<li>当输入数据是[sentence1,sentence2,label]，label是离散的，(对应的任务有NLI/RTE)，具体结构如下：</li>
</ul>
<p><img src="/2020/08/04/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-SentenceBERT/lisan.jpg"></p>
<p>这个结构完全借鉴于InferSent模型，只不过是将BILSTM部分用BERT来进行替换，注意：两个句子使用的BERT部分共享权重。对于BERT部分输出的结果，论文中探究了不同的pooling策略：只使用[CLS]token对应的张量、使用Global max pooling、使用Global avg pooling。(最终是avg pooling策略或者只使用[CLS]token对应的张量最好，这与InferSent中所得出的结论是不一样的，InferSent中使用max pooling最好)。得到<span class="math inline">\(u\)</span> 和<span class="math inline">\(v\)</span>这两个句向量之后，然后再进行拼接，在这里，去掉了<span class="math inline">\(u*v\)</span>这一部分，因为实验表明，加了这一部分，效果反而下降了。最后将拼接后的结果输入到softmax中进行分类即可。loss function采用交叉熵。数据采用SNLI和MultiNLI进行fine-tune，pre-train直接采用使用wikipedia预训练得到的权重即可。</p>
<blockquote>
<p>优化器采用Adam，16 batch size，warm up 16%，pooling策略是avg。</p>
</blockquote>
<ul>
<li>当输入数据是[sentence1,sentence2,label]，label是连续的，(对应的任务有STS(语义相似度计算))，具体结构如下：</li>
</ul>
<p><img src="/2020/08/04/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-SentenceBERT/lianxu.jpg"></p>
<p>整体结构与第一种相同，不同的地方是：在得到各自的句向量之后，我们使用余弦相似度来进行度量，当然也可以使用其他的譬如皮尔森相关系数。loss function采用MSE。注意：论文里说没有任何与STS相关的数据进行训练，也就是说，在使用第一种架构进行fine-tune之后，我们将拼接与softmax给拿掉，换成cosine similarity就可以了。</p>
<ul>
<li>当输入数据是[query，positive_answer，negative_answer],（对应的任务有问答匹配)，具体结构与第一种结构相同，但是采用的loss function为triplet loss，公式如下：</li>
</ul>
<p><span class="math display">\[
loss=max(||s_a-s_p||-||s_a,s_n||+\epsilon,0)
\]</span></p>
<p>也就是说，我们希望query与positive answer的距离比query与negative answer的距离要小至少<span class="math inline">\(\epsilon\)</span>。这样的话，譬如在问答匹配里面，我们最后就会更加倾向于召回正确的回答，而不是错误的回答。在论文中，衡量query句向量与答案句向量之间的距离采用的是欧几里得距离，<span class="math inline">\(\epsilon=1\)</span>。</p>
<h3 id="实验结果">实验结果</h3>
<p><img src="/2020/08/04/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-SentenceBERT/result1.jpg"></p>
<p><img src="/2020/08/04/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-SentenceBERT/result2.jpg" style="zoom:50%;"></p>
<p><img src="/2020/08/04/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B-SentenceBERT/result3.jpg" style="zoom:50%;"></p>
<p>从结果来看，确实要比普通的InferSent或者USE要好。另外，我觉得这篇论文更大的意义在于实际应用吧，整体没有太大的创新。</p>
<h2 id="universal-sentence-encoder模型">Universal Sentence Encoder模型</h2>
<p>打算再来看一下sentence embedding的部分吧，因为之前被问到了，我却只答上了InferSent以及一些普通的方案。。。惭愧啊。其实除了InferSent之外，USE也是非常著名的一个模型，下面具体讲解一下～大致来说，就是USE有两个encoder：一个是transformer，一个是DAN，前者准确率高但是计算代价大，后者准确率略有下降但是计算代价小，两者都在多个数据上进行迁移学习。</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Sentence-BERT</tag>
        <tag>预训练模型</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|LSTM与GRU</title>
    <url>/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/</url>
    <content><![CDATA[<p>到目前为止，已经学了很多东西，但是没有输出，总感觉似乎少了点什么。这篇博客将回顾经典的LSTM与GRU。LSTM与GRU是RNN中最为经典的unit，它的提出解决了RNN中梯度消失的问题，非常地具有开创性。本文将具体探究其原理细节。</p>
<a id="more"></a>
<blockquote>
<p>本文的符号体系借鉴andrew Ng的《deep learning》课程。</p>
</blockquote>
<h2 id="为什么要有lstm">为什么要有LSTM？</h2>
<p>首先我们来回顾一下基本的RNN网络架构：</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/RNN.jpg"></p>
<p><strong>前向传播过程：</strong></p>
<p>t时刻的激活值与输出值的计算公式如下：</p>
<p><span class="math display">\[a^{&lt;t&gt;}=tanh(W_a[a^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_a)\]</span></p>
<p><span class="math display">\[y^{&lt;t&gt;}=g(W_ya^{&lt;t&gt;}+b_y)\]</span></p>
<blockquote>
<p>注意：同一层，所有的<span class="math inline">\(W_a\)</span>参数都是共享的，所有的<span class="math inline">\(W_y\)</span>参数都是共享的，所有的<span class="math inline">\(b_a\)</span>参数都是共享的，所有的<span class="math inline">\(b_y\)</span>参数都是共享的。</p>
</blockquote>
<p><strong>反向传播过程（BPTT）：</strong></p>
<p>确定优化参数：<span class="math inline">\(W_a\)</span>、<span class="math inline">\(W_y\)</span>、<span class="math inline">\(b_a\)</span>、<span class="math inline">\(b_y\)</span>。</p>
<p>接下来需要定义loss function，以二分类问题为例，使用后交叉熵，则计算公式如下：</p>
<p><span class="math display">\[L^{&lt;t&gt;}(y^{&lt;t&gt;},\widehat{y}^{&lt;t&gt;})=-y^{&lt;t&gt;}log(\widehat{y}^{&lt;t&gt;})-(1-y^{&lt;t&gt;})log(\widehat{y}^{&lt;1-t&gt;})\]</span></p>
<p><span class="math display">\[L(y^{&lt;t&gt;},\widehat{y}^{&lt;t&gt;})=\sum_{t=1}^{T_x}L^{}(y^{&lt;t&gt;},\widehat{y}^{&lt;t&gt;})\]</span></p>
<p>之后不断使用SGD/mini-batch GD或者其他的优化算法，优化参数：<span class="math inline">\(W_a\)</span>、<span class="math inline">\(W_y\)</span>、<span class="math inline">\(b_a\)</span>、<span class="math inline">\(b_y\)</span>，便可完成整个反向传播过程。</p>
<p><strong>梯度消失</strong></p>
<p>但是RNN与传统的神经网络一样，如果随着序列长度的增加，会存在梯度消失问题。原因在于：当我们对进行反向传播的时候，譬如我们计算<span class="math inline">\(W_a\)</span>，我们会得到这个式子：<span class="math inline">\(W_a=\prod_{t=1}^{T_x}{tanh}&#39;W_a\)</span>。由于tanh函数的导数在(0,1)之间，如果在反向传播的时候，当参数<span class="math inline">\(W_a\)</span>初始化为小于1的数，那么偏导相乘的结果就会远远小于1，从而导致梯度消失。但是由于RNN中梯度相加，所以梯度永远不会等于0。参看链接：<a href="https://blog.csdn.net/jizhidexiaoming/article/details/81743584" target="_blank" rel="noopener">RNN梯度消失的原因</a></p>
<p>由于RNN梯度消失的问题，就会导致梯度被近距离主导，从而导致模型难以学到远距离的依赖关系（长时期依赖问题）。这也就是为什么LSTM被提出的原因，因为LSTM能够很好地解决这一问题。</p>
<h2 id="lstm原理剖析">LSTM原理剖析</h2>
<p>LSTM，全称long-short term memory networks。LSTM的设计就是为了避免上节所说的长时期依赖问题，能够记住很长时间内地信息。</p>
<h3 id="rnn与lstm内部结构">RNN与LSTM内部结构</h3>
<p>RNN内部结构如下。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/2.RNN内部结构.png"></p>
<p>LSTM内部结构如下。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/3.lstm内部结构.png"></p>
<h3 id="lstm内部结构剖析">LSTM内部结构剖析</h3>
<h4 id="遗忘门">遗忘门</h4>
<p>遗忘门决定要保留哪些信息继续通过当前的cell。<span class="math inline">\(f_t\)</span>通过sigmoid函数实现，从而将<span class="math inline">\(f_t\)</span>限制在(0,1)之间。0表示不让任何信息通过，1表示让所有信息通过。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/4.遗忘门.png"></p>
<h4 id="输入门">输入门</h4>
<p>输入门决定让多少新的信息加入到当前cell的状态中。这分成两步：首先通过sigmoid函数得到输入门<span class="math inline">\(i_t\)</span>，再通过tanh得到备选的要更新的内容。最后，联合这两部分，从而对当前cell的状态进行更新。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/5.更新门.png"></p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/6.更新节点状态.png"></p>
<h4 id="输出门">输出门</h4>
<p>输出门决定要输出哪些信息，传递给下一个cell。当前cell的输出是依赖于<span class="math inline">\(C_t\)</span>的值，但是不完全依赖于<span class="math inline">\(C_t\)</span>的值，而是使用tanh函数对其进行了过滤。而输出门<span class="math inline">\(o_t\)</span>通过sigmoid函数得到一个(0,1)的值，从而决定输出过滤后的<span class="math inline">\(C_t\)</span>值的哪些部分。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/7.输出门.png"></p>
<h2 id="lstm变种gru">LSTM变种—GRU</h2>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/8.GRU.png"></p>
<h2 id="lstm变种peehole-connection">LSTM变种—peehole connection</h2>
<p><img src="/2020/02/17/NLP%EF%BD%9CLSTM%E4%B8%8EGRU/8.peehole%20connection.png"></p>
<p>##一些细节</p>
<p>1.LSTM为什么能够解决梯度消失？</p>
<p>答：秘诀就在这个公式: <span class="math display">\[C_t=i_t*\widehat{C}_t+f_t*C_{t}\]</span>。在RNN中，每个记忆单元h_t-1都会乘上一个W和激活函数的导数，这种连乘使得记忆衰减的很快，而LSTM是通过记忆和当前输入"相加"，使得之前的记忆会继续存在而不是受到乘法的影响而部分“消失”，因此不会衰减。因为LSTM对记忆的操作是相加的，线性的，使得不同时序的记忆对当前的影响相同，为了让不同时序的记忆对当前影响变得可控，LSTM引入了输入门和输出门，之后又有人对LSTM进行了扩展，引入了遗忘门。</p>
<p>2.RNN中为什么要使用tanh？为啥不使用Relu？</p>
<p>答：可以用，但是不用，原因有两个：1.relu函数的值域是0到无穷，如果直接直接把激活函数换成ReLU，会导致非常大的输出值，会造成溢出；2.使用relu仍然无法解决梯度在长程上传递的问题。当反向传播的时候，最后计算的结果会变成多个W连乘，如果W中存在特征值&gt;1的，那么经过BPTT连乘后得到的值会爆炸，产生梯度爆炸的问题，使得RNN仍然无法传递较远距离。</p>
<p>##参考链接</p>
<p>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LSTM</tag>
        <tag>GRU</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch的零碎笔记</title>
    <url>/2020/07/11/Pytorch%E7%9A%84%E9%9B%B6%E7%A2%8E%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>最近在学pytorch，虽然与TF2很类似了，但是感觉很多地方还是不太一样，这篇文章主要就是记录一下学习pytorch中遇到的api。</p>
<a id="more"></a>
<p><strong>torch.tensor与torch.from_numpy</strong></p>
<p>Torch.tensor可以将numpy对象转化为tensor，但是得到的tensor与原来的数据不共享内存；</p>
<p>torch.from_numpy也可以将numpy对象转化为tensor，但是得到的tensor与原来的数据共享内存。</p>
<p><strong>nn.Conv2d</strong></p>
<p>nn.Conv2d(in_channels,out_channels,kernel_size,strides=1,padding=0)</p>
<p>其中，in_channels表示输入的channels数目，out_channels表示输出的channels数目，kernel_size表示卷积核大小，一般是一个数字或者一个tuple。<strong>注意：在nn.conv2d</strong>中，无激活函数，需要使用nn.functional中的函数来激活。输入的维度是(batch_size,channels,height,width)，与tensorflow不太一样。</p>
<p><strong>nn.MaxPool2d</strong></p>
<p>nn.MaxPool2d(kernel_size,stride=0,padding=1)</p>
<p>输入维度是(batch_size,channels,height,width)，输出维度是：(batch_size,channels,height_out,width_out)。</p>
<p><strong>nn.Linear</strong></p>
<p>输入的维度是:(batch_size,seq_length,hidden_size)，这个与卷积层的输入是不一样的！</p>
<p><strong>nn.LSTM(input_size=10,hidden_size=20,num_layers=1,batch_first=False,bidirectional=False)</strong></p>
<p>Input_size表示输入的张量的最后一个维度</p>
<p>hidden_size表示输出的张量的最后一个维度</p>
<p>num_layers表示叠加的LSTM的数目，默认为1</p>
<p>batch_first=True表示batch_size放在第0维，那么输入的维度是：(batch_size,seq_length,input_size)；默认为false，即输入的维度是：(seq_length,batch_size,input_size)</p>
<p>bidirectional=True表示是BILSTM；默认为false。</p>
<p><strong>pytorch中的各种loss function<a href="https://www.jianshu.com/p/579a0f4cbf24" target="_blank" rel="noopener">链接</a></strong></p>
<p>nn.NLLLoss()表示负对数似然函数，适用于多分类，在使用这个之前，需要使用nn.LogSoftmax;</p>
<p>nn.CrossEntropyLoss表示交叉熵损失函数，适用于多分类，在使用这个之前，不需要使用softmax，只需要得到logits就可以了，结果与nn.NLLLoss结果一样；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_input=torch.randn(<span class="number">3</span>,<span class="number">3</span>)<span class="comment">#随机生成输入 </span></span><br><span class="line">print(<span class="string">'x_input:\n'</span>,x_input) </span><br><span class="line">y_target=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>])<span class="comment">#设置输出具体值 print('y_target\n',y_target)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算输入softmax，此时可以看到每一行加到一起结果都是1</span></span><br><span class="line">softmax_func=nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">soft_output=softmax_func(x_input)</span><br><span class="line">print(<span class="string">'soft_output:\n'</span>,soft_output)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在softmax的基础上取log</span></span><br><span class="line">log_output=torch.log(soft_output)</span><br><span class="line">print(<span class="string">'log_output:\n'</span>,log_output)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。</span></span><br><span class="line">logsoftmax_func=nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">logsoftmax_output=logsoftmax_func(x_input)</span><br><span class="line">print(<span class="string">'logsoftmax_output:\n'</span>,logsoftmax_output)</span><br><span class="line"></span><br><span class="line"><span class="comment">#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True</span></span><br><span class="line">nllloss_func=nn.NLLLoss()</span><br><span class="line">nlloss_output=nllloss_func(logsoftmax_output,y_target)</span><br><span class="line">print(<span class="string">'nlloss_output:\n'</span>,nlloss_output)</span><br><span class="line"></span><br><span class="line"><span class="comment">#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样</span></span><br><span class="line">crossentropyloss=nn.CrossEntropyLoss()</span><br><span class="line">crossentropyloss_output=crossentropyloss(x_input,y_target)</span><br><span class="line">print(<span class="string">'crossentropyloss_output:\n'</span>,crossentropyloss_output)</span><br></pre></td></tr></table></figure>
<p><strong>np.random中的各种api</strong></p>
<ul>
<li>np.random.normal(size=(3,4,5))</li>
<li>np.random.randint(low=1,high=10,size=(3,4,5))</li>
<li>np.random.randn(3,4,5)</li>
<li>np.random.choice(tensor,size=(3,4,5))</li>
<li>np.random.rand(3,4,5)</li>
</ul>
<p><strong>permute</strong></p>
<p>一般我们都是使用permute来交换维度，一般不用transpose，注意在使用permute的时候，我们是:x.permute(1,2,3)。</p>
<p><strong>view与reshape</strong><a href="https://www.jianshu.com/p/035d174eb5a6" target="_blank" rel="noopener">讲解</a></p>
<p>如果我们在使用view之前，使用了transpose与permute等操作，我们一般要紧跟着contiguous()，然后再调用view，要不然会报错；</p>
<p>如果我们不想使用contiguous+view这样的操作，我们也可以使用reshape，但是我们会创建新的tensor。</p>
]]></content>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>SOMETHING RIGHT IN MY HEART</title>
    <url>/2020/11/03/SOMETHING-RIGHT-IN-MY-HEART/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="Oh, these decrypted content cannot be verified, but you can still have a look."><div class="hbe-input-container"><input type="password" id="hbePass" placeholder="Hey, password is required here." /><label>Hey, password is required here.</label><div class="bottom-line"></div></div><script id="hbeData" type="hbeData" data-hmacdigest="f7c455b21ace961ff2a908296b528ea7be1fbe1bbc034c9851bc958b00c9a587">cc9e447045b3911cca48686c7cd14e3aad5bd800abf1c1247acdb3830958df4aaeba2e56dae48196fdd7b714c58279e48a5c3589b805718ee5daca7324b09e1f8e5b7d16c7fe42f25cbae48c53e0a926dc41fc4b7b9d90b352be6e973e3cfe95</script></div><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <tags>
        <tag>SomethingRight</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow1.x的零碎笔记</title>
    <url>/2020/10/04/Tensorflow1-x%E7%9A%84%E9%9B%B6%E7%A2%8E%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>世事艰难，即便tensorflow2.0已经出来了，但是工业界全都是1.x的代码，还是得学tensorflow1.x啊😭。这篇文章主要是记录一下学习tensorflow1.x中遇到的问题及解决方案。</p>
<a id="more"></a>
<p><strong>1.tensorboard</strong></p>
<p>关于tensorboard，我们是使用命令<code>tensorboard --logdir='your logs path'</code>.一般使用tensorbaord，都是为了查看graph的结构，以及各种loss、参数的变化，我们需要用到<code>tf.summary</code>。具体步骤如下：<a href="https://www.cnblogs.com/baby-lily/p/10931302.html" target="_blank" rel="noopener">link</a></p>
<ul>
<li><p>首先将需要可视化的全部写入summary；</p></li>
<li><p>然后合并所有summary；</p></li>
<li><p>创建summary_file_writer，将图写入文件；</p></li>
<li><p>不断的写入数据。</p></li>
</ul>
<p>另外，常用的summary有：</p>
<ul>
<li><code>tf.summary.scalar("loss",loss)</code>，常用于loss，accuracy等；</li>
<li><code>tf.summary.histogram(layer_name+"/weight",W)</code>，用于显示参数的变化情况。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"KMP_DUPLICATE_LIB_OK"</span>]=<span class="string">"TRUE"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_data=np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:,np.newaxis].astype(np.float32)</span><br><span class="line">noise=np.random.normal(<span class="number">0</span>,<span class="number">0.5</span>,x_data.shape)</span><br><span class="line">y_data=np.square(x_data)<span class="number">-0.5</span>+noise</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs,in_size,out_size,hidden_layer_name,activation_func=None)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义自己的层</span></span><br><span class="line"><span class="string">    :param inputs:输入的张量</span></span><br><span class="line"><span class="string">    :param in_size: 输入的维度</span></span><br><span class="line"><span class="string">    :param out_size: 输出的维度</span></span><br><span class="line"><span class="string">    :param activation_func: 激活函数</span></span><br><span class="line"><span class="string">    :return: 输出张量</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    hidden_size=inputs.shape[<span class="number">1</span>].value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hidden_size != in_size:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"wrong demension!"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(hidden_layer_name):</span><br><span class="line">        W=tf.get_variable(name=<span class="string">'W'</span>,shape=[in_size,out_size],dtype=tf.float32,initializer=tf.random_normal_initializer())</span><br><span class="line">        tf.summary.histogram(hidden_layer_name+<span class="string">"/weight"</span>,W)</span><br><span class="line">        b=tf.get_variable(name=<span class="string">'b'</span>,shape=[<span class="number">1</span>,out_size],dtype=tf.float32,initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        tf.summary.histogram(hidden_layer_name + <span class="string">"/bias"</span>, b)</span><br><span class="line"></span><br><span class="line">    a=tf.matmul(inputs,W)+b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation_func == <span class="literal">None</span>:</span><br><span class="line">        outputs=a</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs=activation_func(a)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">xs=tf.placeholder(dtype=tf.float32,shape=[<span class="literal">None</span>,<span class="number">1</span>])</span><br><span class="line">ys=tf.placeholder(dtype=tf.float32,shape=[<span class="literal">None</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">hidden_result=add_layer(xs,<span class="number">1</span>,<span class="number">10</span>,<span class="string">"hidden_layer"</span>,tf.nn.relu)</span><br><span class="line">outputs=add_layer(hidden_result,<span class="number">10</span>,<span class="number">1</span>,<span class="string">"output_layer"</span>,<span class="literal">None</span>)</span><br><span class="line">tf.summary.histogram(<span class="string">"output_layer/outputs"</span>,outputs)</span><br><span class="line"></span><br><span class="line">loss=tf.reduce_mean(tf.reduce_sum(tf.square(ys-outputs),axis=<span class="number">-1</span>),axis=<span class="number">-1</span>)</span><br><span class="line">tf.summary.scalar(<span class="string">"loss"</span>,loss)</span><br><span class="line">train_step=tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">init=tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">fig=plt.figure()</span><br><span class="line">ax=fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">ax.scatter(x_data,y_data)</span><br><span class="line">plt.ion()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    merged_summary_all=tf.summary.merge_all()</span><br><span class="line">    summary_writer=tf.summary.FileWriter(<span class="string">"logs/"</span>,sess.graph)</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</span><br><span class="line">        summary_str=sess.run(merged_summary_all,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</span><br><span class="line">        summary_writer.add_summary(summary_str,step)</span><br><span class="line">        print(step,sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_data&#125;))</span><br></pre></td></tr></table></figure>
<p>15308202070</p>
]]></content>
      <tags>
        <tag>tensorflow1.x</tag>
      </tags>
  </entry>
  <entry>
    <title>github上传项目指南</title>
    <url>/2020/02/17/github%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<p>今天刷完了《剑指offer》(repo:<a href="https://github.com/codewithzichao/CodingInterviewCode" target="_blank" rel="noopener">CodingInterviewCode</a>)，撒花🎉并且，拾起了早早就注册使用却没有好好地管理的我的github。因此，总结一下使用git上传本地项目到自己的github上的流程，以便以后查阅。</p>
<a id="more"></a>
<p><strong>目标：</strong>上传本地项目到自己github的repo中。</p>
<p>具体流程如下：</p>
<h3 id="检查git是否安装">检查git是否安装</h3>
<p>输入git --version查看本地是否已经安装git。</p>
<p><img src="/2020/02/17/github%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%8D%97/1.git版本.jpg"></p>
<h3 id="创建ssh">创建ssh</h3>
<p>在指定的目录下创建ssh，命令：<code>ssh-keygen -t rsa -C xxx@xx.com</code>。</p>
<p><img src="/2020/02/17/github%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%8D%97/2.创建ssh.jpg"></p>
<h3 id="打开id_rsa.pub文件复制其内容到gtihub中">打开id_rsa.pub文件，复制其内容到gtihub中</h3>
<p>打开id_rsa.pub文件，使用命令：<code>open id_sra.pub</code>或者<code>cat id_rsa.pub</code>。</p>
<p><img src="/2020/02/17/github%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%8D%97/3.创建ssh%20key.jpg"></p>
<h3 id="检验是否连接成功">检验是否连接成功</h3>
<p>在本地的电脑上添加私钥 ，使用命令：<code>ssh-add id_rsa</code>；之后，在输入命令：<code>ssh -T git@github.com</code>。如果此时响应：Hi encoreMiao! You've successfully authenticated, but GitHub does not provide shell access.便是配置成功了。</p>
<h3 id="在自己的-github中创建新的repo">在自己的 github中创建新的repo</h3>
<p>注意，此时不要选择initialize this repository with a README。</p>
<p><img src="/2020/02/17/github%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%8D%97/4.创建repo.jpg"></p>
<h3 id="clone刚刚创建好的repo到自己想到的路径下">clone刚刚创建好的repo到自己想到的路径下</h3>
<p>使用命令：<code>git clone https://github.com/codewithzichao/CodingInterviewCode.git</code>即可。</p>
<h3 id="切换到刚刚下载好的repo文件夹下">切换到刚刚下载好的repo文件夹下</h3>
<p>执行以下命令👇，从而创建本地仓库以及代码上传。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ touch README.md                                         //新建一个README文档，若上一步勾选了创建README.md，提交时导致冲突</span><br><span class="line">$ git init                                                //初始化本地仓库</span><br><span class="line">$ git add README.md                                       //添加刚刚创建的README文档</span><br><span class="line">$ git commit -m <span class="string">"你的注释...."</span>                             //提交到本地仓库，并写一些注释</span><br><span class="line">$ git remote add origin git@github.com:yourname/xxxx.git  //连接远程仓库并建了一个名叫：origin的别名，当然可以为其他名字，但是origin一看就知道是别名，youname记得替换成你的用户名</span><br><span class="line">$ git push -u origin master                              //将本地仓库的文件提交到别名为origin的地址的master分支下，-u为第一次提交，需要创建master分支，下次就不需要了</span><br></pre></td></tr></table></figure>
<p>初始化完成之后，我们可以把我们项目的源代码提交上去，使用git add命令，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git add 系统签名/                                      // 添加需要提交的文件夹，使用git add .  则添加全部</span><br><span class="line">$ git add assets/</span><br><span class="line">$ git add project.properties </span><br><span class="line">$ git add res/</span><br><span class="line">$ git add src/</span><br><span class="line">$ git commit -m <span class="string">"上传项目源代码"</span>                         // 提交到本地仓库</span><br><span class="line">$ git push origin master                               // 将本地仓库合并到别名为origin地址的master分支下</span><br></pre></td></tr></table></figure>
<p>显示结果如下，则代码上传成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Counting objects: 63, <span class="keyword">done</span>.</span><br><span class="line">Delta compression using up to 8 threads.</span><br><span class="line">Compressing objects: 100% (53/53), <span class="keyword">done</span>.</span><br><span class="line">Writing objects: 100% (63/63), 1.41 MiB | 217.00 KiB/s, <span class="keyword">done</span>.</span><br><span class="line">Total 63 (delta 16), reused 0 (delta 0)</span><br><span class="line">To git@github.com:codewithzichao/CodingInterview.git</span><br><span class="line">   000a667..61357d8  master -&gt; master</span><br></pre></td></tr></table></figure>
<p>刷新一下GitHub，则显示刚刚提交的项目源代码。</p>
<p><img src="/2020/02/17/github%E4%B8%8A%E4%BC%A0%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%8D%97/5,repo.jpg"></p>
<p>到此，成功～</p>
<h3 id="注意事项踩过的坑">注意事项(踩过的坑)</h3>
<p>1.如果要删除本地仓库里的文件，不能直接手动删除，要使用命令：<code>git rm &lt;文件名&gt;</code>或者<code>git rm -r &lt;文件夹名&gt;</code>。要不然的话，github上仍然会有原来的文件。</p>
<p>2.当修改或者增加文件时，使用命令<code>git add &lt;文件夹&gt;/</code>或者<code>git add &lt;文件名&gt;</code>之后，一定要执行<code>git commit -m "title"</code>，要先提交到本地仓库，最后再执行<code>git push origin master</code>命令，才能够更新到github。如果省略掉<code>git commit -m "title"</code>这一步的话，那么修改就不会同步到github中。</p>
<p>over☕️～</p>
]]></content>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title>重新搭建了blog</title>
    <url>/2020/02/17/hello-world/</url>
    <content><![CDATA[<p>最近重新搭建了自己的blog，清除了很多之前的文章，没办法，自己的强迫症又犯了😆，容不得不美观的东西，人生在于折腾，哈哈哈🤣，也算是对之前的blog的整理吧😋。2020，希望一切顺利🤩，运气爆棚🎉～</p>
]]></content>
  </entry>
  <entry>
    <title>tmux的使用</title>
    <url>/2020/06/08/tmux%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>以前一直不知道为何大家推崇tmux，最近使用了一段时间的tmux后，同时配合着iterm2，发觉真香！🤩网上的教程很多，但是很多细节没有说到，导致我最开始接触tmux的时候，费了一两个小时才算入手，所以就打算写写tmux的使用。</p>
<a id="more"></a>
<h2 id="tmux简介">tmux简介</h2>
<p>详细的就不介绍了，它最大的作用就在于：可以让你同时使用多个终端，同时还可以保存下来，必要的时候，然后再次加载回来，非常方便。tmux中有三个非常重要的概念：<strong>会话(session)、窗口(window)、窗格(pane)</strong>，他们的关系是：<strong>一个会话可以有多个窗口，一个窗口可以有多个窗格。</strong>下面讲讲tmux的常用命令。</p>
<blockquote>
<p>注意：以下操作建立在已经安装好tmux以及oh my tmux的基础上！</p>
</blockquote>
<h2 id="tmux常用命令">tmux常用命令</h2>
<ul>
<li>要创建会话，使用命令：<code>tmux new -s 会话名称</code>；</li>
<li>要创建新的窗口，使用命令：<code>ctrl+A</code>，然后再按<code>C</code>键，注意，这两个操作是分开的，不是一起按</li>
<li>要显示该会话下的所有窗口，使用命令：<code>ctrl+A</code>，然后再按<code>W</code>键，注意，这两个操作是分开的，不是一起按</li>
<li>要切换窗口，使用命令：<code>ctrl+A+l</code></li>
<li>要关闭窗口，使用命令：<code>ctrl+A</code>，然后使用<code>shift+&amp;</code>，注意，这两个操作是分开的，不是一起按</li>
<li>分割窗格，使用命令：<code>ctrl+A</code>，然后使用<code>shift+%</code>（纵向分割）或者<code>shift+"</code>（横向分割）</li>
<li>要关闭当前窗格，使用命令：<code>ctrl+A</code>，然后再按<code>x</code>键，注意，这两个操作是分开的，不是一起按</li>
<li>要选择同一窗口的不同窗格，使用命令：<code>ctrl+A</code>，然后再按<code>上下左右</code>键，注意，这两个操作是分开的，不是一起按</li>
<li>要杀死所有会话，使用命令：<code>tmux kill-server</code></li>
<li>要杀死指定的会话，使用命令：<code>tmux kill-session -t 会话名称</code></li>
<li>要进入指定的会话，使用命令：<code>tmux a -t 会话名称</code></li>
<li>要查看目前所有的会话，使用命令：<code>tmux ls</code></li>
<li>保存当前会话，使用命令：<code>ctrl+b</code>，然后再按<code>d</code>键，注意，这两个操作是分开的，不是一起按</li>
</ul>
<p>以上就是tmux的基本操作，基本上可以满足日常的应用场景，最好一直都用tmux，不要一边用着tmux，一边又用着termial。另外，如果要使用的比较爽的话，就要配合着bash命令与vim的使用了，这个大家可以自己摸索，之后如果有机会的话，我也会更新常用的bash命令与vim的使用方法～🤩</p>
]]></content>
      <tags>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划|购买股票问题系列</title>
    <url>/2020/02/17/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BD%9C%E8%B4%AD%E4%B9%B0%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<p><font face="Times New Roman">最近太忙了，没有时间来更新blog，今天刷题刷的有点猛,更新一波～</font></p>
<p><font face="Times New Roman">股票问题是动态规划非常典型的问题，然后有一套阶梯框架，主要就是要找到状态转移方程。anyway，let's do it.</font></p>
<a id="more"></a>
<hr>
<p>以<a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iii/" target="_blank" rel="noopener">123买卖股票的最佳时机</a>为例，进行剖析。</p>
<h4 id="总体思路穷举或者说是状态机">1.总体思路：穷举，或者说是状态机</h4>
<p>在该题中，总共有三种状态：天数、可以进行的交易的数目、持有状态。然后遍历所有状态，从而得到最后的结果。（其实就是穷举的思路）代码如下：</p>
<p><img src="/2020/02/17/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BD%9C%E8%B4%AD%E4%B9%B0%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98%E7%B3%BB%E5%88%97/1.png"></p>
<p>以这道题为例，<span class="math inline">\(dp[i][k][1]\)</span> 表示在第<span class="math inline">\(i\)</span>天，此刻持有股票，至今为止总共进行过<span class="math inline">\(k\)</span>次交易，这个时候的总利润；<span class="math inline">\(dp[i][k][0]\)</span> 表示在第 <span class="math inline">\(i\)</span> 天，此刻没有持有股票，至今为止总共进行过<span class="math inline">\(k\)</span>次交易，这个时候的总利润。</p>
<h4 id="状态转移框架">2.状态转移框架</h4>
<figure>
<img src="/2020/02/17/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BD%9C%E8%B4%AD%E4%B9%B0%E8%82%A1%E7%A5%A8%E9%97%AE%E9%A2%98%E7%B3%BB%E5%88%97/2.png" alt="image-20191018213809486"><figcaption aria-hidden="true">image-20191018213809486</figcaption>
</figure>
<p>上述图片解释了状态转移框架。在这里，需要注意两点：</p>
<p>​ 第一：在计算<span class="math display">\[dp[i][k][1]\]</span>的时候，我们需要用到<span class="math display">\[dp[i-1][k-1][0]\]</span>，在这里，需要将<span class="math inline">\(k\)</span>变成<span class="math inline">\(k-1\)</span>，因为正是因为在<span class="math inline">\(i-1\)</span>天没有进行持有股票，那么经过一次交易后，才会在<span class="math inline">\(i\)</span>天持有股票，那么自然而言的，交易次数要增加1.</p>
<p>​ 第二：在从无到持有股票的过程中，需要减去股票的价值；而反过程中，需要加上股票的价值。</p>
<hr>
<h4 id="base-case的处理">3.base case的处理</h4>
<p>状态转移方程：</p>
<p><span class="math display">\[dp[i][k][0]=max(dp[i-1][k][0],dp[i-1][k][1]+prices[i])\]</span></p>
<p><span class="math display">\[dp[i][k][1]=max(dp[i-1][k][1],dp[i-1][k-1][0]-prices[i])\]</span></p>
<p>在状态转移方程当中，当<span class="math inline">\(i=0\)</span>的时候，需要计算<span class="math inline">\(dp[-1]\)</span>。那么实际上index为-1，其实是有问题的，所以我们需要通过一些方法处理这些case。</p>
<p>实际上，处理这些base case，也是根据状态转移方程来进行分析。当<span class="math inline">\(i=0\)</span>的时候，其状态转移方程为下：</p>
<p><span class="math display">\[dp[0][k][0]=max(dp[-1][k][0],dp[-1][k][1]+prices[0])\]</span></p>
<p><span class="math display">\[dp[0][k][1]=max(dp[-1][k][1],dp[-1][k-1][0]-prices[0])\]</span></p>
<p>那么具体分析上述方程的细节。<span class="math inline">\(dp[-1][k][0]\)</span>表示的是第-1天的时候，不持有股票。那么实际上天数是从第0天开始的，当<span class="math inline">\(i=-1\)</span>的时候，意味着还没有开始。所以：<span class="math inline">\(dp[-1][k][0]=0\)</span>。<span class="math inline">\(dp[-1][k-1][1]\)</span>表示是在第-1天的时候，持有股票。实际上，当还没有开始的时候，是不可能持有股票的。所以：<span class="math inline">\(dp[-1][k-1][1]=-infinity\)</span></p>
<p>。其他的依次类推。</p>
<hr>
<h4 id="程序代码">4.程序代码：</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INF 0x3f3f3f3f</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> NINF -INF-1</span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="comment">//还是套用公式，这是k=2的情况</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>&#123;</span></span><br><span class="line">	<span class="keyword">public</span>:</span><br><span class="line">	<span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; prices)</span> </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> n=prices.<span class="built_in">size</span>();</span><br><span class="line">		<span class="keyword">if</span>(n==<span class="number">0</span>)&#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">int</span> k_max=<span class="number">2</span>;</span><br><span class="line">		<span class="keyword">int</span> dp[n+<span class="number">1</span>][k_max+<span class="number">1</span>][<span class="number">2</span>];</span><br><span class="line">		<span class="built_in">memset</span>(dp,<span class="number">0</span>,<span class="keyword">sizeof</span>(dp));</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">			<span class="keyword">for</span>(<span class="keyword">int</span> k=k_max;k&gt;=<span class="number">1</span>;k--)&#123;</span><br><span class="line">				<span class="keyword">if</span>(i<span class="number">-1</span>==<span class="number">-1</span>)&#123;</span><br><span class="line">					dp[i][k][<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">					dp[i][k][<span class="number">1</span>]=-prices[i];</span><br><span class="line">					<span class="keyword">continue</span>;</span><br><span class="line">				&#125;</span><br><span class="line">				dp[i][k][<span class="number">0</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][k][<span class="number">0</span>],dp[i<span class="number">-1</span>][k][<span class="number">1</span>]+prices[i]);</span><br><span class="line">				dp[i][k][<span class="number">1</span>]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][k][<span class="number">1</span>],dp[i<span class="number">-1</span>][k<span class="number">-1</span>][<span class="number">0</span>]-prices[i]);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> dp[n<span class="number">-1</span>][k_max][<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> n;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n);</span><br><span class="line">	<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; prices;</span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">		<span class="keyword">int</span> x;</span><br><span class="line">		<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;x);</span><br><span class="line">		prices.push_back(x);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	Solution so1;</span><br><span class="line">	<span class="keyword">int</span> ans = so1.maxProfit(prices);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"%d\n"</span>,ans);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其他的关于股票的问题还有：<a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/" target="_blank" rel="noopener">121. 买卖股票的最佳时机</a>、<a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii/" target="_blank" rel="noopener">122. 买卖股票的最佳时机 II</a>、<a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-iv/" target="_blank" rel="noopener">188. 买卖股票的最佳时机 IV</a>、<a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/" target="_blank" rel="noopener">309. 最佳买卖股票时机含冷冻期</a>、<a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/" target="_blank" rel="noopener">714. 买卖股票的最佳时机含手续费</a>。套用框架均可以解决。</p>
<p>OK，终于写完了～🤩☕️</p>
]]></content>
      <tags>
        <tag>C++</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习|FM/FFM模型详解</title>
    <url>/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>FM(Factorization Machines)模型与FFM(Field-aware Factorization Machines)模型，是在推荐系统中常用的两个模型。其实我本不想去写这两个模型的，毕竟我并不是搞推荐系统的，而且NLP方面我还没看的论文太多了😭，但是最近突然发现在看有关LR模型的题的时候，讲到了这两个模型，所以记录一下。</p>
<a id="more"></a>
<h2 id="fm模型介绍">FM模型介绍</h2>
<p>FM模型要解决的是：<strong>稀疏数据的特征组合问题。</strong>在训练样本中，我们往往会碰到样本的特征中会有很多的categorical feature。譬如：国家，性别，名字等等。这些数据往往不能直接使用，我们需要将其转换为数值型数据才能使用。处理categorical feature的一般方法是：<strong>one-hot编码</strong>(当然还有其他的方式，比如说在catBoost中介绍了好几种方法)。具体来说如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/0.png"></p>
<p>上图中，<code>Clicked?</code>是label，<code>Country、Day、Ad_type</code>这三个都是feature，并且这三个feature都是categorical feature。我们需要对其进行one-hot编码，结果如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/1.png"></p>
<p>转换后，特征就能使用了。但是，如果我们仔细观察一下，就会发现：使用one-hot编码后，特征数量增多了，并且大部分的特征值都是0，这就是数据稀疏问题。数据稀疏是不好的，因为这会导致模型难以训练，梯度下降的特别慢。</p>
<p>此外，对于线性模型或者在LR模型中，我们其实都是假设特征之间是相互独立的，但这是不对的。因为在具体的情况中，特征之间应该是有关系。譬如:<code>&lt;厨房，米饭&gt;</code>这样两个特征之间应该会有一定的相关性。所以我们对特征之间进行组合，对于模型的表达能力来说，是有好处的。</p>
<p><strong>那么，如何对特征进行组合？</strong></p>
<p>多项式模型是包含特征组合最为直观的模型。我们是用<span class="math inline">\(x_ix_j\)</span>来表示特征<span class="math inline">\(x_i\)</span>与特征<span class="math inline">\(x_j\)</span>的组合，只有当<span class="math inline">\(x_ix_j\)</span>不等于0的时候，组合特征才有意义。在FM模型，一般是使用二阶的多项式模型，如下： <span class="math display">\[
y(X)=w_0+\sum_{i=1}^{n}w_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}w_{ij}x_ix_j
\]</span> 其中，<span class="math inline">\(n\)</span>表示特征的数量。我们可以看到，公式的前半部分就是LR模型的线性组合，后半部分就是组合特征，也叫做交叉项。所以，<strong>FM模型的表达能力比LR模型是要强的，并且当<span class="math inline">\(w_{ij}\)</span>为0的时候，FM模型就退化成LR模型了。(当然，如果要用于二分类的话，需要在多项式模型之上用上sigmoid函数)。</strong>我们需要训练的参数是：<span class="math inline">\(w_0，w_i，w_{ij}\)</span>。</p>
<p>但是如果直接训练的话，参数<span class="math inline">\(w_{ij}\)</span>是很难训练的。因为其参数规模为：<span class="math inline">\(O(n^2)\)</span>，当<span class="math inline">\(n\)</span>很大的时候，那么计算复杂度非常的大。而且，训练<span class="math inline">\(w_{ij}\)</span>需要大量的满足<span class="math inline">\(x_ix_j\)</span>非零的样本，而现实生活中样本往往都是稀疏的，所以满足要求的样本就很少，所以训练<span class="math inline">\(w_{ij}\)</span>是很难的。</p>
<p><strong>那么，如何训练参数<span class="math inline">\(w_{ij}\)</span>呢？</strong></p>
<p>在FM模型中，它的解决办法是：<strong>为每一个特征引入一个隐向量<span class="math inline">\(v_i=(v_{i1},v_{i2},v_{i3},...,v_{ik})^T\)</span>。 </strong>其中，<span class="math inline">\(k\)</span>是隐向量的维度，也是超参数，需要我们去调整。</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/2.jpg" style="zoom: 67%;"></p>
<p>所以，我们化简之后发现，我们对于参数<span class="math inline">\(w_{ij}\)</span>的训练，只需要知道样本那些不为0的特征就可以了，计算复杂度为：<span class="math inline">\(O(kn)\)</span>。所以，这样一来，我们就可以在线性时间复杂度完成FM模型。所以总结一下：FM模型的优势有两点：<strong>1.能够用来解决数据稀疏的问题；2.具有线性的计算复杂度。</strong></p>
<h2 id="ffm模型介绍">FFM模型介绍</h2>
<p>FFM模型是FM模型的一个特例，它引入了filed这个概念。什么意思呢？如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/3.png"></p>
<p>上面有4个特征，其中<span class="math inline">\(User、Movie、Genre\)</span>这三个特征是categorical feature。我们将其使用one-hot编码之后，可以得到5个特征，4个 field。如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/4.png"></p>
<p>所以使用FFM模型之后，其组合特征就有10个。下面具体给出模型： <span class="math display">\[
y=w_0+\sum_{i=1}^{n}w_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}&lt;v_{i,f_j},v_{j,f_i}&gt;x_ix_j
\]</span> 其中，<span class="math inline">\(f_j\)</span>表示第j个特征属于的filed。这个模型的二次项参数是<span class="math inline">\(nkf\)</span>。预测复杂度为<span class="math inline">\(O(nk^2)\)</span>。</p>
<p>在FFM模型中，将问题定义为分类问题的时候，其损失函数是logsitic loss，但是与平常的logistic loss有所不同，如下： <span class="math display">\[
min \ OBJ=\mathop{min}\limits_{w,b}\sum_{i=1}^{N}log(exp(-y_i(w^Tx_i+b))+\frac {\lambda}{2}w^Tw
\]</span> 这样看是不是很奇怪？和咱们平时看到的LR模型的loss不太一样？其实这里我们都忽略了一个条件：平时我们孙推导的LR loss的标签是0与1，而这里的loss是-1与1。其实和平时我们推导的方式是一样的。推导过程如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-FM-FFM%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/5.jpg"></p>
<p>之后，我们就可以使用SGD来进行优化，这就不介绍了，相信大家都很熟悉了。</p>
<h2 id="fm模型lr模型svm模型的联系">FM模型、LR模型、SVM模型的联系</h2>
<ul>
<li>FM模型是LR模型的扩展，在LR模型的基础上，增加了交叉项，这就让FM模型的表达能力至少要超过LR模型，也即是它能处理更加复杂的问题；同时它来能以与LR模型同样的时间复杂度来训练模型，这使的FM模型要比LR模型要更加优越。</li>
<li>FM模型适合处理数据稀疏问题，这比使用多项式核的SVM要更加好，因为FM模型可以在原始形式下进行训练，而SVM模型需要转换成对偶形式才能训练；此外，FM模型的预测是与训练样本独立的，而SVM则与训练样本有关(支持向量)。</li>
<li>LR模型与SVM模型的很重要的区别在于：其使用的损失函数。LR模型使用的是交叉熵 cross entropy，而SVM模型使用的合页损失hinge loss。</li>
</ul>
<h2 id="参考文献">参考文献</h2>
<p>1 https://zhuanlan.zhihu.com/p/37963267</p>
<p>2 https://blog.csdn.net/hiwallace/article/details/81333604</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>FM模型</tag>
        <tag>FFM模型</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习|LightGBM与catBoost模型原理详解</title>
    <url>/2020/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-LightGBM%E4%B8%8EcatBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>XGBoost、LightGBM、catBoost是GBDT模型的三大工程化的实现。前面已经对XGBoost模型进行了讲解，这篇博客将对LightGBM与catBoost模型进行讲解。由于三大模型有很多需要相似的地方，大部分基础部分在XGBoost那片已经讲解过了，所以这篇博客将着重讲解模型自身创新的地方。</p>
<a id="more"></a>
<h2 id="lightgbm模型介绍">LightGBM模型介绍</h2>
<p>在GBDT模型中，核心在于基本分类器(回归树)的生成，XGBoost模型也不例外。在XGBoost中，在找最佳分裂点的时候，所使用的方法是：<strong>针对每一个特征，去遍历所有训练实例，计算信息增益，从而得到最佳分裂点。</strong>但是，当实例的特征维度非常大，数据量也非常大的时候，计算效率会大幅降低，同时也非常地占用内存。而<strong>LightGBM模型正是从减小数据量以及特征维度这两方面，对XGBoost模型进行性能优化的GBDT模型。</strong>为了减小数据量，LightGBM中所使用的技术叫做：<strong>Gradient based One side sampling(GOSS)</strong>；为了减小特征数量，LgithGBM中所使用的技术叫做：<strong>Exclusive Feature Bundling(EFB)</strong>。下面，我们将从这两方面，来介绍LightGBM模型。</p>
<h3 id="gradient-based-one-side-samplinggoss">Gradient based One side sampling(GOSS)</h3>
<p>GOSS，中文叫做：基于梯度的单边采样。它的目标是：<strong>减小数据量</strong>。那么怎么做呢？作者发现：<strong>训练集中具有不同梯度的实例对于计算信息增益的作用是不一样的。那么具有较小梯度的实例，说明其已经被训练的差不多了，其对于信息增益的大小影响较小；而那些具有较大梯度的实例对信息增益的影响较大。</strong>所以一个很自然地想法是：抛弃那些梯度较小的实例，只保存梯度较大的实例，这样的话，就达到了减小数据量的目标，同时还不够降低准确率。但是这样做会带来一个问题：<strong>抛弃实例会导致训练集的数据分布发生改变</strong>。为了解决这个问题，GOSS算法被提出来了。</p>
<p>GOSS的核心思想是：<strong>保存所有的梯度较大的实例，同时对梯度较小的实例进行一定比例的随机抽样。</strong>具体做法是：</p>
<ul>
<li>首先，对训练实例按照梯度从大到小进行排序；</li>
<li>接着，选取并保存前<span class="math inline">\(a\%\)</span>的梯度大的实例。即：<span class="math inline">\(a\% *\# samples\)</span></li>
<li>接着，在选取<span class="math inline">\(b\%\)</span>的梯度小的实例。即：<span class="math inline">\(b\% *\# samples\)</span></li>
<li>合并选取的梯度大的与小的实例，作为新的训练集；</li>
<li>每一个梯度小实例乘以一个系数：<span class="math inline">\(\frac {1-a}{b}\)</span>，这样做的作用在于：尽量保证数据分布不发生变化。</li>
<li>使用上述的样本训练一个弱分类器。</li>
</ul>
<p><strong>当<span class="math inline">\(a\)</span>为0的时候，就是随机抽样算法；当<span class="math inline">\(a\)</span>为1的时候，就没有抽样，使用全部训练实例。</strong></p>
<p><img src="/2020/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-LightGBM%E4%B8%8EcatBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/0.jpg" style="zoom:67%;"></p>
<h3 id="exclusive-feature-bundlingefb">Exclusive Feature Bundling(EFB)</h3>
<p>EFB，中文叫做：独立特征合并。它的目标是：<strong>减小特征数量</strong>。<strong>这个算法的想法来源是：</strong>在实例的特征中，大部分的特征都是稀疏的，即特征向量大部分都是0。那么，如果我们能够将特征进行合并·，就可以减小使用的特征的数量，还能得到稠密的特征向量，易于训练。<strong>EFB算法的想法是：</strong>我们将两个完全互斥的特征进行合并，这个样才不会丢失信息；如果说两个特征之间并不是完全互斥的，我们可以使用冲突比率来对特征之间的不互斥的程度进行衡量，如果说这个值比较小，那么，我们还是可以将这两个特征进行合并，而不会影响到最后的性能。<strong>这样讲还是比较抽象，举个例子：</strong>特征a的数据为<span class="math inline">\([1, 0, 0, 0, 2, 0]\)</span>，特征b的数据为<span class="math inline">\([0, 0, 3, 0, 0, 2]\)</span>，可见他们的非零部分的位置都是没有冲突的，那么我们将其合并在一起<span class="math inline">\([1,0,3,0,2,4]\)</span>。这里b特征的数值2改成4，是因为a特征已经存在数值2．那么我们就将它改成其他数值避免数值上的冲突。那么问题来了：</p>
<ul>
<li><strong>第一个问题：我们如何确定哪些特征用来合并效果会比较好？</strong></li>
<li><strong>第二个问题：我们如何将确定好的特征进行合并？</strong></li>
</ul>
<h4 id="第一个问题">第一个问题</h4>
<p>对于第一个问题，这是一个NP-hard问题。我们可以将特征看作点，特征之间的冲突值看作边的权重，(这里的冲突值说的是特征的之间的cos值。)<strong>我们要做的是合并特征，并使得合并特征的数目最小，</strong>这其实是一个图着色问题。在EFB中，使用近似贪心算法来解决。<strong>先按照度来对每个特征做降序排序（度数越大与其他点的冲突越大），然后将特征合并到冲突数小于K的bundle或者新建另外一个bundle。算法的时间复杂度为O(#feature^2)。</strong></p>
<p><img src="/2020/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-LightGBM%E4%B8%8EcatBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/1.jpg" style="zoom:67%;"></p>
<h4 id="第二个问题">第二个问题</h4>
<p>第二个是怎么合并这些bundles。由于每一个bundle当中，features的range都是不一样，所以我们需要重新构建合并后bundle feature的range。在第一个for循环当中，我们记录每个feature与之前features累积totalRange。在第二个for循环当中，根据之前的binRanges重新计算出新的bin value：<span class="math inline">\(（F[j]bin[i] + binRanges[j]）\)</span>，保证feature之间的值不会冲突。这是针对于稀疏矩阵进行优化。由于之前Greedy Bundling算法对features进行冲突检查确保bundle内特征冲突尽可能少，所以特征之间的非零元素不会有太多的冲突。比如，假设我们有两个特征，特征A的取值范围是[0,10)，而特征B的取值范围是[0,20)，我们可以给特征B增加偏移量10，使得特征B的取值范围为[10, 30)，最后合并特征A和B，形成新的特征，取值范围为[0,30)来取代特征A和特征B。如此一来，数据的shape就变成了<span class="math inline">\(\#samples * \#bundles\)</span>，且<span class="math inline">\(\#bundles &lt;&lt; \#features\)</span>。EFB降低了数据特征规模提高了模型的训练速度。</p>
<p><img src="/2020/03/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-LightGBM%E4%B8%8EcatBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/2.jpg" style="zoom:67%;"></p>
<h3 id="树的生长方式">树的生长方式</h3>
<p>树的生长方式有两种：leaf-wise与level-wise。level-wise方式是：一次分裂是分裂同一层的所有的叶子结点，这样做的好处在于可以防止过拟合，但是缺点在于会产生不必要的开销，因为其实同一层的叶子结点，没必要去计算它们所有的信息增益。left-wise的方式是：从一层的叶子结点中找到信息增益最大的叶子结点，进行分裂，这样做的好处在于不需要对一层的叶子结点都进行分裂，就减小了计算量；坏处在于容易出现过拟合。在LightGBM模型中，采用了leaf-wise方式，并使用<span class="math inline">\(max \ depth\)</span>来限制树的深度，从而来避免过拟合。</p>
<h2 id="catboost模型介绍">catBoost模型介绍</h2>
<p>catBoost模型主要是为了处理类别特征问题而设计的。对于类别特征，一般的处理方式是：当它的取值个数比较小的时候，那么可以将其转换为one-hot编码；但是当其个数非常多的时候，那么这个时候就不能使用one-hot编码来。因为这样的话，会导致决策树无法学习，即分到这个叶子结点的样本数目太少。所以，catBoost就提出了几种关于处理类别特征的算法，具体的以后有时间再写吧🥱。参看链接：<a href="https://zhuanlan.zhihu.com/p/102540344" target="_blank" rel="noopener">catBoost</a></p>
<h2 id="参考文献">参考文献</h2>
<p>1 《LightGBM: A Highly Efﬁcient Gradient Boosting Decision Tree》</p>
<p>2 《CatBoost: gradient boosting with categorical features support》</p>
<p>3 https://zhuanlan.zhihu.com/p/38516467</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>LightGBM</tag>
        <tag>catBoost</tag>
        <tag>集成学习</tag>
        <tag>XGBoost</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习|softmax模型详解与实现</title>
    <url>/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-softmax%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>softmax是非常简单的多分类模型，常见于神经网络中的输出层。那为什么这么简单的模型还要花时间写篇文章来讲呢？<strong>原因在于：LR模型大家都很熟悉了，而softmax模型大家都不是很关注，因为可能觉得softmax是LR的推广，所以就没有详细去推导过softmax模型以及它的反向传播。</strong>所以，这篇博客，将着重讲解一下关于softmax模型导数的推导与反向传播的推导，并采用python进行实现。</p>
<a id="more"></a>
<blockquote>
<p>这篇博客侧重于softmax函数导数的推导，以及softmax的loss function的导数推导。</p>
</blockquote>
<h2 id="softmax函数介绍">softmax函数介绍</h2>
<p>softmax函数常见于神经网络的输出层，用来做归一化分布。它的形式如下： <span class="math display">\[
\hat y_i=\frac {e^{a_i}}{\sum_{k=1}^{T}e^{a_k}}
\]</span> 其中，<span class="math inline">\(T\)</span>表示输出的总类别数目。下面，我们记那个推导一下关于softmax函数的导数形式：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-softmax%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg" style="zoom:67%;"></p>
<h2 id="softmax的loss-function的求导">softmax的loss function的求导</h2>
<p>在softmax中，损失函数通常使用交叉熵。其对于参数<span class="math inline">\(w,b\)</span>的求导如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-softmax%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom:67%;"></p>
<p>讲解完毕🎉</p>
<h2 id="softmax的实现">softmax的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了python与scikit-learn库来实现softmax模型。我的github里面可以下载到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2020-1-2</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">准确率：0.1532.</span></span><br><span class="line"><span class="string">时间：16.96704387664795.</span></span><br><span class="line"><span class="string">--------------</span></span><br><span class="line"><span class="string">tips:实现的非常简单，直接将输入数据输入到一个线性层，然后输出10个0-1之间的数，中间没有用任何的隐藏层，</span></span><br><span class="line"><span class="string">由于是线性变换，所以效果比较差吧，如果加几个隐藏层，效果会好得多！</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName:数据路径名</span></span><br><span class="line"><span class="string">    :return: 特征向量矩阵、还有标签矩阵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = [];</span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            label_list.append(int(curline[<span class="number">0</span>]))</span><br><span class="line">            data_list.append([int(feature) / <span class="number">255</span> <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Softmax</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_data, train_label, iter, learning_rate)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        :param train_data: 训练数据</span></span><br><span class="line"><span class="string">        :param train_label: 训练数据的标签类别</span></span><br><span class="line"><span class="string">        :param iter: epoch的数目</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.train_data = train_data</span><br><span class="line">        self.train_label = train_label</span><br><span class="line">        self.iter = iter</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.feature_num = self.train_data.shape[<span class="number">1</span>]</span><br><span class="line">        self.input_num = self.train_data.shape[<span class="number">0</span>]</span><br><span class="line">        self.w, self.b = self.initialize_params(self.feature_num)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        softmax函数</span></span><br><span class="line"><span class="string">        :param X: 输入数据</span></span><br><span class="line"><span class="string">        :return: 返回含有10个元素的np.array</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> np.exp(X) / np.sum(np.exp(X))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_params</span><span class="params">(self, feature_dim)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        初始化参数w,b</span></span><br><span class="line"><span class="string">        :param feature_dim:实例特征数目</span></span><br><span class="line"><span class="string">        :return: 参数w,b</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        w = np.random.uniform(<span class="number">0</span>, <span class="number">1</span>, (feature_dim, <span class="number">10</span>))</span><br><span class="line">        b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        训练</span></span><br><span class="line"><span class="string">        :return:返回参数w,b</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">for</span> iter <span class="keyword">in</span> range(self.iter):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_num):</span><br><span class="line">                x = train_data[i].reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">                y = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">                y[train_label[<span class="number">0</span>][i]] = <span class="number">1</span></span><br><span class="line">                y_ = self.softmax(np.dot(self.w.T, x) + self.b)</span><br><span class="line">                self.w -= self.learning_rate * (np.dot((y_ - y), x.T))</span><br><span class="line">                self.b -= self.learning_rate * (y_ - y)</span><br><span class="line">        <span class="keyword">return</span> self.w, self.b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, digit)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        预测单个样本的值</span></span><br><span class="line"><span class="string">        :param digit: 严格样本的特征向量</span></span><br><span class="line"><span class="string">        :return: 返回预测的样本类别</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回softmax中概率最大的值</span></span><br><span class="line">        <span class="keyword">return</span> np.argmax(np.dot(self.w.T, digit) + self.b)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, test_data, test_label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        测试</span></span><br><span class="line"><span class="string">        :param test_data: 测试集的特征向量</span></span><br><span class="line"><span class="string">        :param test_label: 测试集的标签类别</span></span><br><span class="line"><span class="string">        :return: 准确率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        error = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(test_data.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> (self.predict(test_data[i]) != test_label[<span class="number">0</span>][i]):</span><br><span class="line">                error += <span class="number">1</span></span><br><span class="line">                print(<span class="string">f"the prediction is <span class="subst">&#123;self.predict(test_data[i])&#125;</span>,the true is <span class="subst">&#123;test_label[<span class="number">0</span>][i]&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">        accuracy = (test_data.shape[<span class="number">0</span>] - error) / test_data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start load data."</span>)</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    a = Softmax(train_data, train_label, <span class="number">50</span>, <span class="number">0.005</span>)</span><br><span class="line">    accuracy = a.test(test_data, test_label)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>Logistic Regression</tag>
        <tag>softmax</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|AdaBoost模型原理详解与实现</title>
    <url>/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-AdaBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>AdaBoost模型是提升方法的代表，对于之后理解XGBoost等集成学习模型很有帮助。<strong>但是，在学习AdaBoost模型之前，请务必要先搞懂回归树(CART)，因为当AdaBoost的基本分类器是回归树时，需要用到回归树。</strong>本篇博客将详细讲解AdaBoost模型的原理，并会讲解梯度提升树模型(GBDT)，并采用python与scikit-learn对其进行实现。</p>
<a id="more"></a>
<h2 id="adaboost模型介绍">AdaBoost模型介绍</h2>
<p>AdaBoost模型是典型的提升方法。那么何谓提升(boosting)方法呢？<strong>当我们很难一次性找到一个效果很好的模型的时候，我们可以先找到一些效果不那么好的模型，我们称为弱学习算法，然后将这些弱学习算法通过某种组合，将其提升(boost)为强学习算法，这就是提升方法的思想。</strong>那么，AdaBoost模型就是典型的应用于分类问题的提升算法。AdaBoost模型的做法是：从弱学习算法中学习一系列弱分类器，然后通过某种组合，从而得到最终的强分类器。那么，问题来了：<strong>AdaBoost模型是怎么得到一系列弱分类器的呢？</strong>在提升方法中，一般都是通过调整训练数据集的概率分布，通过不同的训练集分布来得到一系列的弱分类器。那么，问题又来了：<strong>AdaBoost模型是怎么改变训练集的概率分布的呢？AdaBoost模型又是怎么将一系列弱分类器组合成强分类器的呢？</strong></p>
<ul>
<li>对于第一个问题：<strong>AdaBoost模型提高在上一轮被错误分类的实例的权重，降低被正确分类的实例的权重，这样一来，在下一轮的学习中，被错误分类的实例将得到更大的关注，从而有更大概率被正确分类。</strong></li>
<li>对于第二个问题：<strong>AdaBoost模型采取加权表决的做法。给予那些分类准确率高的弱分类器更大的权重，而对于分类准确率低的弱分类器更小的权重，这也是很合理的。</strong></li>
</ul>
<p>那么介绍完AdaBoost模型的基本思想之后，下面我将给出AdaBoost模型的具体公式细节：</p>
<p>输入：训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2,...,x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i\in R^n\)</span>，<span class="math inline">\(y_i\in \{-1,+1\}\)</span>；弱学习算法；</p>
<p>输出：分类器<span class="math inline">\(G(x)\)</span>。</p>
<ol type="1">
<li><p>初始化训练数据集的权值分布<span class="math inline">\(D_1\)</span>，实际上是均匀分布。即：<span class="math inline">\(D_1=\{w_{11},w_{12},w_{13},...,w_{1N}\}\)</span>，其中<span class="math inline">\(w_{1i}=\frac 1N\)</span>。</p></li>
<li><p>对于<span class="math inline">\(m=1,2,3,...,M\)</span>，执行以下操作：</p></li>
</ol>
<p>​ (a) 根据带有权值分布的训练数据集，学习弱分类器： <span class="math display">\[
G_m(x) : \cal{X}-&gt;\{-1,+1\}
\]</span> ​ (b) 计算训练数据集的分类误差率： <span class="math display">\[
e_m=\sum_{i=1}^{N}w_{mi}I(G_m(x_i)\not=y_i)
\]</span> ​ (c) 计算该分类器的权重： <span class="math display">\[
\alpha_m=\frac 12log\frac {(1-e_m)}{e_m}
\]</span> ​ (d) 更新训练数据集的权值分布： <span class="math display">\[
D_{m+1}=(w_{m+1,1},w_{m+1,2},...,w_{m=1,i},...,w_{m+1,N}),\\
w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),\\
Z_m=\sum_{i=1}^{N}w_{mi}exp(-\alpha_my_iG_m(x_i)),\\
或者，\ \  w_{m+1,i}=\begin{cases}
                                        \frac{w_{mi}}{Z_m}exp(\alpha_m)&amp; G_m(x_i)\not=y_i\\
                    \frac{w_{mi}}{Z_m}exp(-\alpha_m)&amp; G_m(x_i)=y_i
                                        \end{cases}
\]</span> (3) 构建基本分类器的线性组合： <span class="math display">\[
G(x)=\sum_{m=1}^{M}\alpha_mG_m(x)
\]</span> (4) 得到最终的分类器： <span class="math display">\[
f(x)=sign(G(x))
\]</span></p>
<h2 id="adaboost模型的另一种表述形式">AdaBoost模型的另一种表述形式</h2>
<p>AdaBoost模型还有另外一种解释：AdaBoost模型是<strong>模型为加法模型、损失函数为指数函数、学习算法是前向分步算法的二类分类模型。</strong></p>
<p><strong>加法模型</strong>。加法模型的形式如下： <span class="math display">\[
f(x)=\sum_{m=1}^{M}\beta_mb(x,\gamma_m)
\]</span> 其中，<span class="math inline">\(\beta\)</span>是系数，<span class="math inline">\(b(x,\gamma_m)\)</span>是基本分类器，<span class="math inline">\(\gamma\)</span>是其参数。所以我们要学习的参数就是<span class="math inline">\(\beta_m,\gamma_m\)</span>。</p>
<p><strong>损失函数</strong>。当给定损失函数<span class="math inline">\(L(y,f(x))=exp(-yf(x))\)</span>之后，整个求解问题变成如下： <span class="math display">\[
\mathop{min}\limits_{\beta_m,\gamma_m}\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i,\gamma_m))
\]</span> 但是，一次性要求解M个基分类器的参数与系数，是非常困难的。所以我们需要采用前向分步算法来进行求解。</p>
<p><strong>前向分步算法</strong>。所谓的前向分步算法，就是我们一次只求解一个基分类器的参数与系数，从而能够大大简化运算。如下： <span class="math display">\[
\mathop{min}\limits_{\beta,\gamma}\sum_{i=1}^{N}L(y_i,\beta b(x_i,\gamma))
\]</span></p>
<p>所以，整个AdaBoost模型如下：</p>
<p>输入：给定训练集与损失函数以及基本分类器集合。</p>
<p>输出：最终的模型<span class="math inline">\(f(x)\)</span>。</p>
<ol type="1">
<li><p>初始化<span class="math inline">\(f_0(x)=0\)</span>。</p></li>
<li><p>对<span class="math inline">\(m=1,2,...,M\)</span>，有</p></li>
</ol>
<p>​ (a) 极小化损失函数 <span class="math display">\[
(\beta_m,\gamma_m)=\mathop{argmin}\limits_{\beta,\gamma}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i,\gamma))
\]</span> 得到参数<span class="math inline">\(\beta_m, \gamma_m\)</span>。</p>
<p>​ (b) 更新模型 <span class="math display">\[
f_m(x)=f_{m-1}(x)+\beta_m b(x,\gamma_m)
\]</span> (3) 得到最终的模型 <span class="math display">\[
f(x)=\sum_{m=1}^{M}\beta_m b(x,\gamma_m)
\]</span></p>
<h2 id="gbdt">GBDT</h2>
<p>提升树模型是以分类树或者回归树为基本分类器的提升方法。它也是使用加法模型以及前向分步算法来进行求解模型的。当对于二类分类问题的时候，其等价于AdaBoost模型，这里就不说了。这里，着重讲一下关于应用于回归问题的提升树模型。</p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-AdaBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg" style="zoom:50%;"></p>
<p>在回归问题中，损失函数常使用均方误差MSE。如下： <span class="math display">\[
L(y,f(x))=(y-f(x))^2
\]</span> 再化简一下： <span class="math display">\[
L(y,f(x))=L(y,f_{m-1}(x)+T(x,\theta_m))^2=(y-f_{m-1}(x)=T(x,\theta))^2
\]</span> 我们记<span class="math inline">\(y-f_{m-1}(x)\)</span>为残差<span class="math inline">\(r\)</span>，也就是说，我们要使损失函数最小，只要让当前模型去拟合数据的残差即可。</p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-AdaBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom:50%;"></p>
<p>最后，我们可以得到回归树：<span class="math inline">\(f_M(x)=\sum_{m=1}^{M}T(x,\theta_m)\)</span>。计算的例子可以参考这里：<a href="https://zhuanlan.zhihu.com/p/36108972" target="_blank" rel="noopener">GBDT的计算示例</a></p>
<p>对于损失函数为指数函数或者是MSE，其实学习回归树是很容易的。但是当损失函数是一般的损失函数的时候，计算往往就会变得很复杂。在这里，我们给出一个大致的框架，而之后要讲的XGBoost/Light GBM/catBoost等树模型就是对GBDT的很好地工程化的实现。</p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-AdaBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-AdaBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg" style="zoom:50%;"></p>
<p>理论部分就讲到这里🎉</p>
<h2 id="adaboost模型的实现">AdaBoost模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现AdaBoost模型：python实现以及调用scikit-learn库来实现。我的github里面可以下载到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<h3 id="scikit-learn实现">scikit-learn实现</h3>
<h2 id="参考资料">参考资料</h2>
<p>1 《统计学习方法》</p>
<p>2 https://zhuanlan.zhihu.com/p/105497113</p>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>决策树</tag>
        <tag>scikit-learn</tag>
        <tag>CART</tag>
        <tag>回归树</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|EM算法原理详解与实现</title>
    <url>/2020/02/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>EM算法，中文名叫期望最大算法，主要用来求解含有隐变量的概率模型的参数。在高斯混合模型(GMM)、隐马尔可夫模型(HMM)中都有应用(在HMM模型中叫做Baum-Welch算法)。本篇博客将对EM算法的原理以及高斯混合模型(GMM)进行详细地讲解，并对其采用python与scikit-learn库这两种方式进行实现。</p>
<a id="more"></a>
<h2 id="为什么要有em算法">为什么要有EM算法？</h2>
<p>首先我们要了解<strong>为什么要有EM算法？</strong>对于普通的估计概率模型的参数，我们可以直接通过求他们的极大似然估计<span class="math inline">\(P(y|\theta)\)</span>。譬如说，掷一枚硬币正面朝上的概率为<span class="math inline">\(q\)</span>，连续掷5次，结果为：正面、正面、反面、正面、反面，求<span class="math inline">\(q\)</span>是多少？那么很显然，其中<span class="math inline">\(q\)</span>就是我们需要估计的参数，我们可以根据极大似然估计：<span class="math inline">\(q=argmax_q \{q^3(1-q)^2\}\)</span>。我们只要求其求导，就可以得到，<span class="math inline">\(q=\frac 35\)</span>。这相信，对熟悉MLE的同学来说，是非常容易的。但是，<strong>如果说加入隐变量呢？</strong>具体如下：</p>
<p>假设有三枚硬币A、B和C，A正面朝上的概率是<span class="math inline">\(q_1\)</span>，B正面朝上的概率是<span class="math inline">\(q_2\)</span>，C正面朝上的概率是<span class="math inline">\(q_3\)</span>。当投掷A硬币正面朝上的时候，我们再投掷B，记录B的结果；当投掷A硬币反面朝上的时候，我们再投掷C，记录C的结果。假设只能观察到结果，中间是选择了B或者C，都不知道，重复五次实验，结果为：正面、正面、反面、正面、反面。求<span class="math inline">\(q_1,q_2,q_3\)</span>？（例子来源于《统计学习方法》）</p>
<p>那么，非常显然，我们不能像估计普通的概率模型的参数的方式来估计三个硬币正面朝上的概率。因为，中间选择B或者C的结果我们并不知道，也就是根据观察到的结果，我们不知道每次A的结果是正面还是反面。这就说明问题中含有了隐变量。这个时候，<strong>就需要新的算法来估计像这种含有隐变量的概率模型的参数</strong>。所以，EM算法应运而生。</p>
<h2 id="em算法的推导">EM算法的推导</h2>
<p>首先，我直接给出整个EM算法，然后再给出推导过程。</p>
<p><img src="/2020/02/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom:50%;"></p>
<p>此外，收敛条件如下： <span class="math display">\[
||\theta^{(i+1)}-\theta^{(i)}||&lt;\varepsilon_1, ||Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})||&lt;\varepsilon_2
\]</span> <strong>下面给出推导过程（推导过程可能与《统计学习方法》不一样，其实道理是一样的）：</strong></p>
<p><img src="/2020/02/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.jpg"></p>
<p>OK，那么，我们现在得到了如下式子： <span class="math display">\[
L(\theta)&gt;=ELBO,\\
ELBO=\sum_{z}P(Z|Y,\theta)log\frac {P(Y,Z|\theta)}{P(Z|Y,\theta)}
\]</span> 那么，要让<span class="math inline">\(L(\theta)\)</span>最大化，也就是让下界ELBO最大化即可。也就是说: <span class="math display">\[
\theta^{(i+1)}=\mathop{argmax}\limits_{\theta}\sum_{z}P(Z|Y,\theta^{(i)})log\frac {P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})}
\]</span> 由于<span class="math inline">\({P(Z|Y,\theta^{(i)})}\)</span>是一个常数，不影响<span class="math inline">\(\theta^{(i+1)}\)</span>的更新，所以： <span class="math display">\[
\theta^{(i+1)}=\mathop{argmax}\limits_{\theta}\sum_{z}P(Z|Y,\theta^{(i)})log{P(Y,Z|\theta)}
\]</span> 我们对其整理一下： <span class="math display">\[
Q(\theta,\theta^{(i)})=\sum_{z}P(Z|Y,\theta^{(i)})log{P(Y,Z|\theta)},\\
\theta^{(i+1)}==\mathop{argmax}\limits_{\theta}Q(\theta,\theta^{(i)})
\]</span> 是不是与最开始给出的EM算法一摸一样了🤩？到此，推导完毕🎉～</p>
<p>还有另外一种推导方法，使用KL散度来做(个人认为这种推导过程会更好理解一些)，具体推导过程如下：</p>
<p><img src="/2020/02/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/KL散度.jpg"></p>
<p>有几个需要注意的地方：<strong>EM算法对初值敏感，也就是说取不同的初值<span class="math inline">\(\theta^{(0)}\)</span>，那么结果会不一样；另外，EM算法不能保证全局最优。</strong></p>
<h2 id="em算法的收敛性证明">EM算法的收敛性证明</h2>
<p>其实，知道EM算法的推导就差不多了。但是，实际上，还有一个问题就是：<strong>这样迭代更新，是否能够保证对数似然函数<span class="math inline">\(L(\theta)\)</span>能收敛呢？</strong>证明过程如下:</p>
<p><img src="/2020/02/22/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-EM%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpeg"></p>
<p>OK，证明完毕🎉～当然，EM算法的一个典型应用就是GMM模型，这个之后有时间再写一篇文章吧～</p>
<h2 id="em算法的实现">EM算法的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了python来实现EM算法，以及scikit-learn库来实现GMM模型。我的github里面可以下在到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下</p>
<h3 id="em算法的python实现">EM算法的python实现</h3>
<h3 id="gmm模型的scikit-learn实现">GMM模型的scikit-learn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#Author:codewithzichao</span></span><br><span class="line"><span class="comment">#Date:2020-1-10</span></span><br><span class="line"><span class="comment">#E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    X, y_true = make_blobs(n_samples=<span class="number">400</span>, centers=<span class="number">4</span>,</span><br><span class="line">                           cluster_std=<span class="number">0.60</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    X = X[:, ::<span class="number">-1</span>]  <span class="comment"># 交换列是为了方便画图</span></span><br><span class="line"></span><br><span class="line">    gmm = GaussianMixture(n_components=<span class="number">4</span>).fit(X)</span><br><span class="line">    labels = gmm.predict(X)</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, s=<span class="number">40</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>EM算法</tag>
        <tag>GMM</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|决策树模型原理详解与实现</title>
    <url>/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>决策树模型一种基本的分类与回归方法，是shallow learning的Adaboost、XGBoost、Light GBM、catBoost等树模型的基础，对于理解这些模型大有裨益。这篇博客将详细地讲解基本的决策树模型，主要会侧重回归树的讲解，因为这是Adaboost、XGBoost、Light GBM、catBoost等树模型的核心组成部分。并采用python与scikit-learn来对其进行实现。</p>
<a id="more"></a>
<h2 id="决策树模型介绍">决策树模型介绍</h2>
<p>决策树模型是比较简单的模型。它的三个核心问题是：<strong>特征选择、决策树的生成、决策树的剪枝</strong>。</p>
<h3 id="特征选择">特征选择</h3>
<p>所谓的特征选择，我们可以这么想：在没有构建决策树之前，只看训练数据集，是很混乱的，因为我们无法根据训练数据集，直接判断新的实例的类别，也就是说，训练集是没有分类能力的。那么，我们就需要构建一套规则，当我们应用这套规则的时候，我们能够得到实例的类别。那么，问题来了：<strong>我们怎么选择分类的特征，才能使得分类的效果最好呢？</strong>这就是特征选择需要做的事情。常见的应用与特征选择的准则有：<strong>信息增益与信息增益比。</strong></p>
<h4 id="信息增益">信息增益</h4>
<p><strong>信息增益表示在已知特征<span class="math inline">\(A\)</span>的条件下，从而使得数据集的不确定性减少的程度。</strong>看到不确定性，很自然地就会联想到熵！因为熵正是用来度量随机变量不确定性的程度。那么，下面给出熵的定义：</p>
<p>假设离散随机变量<span class="math inline">\(X\)</span>的概率分布是：<span class="math inline">\(P(X=i)=p_i,i=1,2,...,n\)</span>，那么随机变量<span class="math inline">\(X\)</span>的熵如下： <span class="math display">\[
H(p)=-\sum_{i=1}^{n}p_ilog(p_i)
\]</span> 在给定<span class="math inline">\(X\)</span>的情况下，随机变量<span class="math inline">\(Y\)</span>的条件概率分布<span class="math inline">\(P(Y|X)\)</span>的条件熵如下： <span class="math display">\[
H(Y|X)=\sum_{i=1}^{n}p_iP(Y|X=x_i)
\]</span> 当其中的概率是由数据估计(MLE)得到的时候，就称为经验条件熵。</p>
<p>在知道熵的概念之后，那么信息增益的定义如下： <span class="math display">\[
g(D,A)=H(D)-H(D|A)
\]</span> 其中，<span class="math inline">\(D\)</span>表示训练数据集，<span class="math inline">\(A\)</span>表示特征。即：特征A对于训练数据集<span class="math inline">\(D\)</span>的信息增益就等于<span class="math inline">\(D\)</span>的经验熵与给定特征<span class="math inline">\(A\)</span>的情况下<span class="math inline">\(D\)</span>的经验条件熵之差。那么对于决策树模型来说，特征选择的准则是：<strong>选择信息增益大的特征，因为信息增益大的特征具有更强的分类能力。</strong>具体过程如下：</p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg"></p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom: 50%;"></p>
<h4 id="信息增益比">信息增益比</h4>
<p>那么有了信息增益，为啥还要有信息增益比呢？原因在于：使用信息增益准则会导致决策树会更加偏向于特征取值数目多的特征。因为，选取特征取值数目多的特征，会让训练集的信息增益增大，也就是整个训练集的不纯度降低。但是，这样以来，会导致构建的决策树模型容易过拟合。因此，就有了信息增益比。 <span class="math display">\[
g_R(D,A)=\frac {g(D,A)}{H_A(D)},\\
H_A(D)=-\sum_{i=1}^{n}\frac {|D_i|}{|D|}log_2\frac {|D_i|}{|D|}
\]</span></p>
<h3 id="决策树的生成">决策树的生成</h3>
<p>决策树的生成算法有3中：ID3、C4.5、CART。在这里，我将讲解ID3、C4.5。CART将单独讲，因为其是后来那些大火的集成模型的核心部分。ID3与C4.5其实差不多，但是它们之间的区别在于特征选择的准则不同：<strong>ID3使用了信息增益，C4.5使用了信息增益比。</strong>在这里，我放上《统计学习方法》中关于C4.5的算法过程。</p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.jpg" style="zoom:50%;"></p>
<h3 id="决策树的剪枝">决策树的剪枝</h3>
<p>当我们构建好了决策树之后，我们会发现这样构建的决策树很容易发生过拟合。原因在于：<strong>我们在构建决策树的时候，尽可能地去拟合训练数据，从而得到了过于复杂的决策树。</strong>那么一种很自然的想法就是：对决策树进行剪枝，从而让决策树不那么复杂。当然，这样以来，就会使得决策树的准确率下降，所以，我们就需要在模型复杂度与对训练集的预测误差之间做一个tradeoff。我们所用的损失函数如下： <span class="math display">\[
C_{\alpha}(T)=C(T)+\alpha|T|
\]</span> 其中，<span class="math inline">\(T\)</span>表示决策树，<span class="math inline">\(\alpha\)</span>是参数，用来平衡模型复杂度与预测误差之间的关系。<span class="math inline">\(C(T)\)</span>表示模型对训练数据的误差，<span class="math inline">\(|T|\)</span>表示模型的复杂度。当<span class="math inline">\(\alpha\)</span>越大，模型越简单。(我们可以这么记：当<span class="math inline">\(\alpha\)</span>为0的时候，决策树是过拟合的，所以增大<span class="math inline">\(\alpha\)</span>，会让决策树变得简单。)</p>
<h2 id="cart">CART</h2>
<p>CART，全名叫作：分类与回归树。所以，正如名字一样，它既可以用于分类，也可以用于回归问题。在<strong>分类问题</strong>中，使用的特征选择的准则是：<strong>基尼指数(Gini)最小化</strong>；对于<strong>回归问题</strong>，使用的生成方法是：<strong>平方误差最小化</strong>。分类问题我就不介绍了，只介绍一下用于回归问题的回归树。</p>
<p>给定训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)</span>，那么回归问题就是我们要构造一个函数<span class="math inline">\(f(x)\)</span>，能够使得训练数据集的MSE最小，即： <span class="math display">\[
min\sum_{i=1}^{N}(f(x_i)-y_i)^2
\]</span> 假设，我们将输入空间划分为<span class="math inline">\(R_1,R_2,...,R_M\)</span>，并且在每一个区域有一个输出常数<span class="math inline">\(c_m\)</span>。那么目标函数可以表示为： <span class="math display">\[
min\sum_{i=1}^{N}\sum_{x_i\in R_m}(c_m-y_i)^2
\]</span> 而其中<span class="math inline">\(c_m\)</span>的最优值为就是为该区域的均值，如下： <span class="math display">\[
\hat c_m=ave(y_i|x_i\in R_m)
\]</span> 所以，回归树就可以表示为： <span class="math display">\[
f(x)=\sum_{m=1}^{M}c_mI(x\in R_m)
\]</span> 那么关键是，<strong>怎么对输入空间进行划分呢？</strong>方法：比那里所有的切分变量与切分点，找到使得MSE最小的划分。假设首先随机选择第<span class="math inline">\(j\)</span>个特征做为划分变量，其对应的值为<span class="math inline">\(s\)</span>，那么我们就讲空间划分为两个，如下： <span class="math display">\[
R_1(j,s)=\{x|x^{(j)}&lt;=s\},\\
R_2(j,s)=\{x|x^{(j)}&gt;s\}
\]</span> 接下来，我们需要通过如下函数<span class="math inline">\(m(s)\)</span>，从而找到最优的划分变量与划分值，如下： <span class="math display">\[
\mathop{min}\limits_{j,s}[\mathop{min}\limits_{c_1}\sum_{x_i\in R_1}{(y_i-c_1)^2}+\mathop{min}\limits_{c_2}\sum_{x_i\in R_2}{(y_i-c_2)^2}]
\]</span> 其中，<span class="math inline">\(\hat c_m=ave(y_i|x_i\in R_m),m=1,2\)</span>。那么，遍历所有的变量，找到最小<span class="math inline">\(m(s)\)</span>的<span class="math inline">\(j,s,c_m\)</span>，那么就可以得到最后的回归树了。(在这里附上《统计学习方法》的图)。具体的计算例子，参考：<a href="https://zhuanlan.zhihu.com/p/36108972" target="_blank" rel="noopener">GBDT计算</a>，这里虽然讲解的是提升树，但是由于回归问题的提升树是以回归树做为基本分类器，所以其中也涉及到了回归树的构建过程。</p>
<p><img src="/2020/02/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg" style="zoom:50%;"></p>
<p>OK，理论部分就讲完了🎉～</p>
<h2 id="决策树模型的实现">决策树模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现决策树模型：python实现以及调用scikit-learn库来实现。我的github里面可以下载到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<h3 id="scikit-learn实现">scikit-learn实现</h3>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>CART</tag>
        <tag>回归树</tag>
        <tag>decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|NER-LatticeLSTM模型</title>
    <url>/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解一下《Chinese NER Using Lattice LSTM》论文，即：LatticeLSTM模型。</p>
<a id="more"></a>
<p>LatticeLSTM模型来源于2018年ACL上的《Chinese NER Using Lattice LSTM》论文。非常经典，而且它release的code非常的规范，很值得一读～</p>
<h2 id="latticelstm模型提出的背景">LatticeLSTM模型提出的背景</h2>
<p>目前在NER中，主要分为两大类：基于char level的model与基于word level的model。对于目前的baseline—LSTM+CRF，很多实验中都证明了：对于中文，基于word level的model要比基于char level的model要好。但是两者也都有各自的优劣势。char level model最大的缺点在于：<strong>无法很好的利用字与字之间的有效信息</strong>；word level model最大的缺点在于：<strong>在中文中，一般都需要分词，分词的准确率直接影响到最终的NER结果，所以不好的分词会带来不好的NER结果，而目前word level model还无法很好地处理这个问题。</strong>基于这个背景，一个很自然地想法就是：<strong>如果能将词汇的信息融入到char level model中，那么就能够提高模型的效果</strong>，这就是LatticeLSTM模型。</p>
<blockquote>
<p>注意：在中英文中，char与word的含义是不一样的。在中文中：char指的是单个字，word指的是词语。</p>
</blockquote>
<h2 id="latticelstm模型介绍">LatticeLSTM模型介绍</h2>
<p>个人认为这篇文章数学符号较多，而且整体逻辑并不是很清晰，所以不是很好理解（虽然公式倒是挺简单的），先放模型图吧～</p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/LatticeLSTM.jpg"></p>
<ul>
<li><p><strong>模型输入：</strong>假设输入的序列为<span class="math inline">\(\{c_1,c_2,c_3,...,c_n\}\)</span>，其中<span class="math inline">\(c_i\)</span>表示第<span class="math inline">\(i\)</span>个字，那么整个模型的输入有两部分：<span class="math inline">\(\{c_1,c_2,c_3,...,c_n\}\)</span>以及与词典<span class="math inline">\(\cal D\)</span>相匹配的所有词语。</p></li>
<li><p><strong>embedding+LSTM：</strong>这一部分就是将词汇信息融入到char level model中。有三大部分：字本身的char embedding及LSTM部分、以当前字为结尾的相关的词语的embedding信息以及LSTM部分、两者的融合。</p>
<blockquote>
<p>注意，在论文里面，演示的是单向的LSTM，但是如果去看代码的话，可以看到也可以使用双向的LSTM，只要把forward和backward的结果拼接在一起，然后输送到CRF中即可。</p>
</blockquote>
<ul>
<li><strong>字<span class="math inline">\(c_j\)</span>的char embedding</strong>较为简单，具体表示如下：</li>
</ul>
<p><span class="math display">\[
x_{j}^{c}=e^c(c_j)
\\
h^{c}_{j}=LSTM(x_{j}^{c})
\]</span></p>
<ul>
<li><p>对于<strong>以当前字为结尾的相关的词语的embedding信息</strong>的计算，相对比较复杂一些，但是总体思想还是简单的。以<code>桥</code>为例，与它相关的词语信息有：<code>长江大桥</code>与<code>大桥</code>。具体计算公式如下：</p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/equation_lexicon%20embedding.svg"></p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/equation_lexicon_embedding_2.svg"></p>
<p>这个其实和普通的LSTM很像，只不过不更新输出。其中，<span class="math inline">\(x_{b,e}^{w}\)</span>表示是index从b到e的词语的embedding，公式是：<span class="math inline">\(x_{b,e}^{w}=e^w(w_{b,e}^{d})\)</span>，<span class="math inline">\(h_{b}^{c}\)</span>表示的是词语首个字的LSTM的输出的hidden state，<span class="math inline">\(c_{b}^{c}\)</span>表示的是词语首个字的LSTM的输出的cell state，通过这一步，得到的就是模型图中的红色部分的结果，也就是词汇的cell值：<span class="math inline">\(c_{b,e}^{w}\)</span>。</p></li>
<li><p><strong>字本身的信息与词汇信息进行融合</strong>。以<code>桥</code>为例，有三部分的信息：<code>桥</code>、<code>长江大桥</code>、<code>大桥</code>。融合的方式其实就是softmax。具体如下：</p></li>
</ul>
<p><span class="math display">\[
i_{b,e}^{c}=\sigma(W\left[\begin{matrix} x_{e}^{c} \\ c_{b,e}^{w} \end{matrix}\right]+b)
\\
i_{e}^{c}=\sigma(W\left[\begin{matrix} x_{e}^{c} \\ h_{j-1}^{c} \end{matrix}\right]+b)
\\(i_{e}^{c}就是char 输入到LSTM当中，计算的输入门！)
\]</span></p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/softmax.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/softmax1.jpg" style="zoom:50%;"></p>
<p>这一步我们最终得到的是<span class="math inline">\(c_{j}^{c}\)</span>，那么最终的输入到CRF中的是<span class="math inline">\(h_{j}^{c}\)</span>，其计算公式与LSTM是一样的，为：<span class="math inline">\(h_{j}^{c}=o_{j}^{c}*tanh(c_j^c)\)</span>。后面的就是CRF模型了，比较简单，不再赘述。需要注意的是，最后的loss，加了L2正则防止过拟合。</p></li>
</ul>
<h2 id="实验结果">实验结果</h2>
<ul>
<li>数据集：MSRA、OntoNotes、Weibo、resume(released by this paper)</li>
</ul>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/data.jpg" style="zoom:50%;"></p>
<ul>
<li><p>评价指标：P/R/F1(exact match)</p></li>
<li><p>使用了pretrained word embedding，在大规模的无监督的语料中使用word2vec算法，得到字向量与词向量，并在训练过程中fine-tune。</p></li>
<li><p>参数设置如下：</p></li>
</ul>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/参数.jpg" style="zoom:50%;"></p>
<ul>
<li>结果：</li>
</ul>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/ontonotes-result.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/msra-result.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/weibo.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/11/NLP-NER-LatticeLSTM%E6%A8%A1%E5%9E%8B/resume.jpg" style="zoom:50%;"></p>
<p>从实验结果来看，LatticeLSTM在四个数据集上都达到了SOTA，并且相比于base（LSTM+CRF）来说，提升效果是非常显著的。</p>
<ul>
<li>有意思的结论</li>
</ul>
<p>paper最后，还做了一些有意思的实验，总结起来有如下结论：</p>
<ol type="1">
<li>随着序列长度越来越长，NER的结果F1值也是逐渐下跌的，但是LatticeLSTM下跌的速度要慢一些，所以更加鲁棒。</li>
<li>lexicon的词语会影响到最终的结果，所以制作lexicon要更加小心谨慎一些，确保得到尽量准确的词语。</li>
</ol>
<h2 id="核心代码走读">核心代码走读</h2>
<p>原始代码挺清晰的，读起来应该不难，最核心的代码是latticelstm.py，后续的bilstm.py以及bilstmcrf.py都有调用latticelstm.py中的latticelstm部分。所以，在这里我就放一下latticelstm的代码部分吧，要是感兴趣的话，直接去读完整的原始代码就可以了～<a href="https://github.com/jiesutd/LatticeLSTM" target="_blank" rel="noopener">code link</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiInputLSTMCell</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""A basic LSTM cell."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, use_bias=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Most parts are copied from torch.nn.LSTMCell.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        super(MultiInputLSTMCell, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.use_bias = use_bias</span><br><span class="line">        self.weight_ih = nn.Parameter(</span><br><span class="line">            torch.FloatTensor(input_size, <span class="number">3</span> * hidden_size))</span><br><span class="line">        self.weight_hh = nn.Parameter(</span><br><span class="line">            torch.FloatTensor(hidden_size, <span class="number">3</span> * hidden_size))</span><br><span class="line">        self.alpha_weight_ih = nn.Parameter(</span><br><span class="line">            torch.FloatTensor(input_size, hidden_size))</span><br><span class="line">        self.alpha_weight_hh = nn.Parameter(</span><br><span class="line">            torch.FloatTensor(hidden_size, hidden_size))</span><br><span class="line">        <span class="keyword">if</span> use_bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.FloatTensor(<span class="number">3</span> * hidden_size))</span><br><span class="line">            self.alpha_bias = nn.Parameter(torch.FloatTensor(hidden_size))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line">            self.register_parameter(<span class="string">'alpha_bias'</span>, <span class="literal">None</span>)</span><br><span class="line">        self.reset_parameters()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize parameters following the way proposed in the paper.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        init.orthogonal(self.weight_ih.data)</span><br><span class="line">        init.orthogonal(self.alpha_weight_ih.data)</span><br><span class="line"></span><br><span class="line">        weight_hh_data = torch.eye(self.hidden_size)</span><br><span class="line">        weight_hh_data = weight_hh_data.repeat(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        self.weight_hh.data.set_(weight_hh_data)</span><br><span class="line"></span><br><span class="line">        alpha_weight_hh_data = torch.eye(self.hidden_size)</span><br><span class="line">        alpha_weight_hh_data = alpha_weight_hh_data.repeat(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.alpha_weight_hh.data.set_(alpha_weight_hh_data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The bias is just set to zero vectors.</span></span><br><span class="line">        <span class="keyword">if</span> self.use_bias:</span><br><span class="line">            init.constant(self.bias.data, val=<span class="number">0</span>)</span><br><span class="line">            init.constant(self.alpha_bias.data, val=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_, c_input, hx)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            batch = 1</span></span><br><span class="line"><span class="string">            input_: A (batch, input_size) tensor containing input</span></span><br><span class="line"><span class="string">                features.</span></span><br><span class="line"><span class="string">            c_input: A  list with size c_num,each element is the input ct from skip word (batch, hidden_size).</span></span><br><span class="line"><span class="string">            hx: A tuple (h_0, c_0), which contains the initial hidden</span></span><br><span class="line"><span class="string">                and cell state, where the size of both states is</span></span><br><span class="line"><span class="string">                (batch, hidden_size).</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            h_1, c_1: Tensors containing the next hidden and cell state.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        h_0, c_0 = hx <span class="comment"># 初始的hidden state与cell value，shape=[batch_size,hidden_size]</span></span><br><span class="line">        batch_size = h_0.size(<span class="number">0</span>) <span class="comment"># 记住pytorch中这种获取指定维度的方式</span></span><br><span class="line">        <span class="keyword">assert</span>(batch_size == <span class="number">1</span>)</span><br><span class="line">        bias_batch = (self.bias.unsqueeze(<span class="number">0</span>).expand(batch_size, *self.bias.size()))</span><br><span class="line">        wh_b = torch.addmm(bias_batch, h_0, self.weight_hh) <span class="comment">#[batch_size,3*hidden_size]</span></span><br><span class="line">        wi = torch.mm(input_, self.weight_ih) <span class="comment">#[batch_size,3*hidden_size]</span></span><br><span class="line">        i, o, g = torch.split(wh_b + wi, split_size=self.hidden_size, dim=<span class="number">1</span>) <span class="comment">#element size:[batch_size,hidden_size]</span></span><br><span class="line">        g = torch.tanh(g) <span class="comment"># totile g</span></span><br><span class="line">        o = torch.sigmoid(o) <span class="comment"># 输出门</span></span><br><span class="line">        c_num = len(c_input) <span class="comment"># 词汇的数目</span></span><br><span class="line">        <span class="keyword">if</span> c_num == <span class="number">0</span>:</span><br><span class="line">            f = <span class="number">1</span> - i <span class="comment"># 遗忘门</span></span><br><span class="line">            c_1 = f*c_0 + i*g <span class="comment"># cell值</span></span><br><span class="line">            h_1 = o * torch.tanh(c_1) <span class="comment"># 新的 hidden state</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c_input_var = torch.cat(c_input, <span class="number">0</span>) <span class="comment">#[c_num,hidden_size]</span></span><br><span class="line">            alpha_bias_batch = (self.alpha_bias.unsqueeze(<span class="number">0</span>).expand(batch_size, *self.alpha_bias.size())) <span class="comment"># [batch_size,hidden_size]</span></span><br><span class="line">            c_input_var = c_input_var.squeeze(<span class="number">1</span>) <span class="comment">## [c_num, hidden_size]</span></span><br><span class="line">            alpha_wi = torch.addmm(self.alpha_bias, input_, self.alpha_weight_ih).expand(c_num, self.hidden_size)</span><br><span class="line">            alpha_wh = torch.mm(c_input_var, self.alpha_weight_hh)<span class="comment">#[batch_size,hidden_size]</span></span><br><span class="line">            alpha = torch.sigmoid(alpha_wi + alpha_wh) <span class="comment">#[batch_size,hidden_size]</span></span><br><span class="line">            <span class="comment">## alpha  = i concat alpha</span></span><br><span class="line">            alpha = torch.exp(torch.cat([i, alpha],<span class="number">0</span>))</span><br><span class="line">            alpha_sum = alpha.sum(<span class="number">0</span>)</span><br><span class="line">            <span class="comment">## alpha = softmax for each hidden element</span></span><br><span class="line">            alpha = torch.div(alpha, alpha_sum)</span><br><span class="line">            merge_i_c = torch.cat([g, c_input_var],<span class="number">0</span>)</span><br><span class="line">            c_1 = merge_i_c * alpha</span><br><span class="line">            c_1 = c_1.sum(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            h_1 = o * torch.tanh(c_1)</span><br><span class="line">        <span class="keyword">return</span> h_1, c_1</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        s = <span class="string">'&#123;name&#125;(&#123;input_size&#125;, &#123;hidden_size&#125;)'</span></span><br><span class="line">        <span class="keyword">return</span> s.format(name=self.__class__.__name__, **self.__dict__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LatticeLSTM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""A module that runs multiple steps of LSTM."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, hidden_dim, word_drop, word_alphabet_size, word_emb_dim, pretrain_word_emb=None, left2right=True, fix_word_emb=True, gpu=True,  use_bias = True)</span>:</span></span><br><span class="line">        super(LatticeLSTM, self).__init__()</span><br><span class="line">        skip_direction = <span class="string">"forward"</span> <span class="keyword">if</span> left2right <span class="keyword">else</span> <span class="string">"backward"</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"build LatticeLSTM... "</span>, skip_direction, <span class="string">", Fix emb:"</span>, fix_word_emb, <span class="string">" gaz drop:"</span>, word_drop</span><br><span class="line">        self.gpu = gpu</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.word_emb = nn.Embedding(word_alphabet_size, word_emb_dim)</span><br><span class="line">        <span class="keyword">if</span> pretrain_word_emb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"load pretrain word emb..."</span>, pretrain_word_emb.shape</span><br><span class="line">            self.word_emb.weight.data.copy_(torch.from_numpy(pretrain_word_emb))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word_emb.weight.data.copy_(torch.from_numpy(self.random_embedding(word_alphabet_size, word_emb_dim)))</span><br><span class="line">        <span class="keyword">if</span> fix_word_emb:</span><br><span class="line">            self.word_emb.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        self.word_dropout = nn.Dropout(word_drop)</span><br><span class="line"></span><br><span class="line">        self.rnn = MultiInputLSTMCell(input_dim, hidden_dim)</span><br><span class="line">        self.word_rnn = WordLSTMCell(word_emb_dim, hidden_dim)</span><br><span class="line">        self.left2right = left2right</span><br><span class="line">        <span class="keyword">if</span> self.gpu:</span><br><span class="line">            self.rnn = self.rnn.cuda()</span><br><span class="line">            self.word_emb = self.word_emb.cuda()</span><br><span class="line">            self.word_dropout = self.word_dropout.cuda()</span><br><span class="line">            self.word_rnn = self.word_rnn.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">random_embedding</span><span class="params">(self, vocab_size, embedding_dim)</span>:</span></span><br><span class="line">        pretrain_emb = np.empty([vocab_size, embedding_dim])</span><br><span class="line">        scale = np.sqrt(<span class="number">3.0</span> / embedding_dim)</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(vocab_size):</span><br><span class="line">            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [<span class="number">1</span>, embedding_dim])</span><br><span class="line">        <span class="keyword">return</span> pretrain_emb</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, skip_input_list, hidden=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            input: variable (batch, seq_len), batch = 1</span></span><br><span class="line"><span class="string">            skip_input_list: [skip_input, volatile_flag]</span></span><br><span class="line"><span class="string">            skip_input: three dimension list, with length is seq_len. Each element is a list of matched word id and its length. </span></span><br><span class="line"><span class="string">                        example: [[], [[25,13],[2,3]]] 25/13 is word id, 2,3 is word length . </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        volatile_flag = skip_input_list[<span class="number">1</span>]</span><br><span class="line">        skip_input = skip_input_list[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.left2right:</span><br><span class="line">            skip_input = convert_forward_gaz_to_backward(skip_input)</span><br><span class="line">        input = input.transpose(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">        seq_len = input.size(<span class="number">0</span>)</span><br><span class="line">        batch_size = input.size(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">assert</span>(batch_size == <span class="number">1</span>)</span><br><span class="line">        hidden_out = []</span><br><span class="line">        memory_out = []</span><br><span class="line">        <span class="keyword">if</span> hidden:</span><br><span class="line">            (hx,cx)= hidden</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hx = autograd.Variable(torch.zeros(batch_size, self.hidden_dim))</span><br><span class="line">            cx = autograd.Variable(torch.zeros(batch_size, self.hidden_dim))</span><br><span class="line">            <span class="keyword">if</span> self.gpu:</span><br><span class="line">                hx = hx.cuda()</span><br><span class="line">                cx = cx.cuda()</span><br><span class="line">        </span><br><span class="line">        id_list = range(seq_len)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.left2right:</span><br><span class="line">            id_list = list(reversed(id_list))</span><br><span class="line">        input_c_list = init_list_of_objects(seq_len)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> id_list:</span><br><span class="line">            (hx,cx) = self.rnn(input[t], input_c_list[t], (hx,cx))</span><br><span class="line">            hidden_out.append(hx)</span><br><span class="line">            memory_out.append(cx)</span><br><span class="line">            <span class="keyword">if</span> skip_input[t]:</span><br><span class="line">                matched_num = len(skip_input[t][<span class="number">0</span>])</span><br><span class="line">                word_var = autograd.Variable(torch.LongTensor(skip_input[t][<span class="number">0</span>]),volatile =  volatile_flag)</span><br><span class="line">                <span class="keyword">if</span> self.gpu:</span><br><span class="line">                    word_var = word_var.cuda()</span><br><span class="line">                word_emb = self.word_emb(word_var)</span><br><span class="line">                word_emb = self.word_dropout(word_emb)</span><br><span class="line">                ct = self.word_rnn(word_emb, (hx,cx))</span><br><span class="line">                <span class="keyword">assert</span>(ct.size(<span class="number">0</span>)==len(skip_input[t][<span class="number">1</span>]))</span><br><span class="line">                <span class="keyword">for</span> idx <span class="keyword">in</span> range(matched_num):</span><br><span class="line">                    length = skip_input[t][<span class="number">1</span>][idx]</span><br><span class="line">                    <span class="keyword">if</span> self.left2right:</span><br><span class="line">                        <span class="comment"># if t+length &lt;= seq_len -1:</span></span><br><span class="line">                        input_c_list[t+length<span class="number">-1</span>].append(ct[idx,:].unsqueeze(<span class="number">0</span>))</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># if t-length &gt;=0:</span></span><br><span class="line">                        input_c_list[t-length+<span class="number">1</span>].append(ct[idx,:].unsqueeze(<span class="number">0</span>))</span><br><span class="line">                <span class="comment"># print len(a)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.left2right:</span><br><span class="line">            hidden_out = list(reversed(hidden_out))</span><br><span class="line">            memory_out = list(reversed(memory_out))</span><br><span class="line">        output_hidden, output_memory = torch.cat(hidden_out, <span class="number">0</span>), torch.cat(memory_out, <span class="number">0</span>)</span><br><span class="line">        <span class="comment">#(batch, seq_len, hidden_dim)</span></span><br><span class="line">        <span class="comment"># print output_hidden.size()</span></span><br><span class="line">        <span class="keyword">return</span> output_hidden.unsqueeze(<span class="number">0</span>), output_memory.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>顺便记录一下另外看的一片IDCNN+CRF的论文，比较简单，而且效果一般，主要的卖点就是快，但实际上我感觉没有快多少。</p>
<h2 id="idcnncrf模型">IDCNN+CRF模型</h2>
<p>在NER中，RNN系列的模型成为了标准的提取文本表示的模型，但是RNN一个很大的缺陷是：无法并行化，从而导致整个模型运行非常慢，计算复杂度与序列长度成正相关。而CNN系列的模型的一大优势就是可以并行化，且计算复杂度与序列长度无关，只与层数有关。但是CNN应用于NER的一个非常大的问题在于：每一次的卷积感受野较小，对于NER这种非常看重句子长依赖的NLP任务，具有很大的劣势，一种办法是通过不断叠加卷积层来扩大感受野，但是这样就会导致模型的计算复杂度与序列长度成正相关。所以，<strong>如何能够让整个模型并行化，同时又能够较好地捕捉到整个句子的长依赖关系呢？</strong>这就是IDCNN+CRF模型的由来。至于什么是空洞卷积，请见<a href="https://zhuanlan.zhihu.com/p/113285797" target="_blank" rel="noopener">link</a>.</p>
<h2 id="参考文献">参考文献</h2>
<p>《Chinese NER Using Lattice LSTM》</p>
<p>《Fast and Accurate Entity Recognition with Iterated Dilated Convolutions》</p>
<p>https://zhuanlan.zhihu.com/p/143272435</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>命名实体识别</tag>
        <tag>LatticeLSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|RE-Dialogue-based relation extraction</title>
    <url>/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/</url>
    <content><![CDATA[<p>这篇博客主要讲解两篇论文：《Dialogue-Based Relation Extraction》与《Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks》。</p>
<a id="more"></a>
<h2 id="dialogue-based-relation-extractionacl2020">Dialogue-Based Relation Extraction(ACL2020)</h2>
<h3 id="background">Background</h3>
<p>目前的document-level RE dataset都是使用比较正式的文本构建而成的，这样构建的数据集内部含有丰富的逻辑推理关系，并且有足够的信息能够提取出关键信息。但是在dialogue relation extraction中，对话文本往往是非正式的、语法不严谨的，虽然逻辑简单，但是其信息密度低，且很容易有歧义的问题。而目前针对于DRE任务仍然没有引起广泛的研究。所以这篇paper构建了第一个dialogue-based RE dataset——DialogRE，并且提出了新的评价指标：<span class="math inline">\(F1_c\)</span>以及相关的baseline，用于以后的研究。</p>
<blockquote>
<p>之前一直很喜欢《老友记》，有人能够根据《老友记》来构建数据集，真是有生之年啊！爱了爱了🤩</p>
</blockquote>
<h3 id="statistics-of-dialogre">Statistics of DialogRE</h3>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/statistics_DialogRE.jpg" style="zoom:50%;"></p>
<p>DialogRE数据集是根据美国电视情景喜剧《老友记》中共10季的台词构建而来，共有1788个dialogue，10168个triple, 其中train:dev:test=6:2:2，每一个样本的形式还是SPO，与其他的RE并无区别。但是DialogRE与其他的document-level RE dataset最大的不同是：1. argument pair的relation span sentences的比例(95.6%)要其他的数据集(40.7%)高得多高得多(95.6%)；2.以speaker为中心，77.3%的triple全是speaker name，90.0%的triple至少包含一个speaker name；3.信息密度低，只有0.21/per sen。DialogRE数据集与其他数据集的比较如下：</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/DialogRE_otherRE.jpg"></p>
<p>下面是dialogRE数据集中的relation type：</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/dialog_realtion_type.jpg"></p>
<p>⚠️在dialogRE中，有一个概念叫做trigger，其实和DocRED中的evidence是差不多的，用来证明argument pair的relation的正确性。</p>
<h3 id="task-formulation-and-evaluation">Task formulation and Evaluation</h3>
<p>给定一个dialogue：<span class="math inline">\(D=s_1:t_1,...,s_m:t_m\)</span>以及argument pair: <span class="math inline">\((a_1,a_2)\)</span>，其中<span class="math inline">\(s_i,t_i\)</span>表示第<span class="math inline">\(i\)</span>轮的speaker id与text，我们的目标是得到<span class="math inline">\((a_1,a_2)\)</span>的relation。有两个评价方法：<span class="math inline">\(F1\)</span>与<span class="math inline">\(F1_c\)</span>。<span class="math inline">\(F1\)</span>其实就是我们寻常用的，而<span class="math inline">\(F1_c\)</span>是这篇paper作者专门设计的，下面具体讲讲～</p>
<p>我们首先定义一些符号：<span class="math inline">\(O_i\)</span>表示前<span class="math inline">\(i\)</span>轮对话中，使用模型预测得出的argument pair<span class="math inline">\((a_1,a_2)\)</span>的relation type set，<span class="math inline">\(L\)</span>表示在整个对话中argument pair<span class="math inline">\((a_1,a_2)\)</span>的真实的relation type set，<span class="math inline">\(R\)</span>表示所有的36个relation type，所以：<span class="math inline">\(O_i,L\in R\)</span>；我们定义辅助函数<span class="math inline">\(j(x)\)</span>，如果<span class="math inline">\(x\)</span>不出现在<span class="math inline">\(D\)</span>中，那么返回<span class="math inline">\(m\)</span>，否则的话，返回<span class="math inline">\(x\)</span>第一次出现的对话轮数<span class="math inline">\(i\)</span>；我们定义辅助函数<span class="math inline">\(i(r)\)</span>，表示：对于每一个relation type <span class="math inline">\(r\in L\)</span>，如果存在一个trigger for <span class="math inline">\(r\)</span>，那么<span class="math inline">\(i(r)=j(\lambda_r)\)</span>，其中<span class="math inline">\(\lambda_r\)</span>表示trigger，否则<span class="math inline">\(i(r)=m\)</span>，对于relation <span class="math inline">\(r{\in} R {\text \ }L\)</span>，<span class="math inline">\(i(r)=1\)</span>。我们使用<span class="math inline">\(E_i\)</span>表示前<span class="math inline">\(i\)</span>轮可以评估的relation type set，如下： <span class="math display">\[
E_i=\{r|i\geq max\{j(a_1),j(a_2),i(r)\}\}
\]</span> <span class="math inline">\(E_i\)</span>的意思是：在前<span class="math inline">\(i\)</span>轮对话中，如果出现了<span class="math inline">\(a_1,a_2\)</span>以及与relation <span class="math inline">\(r\)</span>相关的trigger的话，那么relation <span class="math inline">\(r\)</span>就是可评估的，也就是说作为候选。之所以这样定义，是因为这样做可以知道对于每一个argument pair，我们大致可以推算预测其relation大概需要多少轮数。之后，我们定义conversation precision与recall依旧conversation F1，如下： <span class="math display">\[
P_c(D,a_1,a_2)=\frac{\sum_{i=1}^{m}|O_i\cap L\cap E_i|}{\sum_{i=1}^{m}|O_i\cap E_i|} \\
R_c(D,a_1,a_2)=\frac{\sum_{i=1}^{m}|O_i\cap L\cap E_i|}{\sum_{i=1}^{m}|L\cap E_i|} \\
P_c=\frac{\sum_{D^{\prime},a_1^{\prime},a_2^{\prime}}P_c(D^{\prime},a_1^{\prime},a_2^{\prime})}{\sum_{D^{\prime},a_1^{\prime},a_2^{\prime}}1} \\
R_c=\frac{\sum_{D^{\prime},a_1^{\prime},a_2^{\prime}}R_c(D^{\prime},a_1^{\prime},a_2^{\prime})}{\sum_{D^{\prime},a_1^{\prime},a_2^{\prime}}1} \\
F1_c=\frac{2P_cR_c}{P_c+R_c} 
\]</span></p>
<h3 id="baseline">Baseline</h3>
<p>在paper中，介绍了四种baseline：Majority、CNN/LSTM/BILSTM、BERT、BERT<span class="math inline">\(_s\)</span>。所谓的Majority就是：如果一个argument pair的relation没出现在training set当中，那么输出training set中数目最大的relation type，否则的话，输出与argument pair最相关的relation type；CNN/LSTM/BILSTM指的是：采用CNN/LSTM/BILSTM作为编码器；BERT指的是：采用BERT以及BERT-like预训练模型作为编码器，不过其输入格式是：<span class="math inline">\([CLS]d[SEP]a_1[SEP]a_2[SEP]\)</span>；BERT<span class="math inline">\(_s\)</span>是作者所改进的一个方法，指的是：仍然采用BERT以及BERT-like预训练模型作为编码器，但是其输入格式是：<span class="math inline">\([CLS]\hat d[SEP]\hat a_1[SEP]\hat a_2[SEP]\)</span>，其中： <span class="math display">\[
\hat d=\hat s_1:\hat t_1,...,\hat s_n:\hat t_n \\
\hat s_i=\begin{cases}[S1] &amp; if \ s_i=a_1 \\ [S2] &amp; if \ s_1=a_2 \\ s_1 &amp; otherwise \end{cases}
\]</span> 其中，<span class="math inline">\([S1],[S2]\)</span>表示两个新的特殊的token。</p>
<h3 id="experiment">Experiment</h3>
<p>放一下上述四种方法在DialogRE数据集上的结果:</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/dialogRE_result2.jpg"></p>
<p>下面是加了argument type之后的结果，发现结果是有提升的，说明argument type是有作用的。</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/dialog_result_1.jpg" style="zoom:50%;"></p>
<p>总的来说，DialogRE是一个新的工作，在这个上面还是可以做很多的工作的。</p>
<h2 id="dialogue-relation-extraction-with-document-level-heterogeneous-graph-attention-networksarxiv2020">Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks(arxiv2020)</h2>
<h3 id="background-1">Background</h3>
<p>这篇paper研究的是dialogue relation extraction task。之前的关于DRE的方法都没有很好地利用好speaker的信息，所以就忽略了speaker之间的信息可能会带给argument pair的影响。怎么解决呢？在这篇paper当中，采用的是基于graph的方法，提出了HGAT模型。</p>
<h3 id="model">Model</h3>
<p>先放图～</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/HGAT_model.jpg"></p>
<p>整个HGAT模型分为三部分：<strong>utterance encoder、graph construction、relation classification。</strong></p>
<ul>
<li><p><strong>task definition：</strong>给定含有<span class="math inline">\(N\)</span>个utterance的dialogue：<span class="math inline">\(D=\{u_1,...,u_N\}\)</span>以及argument pair：<span class="math inline">\({\cal A}={(x_1,y_1),...,(x_1,y_1)}\)</span>，其中<span class="math inline">\(x_i,y_i\)</span>是dialogue中的entity，我们的目标是：建立模型，预测argument pair的relation。</p></li>
<li><p><strong>data processing：</strong>使用spaCy获得POS tag以及named-entity types of each token。</p></li>
<li><p><strong>utterance encoder：</strong>对于每一个utterance <span class="math inline">\(u_i\)</span>，我们使用GloVe初始化，并且与其他两种embedding进行concat，得到最终的初始的token embedding，如下： <span class="math display">\[
e=[e_w;e_p;e_t]
\]</span> 其中，<span class="math inline">\(e_p\)</span>表示POS embedding，<span class="math inline">\(e_t\)</span>表示entity-type embedding；然后送入encoder当中进行编码，这里使用的是BILSTM(其实其他的编码器也是OK的)<strong>(local BILSTM)</strong>，然后得到contextual rep，如下： <span class="math display">\[
\overleftarrow {h^{i}_{j}}=BILSTM_l(\overleftarrow {h^i_{j+1}},e^i_j) \\
\overrightarrow {h^{i}_{j}}=BILSTM_r(\overrightarrow {h^i_{j-1}},e^i_j) \\
h^i_j=[\overleftarrow {h^i_j};\overrightarrow {h^i_j}]
\]</span> 其中，<span class="math inline">\(e^i_j\)</span>表示第<span class="math inline">\(i\)</span>个utterance的第<span class="math inline">\(j\)</span>个token的初始embedding rep，<span class="math inline">\(h^i_j\)</span>表示第<span class="math inline">\(i\)</span>个utterance的第<span class="math inline">\(j\)</span>个token的rep。之后，我们对其进行max over time pooling，得到sentence vecotor，再送入到BILSTM当中<strong>(global BILSTM)</strong>，得到新的contextual rep，这个时候是三维的张量。</p></li>
<li><p><strong>graph construction：</strong>这一部分就是graph的构建，分为两部分：<strong>node construction、edge construction。</strong></p>
<ul>
<li><p>node construction：共有5种类型的node：utterance node、entity-type node、word node、speaker node、argument node。utterance node指的是：一个utterance是一个node，其表示就是使用的utterance encoder输出的结果；entity-type node指的是：word的type，每个named entity的每一个word都连接到其对应的entity-type node；word node指的是：对话的vocabulary，初始状态使用GloVe，每一个word node都与包含这个word的utterance node相连接，并且与在这个对话当中，这个word有可能的entity type node相连接；speaker node指的是：每一个唯一的speaker，并且其与ta自身的utterance相连接；argument node指的是：用来编码argument pair中的argument的相对位置信息(subject or object?)，每一个argument pair都有两个argument 。</p></li>
<li><p>edge construction：<strong>⚠️graph本身是无向的，但是propagation是有方向的。</strong>这里总共有5种edge：utterance-word、utterance-argument、utterance-speaker、type-word、type-argument。所有的edge都是无向的，并且都是随机初始化(除了utterance-word，其采用POS tag来初始化)。</p>
<blockquote>
<p>ps：我个人觉得这种需要用已有的语法分析工具来得到feature的方法不是很优雅，一方面这些语法分析工具得到的feature并不是很准确，另一方面一体化不好，个人拙见是一个好的模型应该是其本身与数据无关的、与第三方工具无关的、针对问题本身进行建模的。</p>
</blockquote></li>
<li><p>message propagation：构建好graph之后，我们就需要进行aggregate。由于我们构建的是一个异质图，所以paper中采用的是GAT与metapath(HGNN中经常使用的一种方法)。GAT很简单就不提了，不懂可见<a href="https://zhuanlan.zhihu.com/p/81350196?utm_source=wechat_session" target="_blank" rel="noopener">link</a>，主要讲一下metapath的设计，这对于HGNN的效果是最重要的。</p>
<p>Metapath：第一，使用utterance node来update word node、speaker node与argument node；第二，更新好的word node来update entity-type node；第三，更新好的entity-type node来update word node、argument node、entity-type node；第四，我们使用word node、speaker node、argument node来update utterance node；最后，utterance node再一次update word node、speaker node、argument node。即： <span class="math display">\[
V_u\rightarrow V_b\rightarrow V_t\rightarrow V_b\rightarrow V_u\rightarrow V_b
\]</span> 然后借鉴了ACL2020邱锡鹏老师那篇使用HGNN来做文本摘要的paper，对一个node的更新，都使用了残差链接，即：<span class="math inline">\(\hat h_i=\tilde h_i+h^{\prime}_i\)</span>，其中<span class="math inline">\(h^{\prime}_i\)</span>是GAT layer输入，<span class="math inline">\(\tilde h_i\)</span>是GAT layer的输出，然后我们使用FFN，增加非线性，即：<span class="math inline">\(h^{new}_i=FFN(\hat h_i)\)</span>。OK，在HGAT模型中，更新次数为三次，其整体而言公式如下： <span class="math display">\[
H^1_b=GAT(H^0_b,H^0_u) \\
H^1_t=GAT(H^0_t,H^1_b) \\
H^2_b=GAT(H^1_b,H^1_t) \\
H^1_u=GAT(H^0_u,H^2_b) \\
H^3_b=GAT(H^2_b,H^1_u)
\]</span> ⚠️注意，<span class="math inline">\(b\)</span>表示basic node，包括：word node、speaker node、argument node。</p></li>
</ul></li>
<li><p><strong>relation classifier：</strong>选择argument node <span class="math inline">\(\tau_x，\tau_y\)</span>与word node <span class="math inline">\(e_x,e_y\)</span>，将两者concat，并且送入到sigmoid中，如下： <span class="math display">\[
e^{\prime}_x=[maxpooling(\tau_x);maxpooling(e_x)] \\
e^{\prime}_y=[maxpooling(\tau_y);maxpooling(e_y)] \\
e^{\prime}=[e^{\prime}_x;e^{\prime}_y] \\
P(r|e_x,e_y)=sigmoid(W_ee^{\prime}+b_e)_r
\]</span></p></li>
</ul>
<h3 id="experiment-1">Experiment</h3>
<ol type="1">
<li><p>数据集：DialogRE</p></li>
<li><p>hyperparameters</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/HGAT_hyperparameters.jpg" style="zoom:50%;"></p></li>
<li><p>结果</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/HGAT_result.jpg"></p>
<p>从结果来看，提升还是蛮大的。我们再来看看消融实验与case study。</p></li>
<li><p>消融实验</p>
<p><img src="/2020/10/28/NLP-RE-Dialogue-based-relation-extraction/HGAT_result_ablation.jpg" style="zoom:50%;"></p>
<p>从消融实验结果来看：1.去掉global BILSTM的结果与POSembedding的影响比较大，这说明如何得到一个好的sentence vector并对其进行编码是非常重要的，另外，POS embedding的结果也说明句子结构的建模也是很重要的，所以如果建模也是一个值得探讨的问题；2. 一个好的embedding初始化很重要；3. 默认update是3次，目前是最优。我觉得这个消融实验做的一般，我觉得应该对不同的node与edge的作用也要进行实验，不然怎么证明你的graph是比较好的呢？</p></li>
<li><p>case study</p>
<p>从case study中可以得出：95%的argument pair是span 2个sentence，所以长距离建模很重要；entity-type很重要，如果模型无法从训练数据中获取到足够的信息，那么就会判断出错。这个case study也做的一般。。。</p>
<p>总的来说，HGAT模型一般吧，但是由于是第一个在DialogRE数据集上的模型，结果还是很令人满意的。</p></li>
</ol>
<h2 id="references">References</h2>
<p>《Dialogue-Based Relation Extraction》</p>
<p>《Dialogue Relation Extraction with Document-level Heterogeneous Graph Attention Networks》</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>关系抽取</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>dialogue-based RE</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|Adaptive Embedding paradigm in ChineseNER</title>
    <url>/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解中文NER的Adaptive Embedding paradigm。所谓的Adaptive Embedding paradigm，指的是只在embedding部分进行改进，从而让我们不需要设计非常的复杂的结构，能够迁移到不同的模型结构上(RNN/CNN/transformer etc)，所以这种方法的好处就在于其可迁移性。这篇文章主要讲解三篇论文：《An Encoding Strategy Based Word-Character LSTM for Chinese NER》、《A Neural Multi-digraph Model for Chinese NER with Gazetteers》以及ACL2020的《Simplify the Usage of Lexicon in Chinese NER》。</p>
<a id="more"></a>
<h2 id="wc-lstm模型">WC-LSTM模型</h2>
<h3 id="提出的背景">提出的背景</h3>
<p>WC-LSTM模型是2019年发表在NAACL上的《An Encoding Strategy Based Word-Character LSTM for Chinese NER》论文。提出的背景还是因为LatticeLSTM模型。LatticeLSTM是词汇增强的典型模型，但是其存在一个很严重的问题：<strong>由于每一个sentence所被匹配到的word的数量与位置都不一样，所以导致LatticeLSTM模型无法batch训练与inference。</strong>在WC-LSTM中，为了实现并行化，需要对每一个character所匹配到的words进行编码，从而得到一个fixed embedding，再与character本身的embedding进行融合，最后输送到网络中进行训练。所以，关键就在于：<strong>怎么得到一个fixed embedding？</strong>下面具体介绍。</p>
<h3 id="wc-lstm模型介绍">WC-LSTM模型介绍</h3>
<p>先放图～</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/wc-lstm.jpg"></p>
<ol type="1">
<li><strong>input：</strong>输入有两部分：sentence以及与sentence相匹配的lexicon words，假设输入的sentence表示为：<span class="math inline">\(s=\{c_1,c_2,...,c_n\}\)</span>，其中<span class="math inline">\(c_i\)</span>表示sentence的第<span class="math inline">\(i\)</span>个character；lexicon words表示为：<span class="math inline">\(c_{b,e}\)</span>，在前向的wc-lstm中，使用$ <span class="math inline">\(来表示以\)</span>c_i<span class="math inline">\(的结尾的matched lexicon words的集合，后向的也是类似的。所以最终的输入是：\)</span>={(c_1,),(c_2,),...,(c_n,)}<span class="math inline">\(，在WC-LSTM中，使用双向的LSTM，所以还有\)</span>={(c_n,),(c_{n-1},),...,(c_1,)}$。</li>
<li><strong>word-character embedding layer：</strong>在<span class="math inline">\(\overrightarrow {rs}\)</span>中，对于每一个character <span class="math inline">\(c_i\)</span>，对应一个lexicon words集合<span class="math inline">\(\overrightarrow{ws_i}=\{\overrightarrow{w_{i1}},...,\overrightarrow{w_{s^p_i}}\}\)</span>，<span class="math inline">\(\overrightarrow{ws_i}\)</span>中的word数目记为：<span class="math inline">\(s^t_i\)</span>，对于一个batch来说，我门通过padding的方式让每一个sentence的相同位置有同样的<span class="math inline">\(s^p_i\)</span>。每一个character的embedding，可以通过查pretrained char embedding table得到，记做：<span class="math inline">\(x^c_i=e^c(c_i)\)</span>，每一个lexicon word的embedding也可以查表，记做：<span class="math inline">\(x^{\overrightarrow{w}}_{il}=e(\overrightarrow{w_{il}})\)</span>。</li>
<li><strong>word encoding strategy：</strong>在WC-LSTM中，探索了四种encoding策略：最短词汇信息、最长词汇信息、avg、self-attention。最短词汇信息指的是：使用<span class="math inline">\(c_i\)</span>中的词汇集合中长度最短的词作为<span class="math inline">\(c_i\)</span>的词汇信息；最长词汇信息指的是：使用当前character的词汇集合中的最长的词作为<span class="math inline">\(c_i\)</span>的词汇信息；avg是对所有的词汇信息进行简单的算术平均；self-attention就是自己对自己进行attention，比较简单。得到<span class="math inline">\(c_i\)</span>的词汇信息后，将其与<span class="math inline">\(c_i\)</span>的embeding进行concat，作为后续的输入。</li>
<li><strong>wc-lstm：</strong>这就是前向的LSTM的结果与后向的LSTM的结果进行concat，作为CRF的输入。</li>
<li><strong>decoding：</strong>使用标准的CRF层。</li>
</ol>
<h3 id="实验结果">实验结果</h3>
<ol type="1">
<li>数据集：OntoNotes、MSRA、Weibo、resume</li>
<li>实验结果就不放了，比Lattice只稍微好一点点，这也是预料之中的，毕竟wc-lstm的主要目的是并行化。</li>
<li>实验还探究了不同的encoding策略，最后得出的结论是：如果训练数据比较充足的话，建议使用self-attention和avg，因为这两个参数较多，如果训练数据比较少的话，容易过拟合，从而在测试集上表现不好；如果训练数据比较少的话，最短词汇信息和最长词汇信息分别适用于对嵌套实体与flat实体有要求的情况。(吐槽一下，这结论有啥用？我不做实验我也知道啊🐶)</li>
</ol>
<h2 id="multi-digraph模型">Multi-digraph模型</h2>
<h3 id="提出的背景-1">提出的背景</h3>
<p>Multi-digraph模型是2019年发表在ACL上的《A Neural Multi-digraph Model for Chinese NER with Gazetteers》论文。这个模型所关注的问题word conflicts，具体来说就是：在融合词汇信息的时候，如果有多个词汇表的话，不同的词汇表对于某些话有不同的匹配，这样的话对于句子中的character的标注就会有干扰，<strong>那么怎么很好地建模character与词典之间的关系以及怎么很好地融合不同词典的词汇信息？</strong>这就是multi-digraph模型提出的背景。</p>
<h3 id="multi-digraph模型介绍">Multi-digraph模型介绍</h3>
<p>先放图～</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/multi-graph.jpg" style="zoom: 67%;"></p>
<p>整体模型主要分为两大块：embedding部分+(LSTM+CRF)部分，第二部分没什么可讲的，主要介绍一下embedding部分。</p>
<p>embeding 部分主要是根据GGNN来进行构建。怎么构建图呢？首先明确模型的输入：sentence以及实体词典(gazetteer)，那么node set就是：sentence中每一个chatacter作为node+<strong>每一种词典的每一类</strong>对应一对节点(起始与结束)，上方有四种词典，因此有四对节点。在模型图中，raw space中五颜六色的箭头就是edge。构建好图后，就可以使用GGNN来进行aggregate，从而产生更好的node representation。下面介绍一下什么是GGNN，以及为什么要选择GGNN而不是其他的GNN。</p>
<p><strong>GGNN</strong>：全称是Gated graph sequence neural networks，是一种spatial GNN，其aggregate的方式优点类似于GRU，如下：</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/ggnn.jpg"></p>
<p>其中，<span class="math inline">\(h_v^{(1)}\)</span>表示的是初始的embedding，具体的可见<a href="https://zhuanlan.zhihu.com/p/83410937" target="_blank" rel="noopener">link</a>。从aggregate更新的方式来看，就是知道GGNN相比于GCN等GNN来说，最大的优势是比较适用于序列，能够更好地提取文本信息。不过说实话，这里使用GNN有点强行上的意思，没看出来为什么一定要用GGNN。🧐</p>
<h3 id="实验结果-1">实验结果</h3>
<ol type="1">
<li><p>数据集：OntoNotes、MSRA、Weibo、E-commerce(released by this paper)</p></li>
<li><p>结果如下：</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/multi-digraph-result.jpg" style="zoom:80%;"></p>
<p>在OntoNotes和MSRA上的结果比大部分模型要好，在Weibo数据集上的结果要差一些，所以多加词汇表果然还是王道啊，不过相对的，估计也无法并行了吧。</p></li>
</ol>
<h2 id="softlexicon模型">SoftLexicon模型</h2>
<h3 id="提出的背景-2">提出的背景</h3>
<p>像LatticeLSTM模型，它的缺点就是：<strong>1.模型结构过于复杂，无法迁移到其他架构上，而且无法并行化；2.并没有完全融入lexicon的所有信息，也就是有信息损失。</strong>针对第一点，softlexicon和wclstm模型一样，通过将character所匹配到的lexicon words编码为一个fixed vector，并与character embedding进行concat，作为后续模型的输入；针对第二点，softlexicon通过categorizing 所匹配到的lexicon words，从而避免信息损失。具体的在下面小节介绍～</p>
<h3 id="softlexicon模型介绍">SoftLexicon模型介绍</h3>
<p>先放图～</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/softlexicon.jpg"></p>
<p>假设输入的sentence表示为：<span class="math inline">\(s=\{c_1,c_2,...,c_n\}\)</span>，<span class="math inline">\(w_{i,j}\)</span>表示sentence中与lexicon中相匹配的word。为了不损失任何信息，我们首先对所有的与character <span class="math inline">\(c_i\)</span>相匹配的lexicon words都采用<code>BMES</code>来进行编码，从而每一个character都有4个word sets。其公式如下： <span class="math display">\[
B(c_i)=\{w_{i,k},\forall w_{i,k}\in L,i&lt;k&lt;=n\} \\
M(c_i)=\{w_{j,k},\forall w_{j,k}\in L,1&lt;=i&lt;j&lt;k&lt;=n\} \\
E(c_i)=\{w_{j,i},\forall w_{j,i}\in L,1&lt;=j&lt;i&lt;=n\} \\
S(c_i)=\{c_i,\exists c_i\in L\}
\]</span> 其中，<span class="math inline">\(L\)</span>是lexicon,当然如果word set为空的话，那么会添加一个<code>NONE</code>进去。具体的例子如下：</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/softlexicon_example.jpg"></p>
<p>对于每一个character <span class="math inline">\(c_i\)</span>，我们都能得到word sets。得到word sets之后，我们需要对word sets进行压缩。论文里实验了两种方法：mean-pooling与weighting method，实验表明后者要更好一些。具体来说，假设我们要计算character <span class="math inline">\(c_i\)</span>的<span class="math inline">\(B(c_i)\)</span>的representation vector，我们使用每一个word的frequency作为该词的权重，之所以这样做而没有使用像attention这样的方法，是因为想要提高速度。具体公式如下： <span class="math display">\[
v^s(S)=\frac{4}{Z}\sum_{w\in S}z(w)e^w(w) \\
Z=\sum_{w\in B\cup M\cup E\cup S}z(w)
\]</span> 其中，<span class="math inline">\(S\in \{B,M,E,S\}\)</span>，<span class="math inline">\(z(w)\)</span>表示word <span class="math inline">\(w\)</span>的词频。</p>
<p>分别得到<code>BMES</code>的representation vector之后，我们需要将其与character进行concat，具体如下： <span class="math display">\[
x^c=[x^c,e^s(B,M,E,S)] \\
e^s(B,M,E,S)=[v^s(B),v^s(M),v^s(E),v^s(S)]
\]</span> 然后将<span class="math inline">\(x^c\)</span>输入到BILSTM+CRF中，得到最终的结果。</p>
<h3 id="实验结果-2">实验结果</h3>
<ol type="1">
<li><p>数据集：OntoNotes、MSRA、Weibo、resume</p></li>
<li><p>实验结果</p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/ontonotes_msra_softlexicon.jpg"></p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/weibo_softmlexicon.jpg" style="zoom:50%;"></p>
<p><img src="/2020/10/19/NLP-NER-SimpleLexicon%E6%A8%A1%E5%9E%8B/resume_softlexicon.jpg" style="zoom:50%;"></p>
<p>从结果上看，还是不错的，而且speed上有了很大的提升。</p></li>
</ol>
<h2 id="参考文献">参考文献</h2>
<p>《An Encoding Strategy Based Word-Character LSTM for Chinese NER》</p>
<p>wc-lstm code：https://github.com/liuwei1206/CCW-NER</p>
<p>《A Neural Multi-digraph Model for Chinese NER with Gazetteers》</p>
<p>multi-digraph code：https://github.com/PhantomGrapes/MultiDigraphNER</p>
<p>《Simplify the Usage of Lexicon in Chinese NER》</p>
<p>softlexicon code：https://github.com/v-mipeng/LexiconAugmentedNER</p>
<p>《GATED GRAPH SEQUENCE NEURAL NETWORKS》</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>WC-LSTM</tag>
        <tag>Multi-digraph</tag>
        <tag>SoftLexicon</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|RE-DocRED and SCIREX Dataset</title>
    <url>/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/</url>
    <content><![CDATA[<p>这篇博客主要讲解一下Document-level 关系抽取上的最新的两个数据集：DocRED以及SCIREX，paper名称：《DocRED: A Large-Scale Document-Level Relation Extraction Dataset》、《SCIREX: A Challenge Dataset for Document-Level Information Extraction》。</p>
<a id="more"></a>
<h2 id="docred-dataset">DocRED dataset</h2>
<h3 id="background">Background</h3>
<p>DocRED数据集是发表在ACL2019上的《DocRED: A Large-Scale Document-Level Relation Extraction Dataset》。关系抽取是NLP中一个非常基础且重要的任务，主要是：辨别text中实体之间的relation facts。但是在目前的relation extraction中，主要都是集中在sentence-level上，也就是提取句子内的实体，并识别实体之间的关系。但是在实际的问题当中，很多实体之间的关系需要通过多个句子来获得，所以将sentence-level的raltion extraction扩展到document-level 的relation extraction是非常有必要的。而目前已有的document-level的数据集，要么数据量不够大，要么有很多的噪音，要么构造的数据集只适用于某一个领域，这对于构建general domain下的document-level relation extraction model是不利的，在这种情况下，就有了DocRED。</p>
<h3 id="statistics-of-docred">Statistics of DocRED</h3>
<p>DocRED是根据wikipedia和wikidata构建而来，最后DocRED的各项数值如下：</p>
<p><img src="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/statistic_docred.jpg"></p>
<p>从图中可以看到，在DocRED中，平均每一个document的word大概在198.2～209.7之间，说实话，这个程度其实只能算是paragraph，离document还差的比较远，这一点SCIREX数据集弥补了(平均有5000左右)。</p>
<h3 id="baseline">Baseline</h3>
<p>paper中实验了当时SOTA的三种模型。大致的过程是：首先对于一篇doucment表示为：<span class="math inline">\(\cal D=\{w_i\}_{i=1}^{n}\)</span>，其中<span class="math inline">\(w_i\)</span>表示第<span class="math inline">\(i\)</span>个word，我们将其输入到CNN/LSTM/BILSTM当中，得到encoder后的<span class="math inline">\(\{h_i\}_{i=1}^{n}\)</span>，然后我们可以得到entities的representation，最后预测每一个entity pair的relation type。下面讲解一些details：</p>
<ul>
<li><p><strong>word features：</strong>对于sentence中的每一个word，它的feature=[GloVe_word_embedding; entity_type_embedding; coreferance_embedding]​。</p></li>
<li><p><strong>named entity mention representation：</strong>这里需要明确named entity mention与entity的区别。named entity mention指的是与entity相关的word的集合，包括指代、缩写、nickname等等。举个例子：<code>北京大学</code>是一个entity，它的named entity mention可以有：<code>北大</code>、<code>p大</code>、<code>贵校</code>、<code>你北</code> 、<code>隔壁</code>等等。那么在relation extraction中，我们要做的recognize named entity mentions。当我们identity named entity mentions后，我们怎么表示entity呢？entity <span class="math inline">\(e_i\)</span>如下： <span class="math display">\[
e_i=\frac {1}{K}\sum_{i=1}^{K}m_k \\
m_k=\frac{1}{t-s+1}\sum_{j=t}^{s}h_j
\]</span> 其中，<span class="math inline">\(K\)</span>表示entity <span class="math inline">\(e_i\)</span>所有的mentions的数量，<span class="math inline">\(m_k\)</span>表示第<span class="math inline">\(k\)</span>个mention，t与s是第<span class="math inline">\(K\)</span>个mention的开头与结尾的index。</p></li>
<li><p><strong>prediction：</strong>对于relation extraction，我们是将其视为一个多标签分类的问题，所以采用的是sigmoid函数，对于每一个entity pair <span class="math inline">\((e_i,e_j)\)</span>，具体如下： <span class="math display">\[
\tilde e_i=[e_i;E(d_{ij})], \ \tilde e_j=[e_j;E(d_{ji})] \\
P(r|e_i,e_j)=sigmoid(\tilde e_i^TW_r\tilde e_j+b_r)
\]</span> 其中，<span class="math inline">\(d_{ij}\)</span>与<span class="math inline">\(d_{ji}\)</span>表示的是两个entity在document中第一个mention之间的相对距离，<span class="math inline">\(E\)</span>是一个embedding matrix。</p></li>
</ul>
<h3 id="experiment">experiment</h3>
<p>先放结果～</p>
<p><img src="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/result_docred.jpg"></p>
<p><img src="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/human_perfance_docred.jpg" style="zoom:50%;"></p>
<ul>
<li>对于DocRED，评价指标采用的是F1和AUC，但是为了避免训练集与验证集/测试集上relation facts的overlap，所以又提出了Ign F1和Ign AUC指标，以获得更好的效果。</li>
<li>从上面的表中可以看到，目前的baseline的效果与human的performance相去甚远，还有很大的可以提高的空间。</li>
<li>在DocRED中，除了关系分类任务，还提出了另一个子任务——identity 关系实例(relation instances)和支持证据(supporting envidences)，用于进一步提高性能。</li>
<li>总的来看，DocRED是一个general domain document-level relation extraction datasets，目前已有的方法的效果与human performance相去甚远，有很大的提升空间，是一个可以做的方向。</li>
</ul>
<h3 id="data-format">Data format</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &#39;title&#39;,</span><br><span class="line">  &#39;sents&#39;:     [</span><br><span class="line">                  [word in sent 0],</span><br><span class="line">                  [word in sent 1]</span><br><span class="line">               ]</span><br><span class="line">  &#39;vertexSet&#39;: [</span><br><span class="line">                  [</span><br><span class="line">                    &#123; &#39;name&#39;: mention_name, </span><br><span class="line">                      &#39;sent_id&#39;: mention in which sentence, </span><br><span class="line">                      &#39;pos&#39;: postion of mention in a sentence, </span><br><span class="line">                      &#39;type&#39;: NER_type&#125;</span><br><span class="line">                    &#123;anthor mention&#125;</span><br><span class="line">                  ], </span><br><span class="line">                  [anthoer entity]</span><br><span class="line">                ]</span><br><span class="line">  &#39;labels&#39;:   [</span><br><span class="line">                &#123;</span><br><span class="line">                  &#39;h&#39;: idx of head entity in vertexSet,</span><br><span class="line">                  &#39;t&#39;: idx of tail entity in vertexSet,</span><br><span class="line">                  &#39;r&#39;: relation,</span><br><span class="line">                  &#39;evidence&#39;: evidence sentences&#39; id</span><br><span class="line">                &#125;</span><br><span class="line">              ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="scirex-dataset">SCIREX dataset</h2>
<h3 id="background-1">Background</h3>
<p>SCIREX数据集是发表在ACL2020上的《SCIREX: A Challenge Dataset for Document-Level Information Extraction》。它提出的主要背景与DocRED类似，目前主要的relation extraction主要都是围绕着sentence-level进行的，但是实际问题中，entity之间的relation往往要通过多个sentence 推理得到，而针对sentence-level的model无法处理这种问题，所以有必要将sentence-level扩展到document-level relation extraction/information extraction。而目前已有的document-level information extraction datasets，远远不能满足document-level IE/RE的要求，最接近的是DocRED数据集，但是这个数据集是从wikipiedia paragraph中构建的，所以它的长度远远不能达到document的要求(DocRED大约在200 word/doc)，所以就有了SICREX。</p>
<h3 id="statistics-of-scirex">Statistics of SCIREX</h3>
<p>SCIREX是根据scientific articles构建而来，具体数据如下：</p>
<p><img src="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/scirex.jpg" style="zoom:50%;"></p>
<p>从上图中可以知道，SCIREX数据集每一篇document有5737个word，比DocRED中的document长的多，可以说是真的document了；此外，SCIREX共有438篇document，虽然相对较少，但是从总的word来说，SCIREX的word数目是DocRED的2倍(human-annotated version)。</p>
<h3 id="tasks-in-sicrex">Tasks in SICREX</h3>
<p>在SCIREX数据集中，作者定义了四种任务：<strong>entity recognition、salient entity recognition、coreference、relation extraction。</strong></p>
<ul>
<li><strong>entity recognition：</strong>这个其实和DocRED中的named entity mention recognition是一样的，不过又有点不太一样。在SCIREX中，共有四种entity：Method、Task、Metric、Dataset，各自的mentions是这四种entity的各种mention(好像和没说一样🤣，这个在DocRED的介绍里有的，这里不再赘述)。总之这个任务是：识别各种entity mentions并分类到各自的entity type中。</li>
<li><strong>salient entity recognition：</strong>由于出现在document中的每一个entity mentions的重要性程度是不一样的。就比如relation work中提到的task相比于文章主体的task，重要性要小的多，所以就有必要进行salient entity recognition。</li>
<li><strong>coreference：</strong>这个任务是identity在单篇document中指向同一个entity的mentions的集合(cluster)。</li>
<li><strong>relation extraction：</strong>RE是指在单篇document中提取entities之间N元关系的任务。在SCIREX中，有2元关系、三元关系以及四元关系。但是需要注意的是，在SCIREX中，N-元关系不能被切分为多个二元关系，因为一个Datasets可能会有多个Task，一个任务可能会有多种Metric，所以metric不能仅仅根据Dataset或者Task来决定。</li>
</ul>
<h3 id="baseline-1">Baseline</h3>
<p>SCIREX中给出了一个非常强的Baseline，但是我个人觉得比较遗憾的是，这篇paper没有给出human performance，不知道up bound是怎么样的。baseline具体如下：</p>
<p><img src="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/scirex-model.jpg"></p>
<ul>
<li><p><strong>Document representation：</strong>我们假设一个输入的文档<span class="math inline">\(\cal D\)</span>表示为：<span class="math inline">\([s_1,...,s_{|S|}]\)</span>，其中<span class="math inline">\(s_i\)</span>表示第i个section(section可以理解为paragraph)，共有<span class="math inline">\(|S|\)</span>个section。document encoding分为两步：section-level and document-level。每一个section的tokens embedding通过SCIBERT来得到，每一个section-level token embedding的维度是：[tokens_nums,embedding_dim]，得到之后，我们对所有的section-level token embedding进行concat，维度是：[|S|*tokens_nums,embedding_dim]，然后输入到一个BILSTM当中，得到结果<span class="math inline">\(e_i，i\in\{1,..,|S|\}\)</span>。注意如果是batch输入的话，经过BILSTM的得到的维度是：[batch_size,|S|*token_nums,embedding_dim]。</p></li>
<li><p><strong>Mention identification and classification：</strong>在得到token embedding之后，经过BILSTM+CRF，去对mentions进行identification与classification。</p></li>
<li><p><strong>Mention representation：</strong>得到mentions之后，我们就要执行之后的<strong>salient mention classification</strong>与<strong>relation extraction</strong>任务，但是要执行这两个任务，需要先得到mention embedding。那么对于一个mention <span class="math inline">\(m_j\)</span>，它的representation是怎么样的呢？具体如下：</p>
<p>对于一个mention <span class="math inline">\(m_j\)</span>,他所包含的words表示为：<span class="math inline">\(\{w_{j_1},w_{j_2},...,w_{j_N}\}\)</span>，<span class="math inline">\(m_j\)</span>的mention embedding是： <span class="math display">\[
me_j=[e_{j_1};e_{j_N};\sum_{k=1}^{N}\alpha_{j_k}e_{j_k}]
\]</span> <span class="math inline">\(e_{j_1}\)</span>与<span class="math inline">\(e_{j_n}\)</span>分别表示该mention的first word与last word的token embedding。除此之外，还会将span在文档中的相对位置信息加入到其中，以获得更多的信息。</p></li>
<li><p><strong>Salient mention classification：</strong>这个任务就是去看每一个mention是不是salient，是一个二分类的任务。</p></li>
<li><p><strong>Pairwise coreference resolution：</strong>这个任务就是计算每一对mention pair<span class="math inline">\((m_i,m_j)\)</span>的coreference score <span class="math inline">\(c_{ij}\)</span>。</p></li>
<li><p><strong>Mention clustering：</strong>给定mention pair的集合，以及他们对应的coreference score，我们需要对它进行层次聚类，每一个cluster都表示一个entity。</p></li>
<li><p><strong>Salient entity cluster classification：</strong>这一步在上一步的mention clustering的基础上，过滤掉那些不够salient的cluster，只留下salient的cluster，从而用于最后的relation extraction task。具体怎么做呢？paper里面采用了一个比较简单的方法，就是只要这个cluster里面出现了一个salient mention，我们就认为这个cluster是salient，就保留该cluster。这一步最终的输出cluster的集合：<span class="math inline">\(C=\{C_1,..,C_L\}\)</span>，其中<span class="math inline">\(C_i\)</span>是同一个type的mention的集合。</p></li>
<li><p><strong>Relation extraction：</strong>这是最终的任务，我们需要通过上述的salient cluster，构建所有cluster的二元组和四元组，然后我们要判断这些二元组与四元组是否在document中expressed or not expressed。怎么做呢？这里以四元组为例：</p>
<p>假设对于一个四元组：<span class="math inline">\(R=(C_1,C_2,C_3,C_4)\)</span>，其中<span class="math inline">\(C_i\)</span>表示一个cluster，我们通过一个two-step的precedure来获得 R的vector：首先构建一个section embedding，然后aggregate 所有的section embedding，从而得到document embedding。对于<span class="math inline">\(\forall C_i \in R\)</span>，我们可以通过max-pooling 出现在section <span class="math inline">\(s\)</span> 中的<span class="math inline">\(C_i\)</span>中的mentions的span embedding，从而得到<span class="math inline">\(C_i\)</span>在section <span class="math inline">\(s\)</span>的section embedding <span class="math inline">\(E^s_i\)</span>。relation <span class="math inline">\(R\)</span>在section <span class="math inline">\(s\)</span>的section embedding是：<span class="math inline">\(E^s_R=FFN([E^s_1,E^s_2,E^s_3,E^s_4])\)</span>；然后relation <span class="math inline">\(R\)</span>的document embedding是：<span class="math inline">\(E_R=\frac{1}{|S|}\sum_{s=1}^{|S|}E^s_R\)</span>。最终的分类是将<span class="math inline">\(E_R\)</span>输入到另一个FFN层，然后进行分类。loss为binary CE。</p></li>
</ul>
<h3 id="experiment-1">experiment</h3>
<p><img src="/2020/10/24/NLP-RE-DocRED-and-SCIREX-Dataset/scirex_result_1.jpg"></p>
<p>左图是本文提出的模型与DYGIE++模型在各个子任务上进行对比，右图是本文提出的模型的各个子任务上的结果，可以看到，采用端到端的烦事，最终的relation extraction的结果非常低，而采用glod input的结果，要高得多，所以可以看到<strong>salient entity cluster classification</strong>是整个端到端系统的影响最大的一步。感觉还是有很大提升空间的。</p>
<h3 id="data-format-1">Data format</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;doc_id&quot; : str &#x3D; Document Id as used by Semantic Scholar,</span><br><span class="line">    &quot;words&quot; : List[str] &#x3D; List of words in the document,</span><br><span class="line">    &quot;sentences&quot; : List[Span] &#x3D; Spans indexing into words array that indicate sentences,</span><br><span class="line">    &quot;sections&quot; : List[Span] &#x3D; Spans indexing into words array that indicate sections,</span><br><span class="line">    &quot;ner&quot; : List[TypedMention] &#x3D; Typed Spans indexing into words indicating mentions ,</span><br><span class="line">    &quot;coref&quot; : Dict[EntityName, List[Span]] &#x3D; Salient Entities in the document and mentions belonging to it,</span><br><span class="line">    &quot;n_ary_relations&quot; : List[Dict[EntityType, EntityName]] &#x3D; List of Relations where each Relation is a dictionary with 5 keys (Method, Metric, Task, Material, Score),</span><br><span class="line">    &quot;method_subrelations&quot; : Dict[EntityName, List[Tuple[Span, SubEntityName]]] &#x3D; Each Methods may be subdivided into simpler submethods and Submenthods in coref array. For example, DLDL+VGG-Face is broken into two methods DLDL , VGG-Face.</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Span &#x3D; Tuple[int, int] # Inclusive start and Exclusive end index</span><br><span class="line">TypedMention &#x3D; Tuple[int, int, EntityType]</span><br><span class="line">EntityType &#x3D; Union[&quot;Method&quot;, &quot;Metric&quot;, &quot;Task&quot;, &quot;Material&quot;]</span><br><span class="line">EntityName &#x3D; str</span><br></pre></td></tr></table></figure>
<h2 id="references">References</h2>
<p>《DocRED: A Large-Scale Document-Level Relation Extraction Dataset》</p>
<p>code：https://github.com/thunlp/DocRED</p>
<p>《SCIREX: A Challenge Dataset for Document-Level Information Extraction》</p>
<p>code：https://github.com/allenai/SciREX</p>
<p>https://blog.csdn.net/qq_38556984/article/details/108835600</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>关系抽取</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>DocRED</tag>
        <tag>SCIREX</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|distill系列预训练模型</title>
    <url>/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>目前关于预训练模型的改进大概是：模型小型化(蒸馏、剪枝、量化(混合精度训练)等)、网络结构的改进、多任务学习、更大的模型(增加模型capacity、增加算力等)、跨语言预训练模型等等，具体的可以看看FDU邱锡鹏老师关于PTMs的survey。这一篇博客主要介绍distill系列的预训练模型，主要是介绍distiilBERT、tinyBERT、fastBERT。(ps：最近和大佬们交流，深刻地意识到自己太菜了。。。害，努力吧。）</p>
<a id="more"></a>
<h2 id="distillbert模型">distillBERT模型</h2>
<p>Google所发布的BERT以及后续对BERT进行改进的一系列预训练模型，虽然效果非常好，但是在infernce的速度仍然是比较慢的，无法满足生产环境中很多的场景。而hinton等人在2014年提出的知识蒸馏方法是提升速度的方法之一。而distillBERT正是将知识蒸馏方法应用于BERT模型所产生的一种预训练模型。</p>
<p>首先简单的介绍一下知识蒸馏，具体来说，我们定义两个网络：teacher model与student model。teacher model原有的预训练模型(譬如BERT)，通过teacher model得到的结果的soft label作为student model学习的一部分。student model的训练由soft label与hard label组成。</p>
<blockquote>
<p>为什么要用soft label呢？因为作者认为softlabel中包含有hard label中没有信息，也就是样本的概率信息，可以达到泛化的效果。</p>
</blockquote>
<h3 id="distillbert模型架构">distillBERT模型架构</h3>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/distillbert.jpg"></p>
<p>上面是distllBERT的结构图，整体来说与BERT非常类似，改动的地方如下：</p>
<ul>
<li>将原始的BERT-base作为teacher model，并每两层只选择一层作为student model的layer，也就是说将BERT的层数减少了一半，每一层的参数初始化为teacher model的参数，即：用teacher的第2层初始化student的第1层，teacher的第4层初始化student的第2层；</li>
<li>去掉了token type embedding和pooler，也就是去掉了segment embedding；</li>
<li>使用与RoBERTa类似的动态masking的方法，并且去掉了NSP loss。</li>
</ul>
<p>另外，关于distillBERT的预训练阶段的loss，共分为3部分，这里需要好好提一下：</p>
<p><strong><span class="math inline">\(L_{ce}\)</span> loss</strong></p>
<p>所谓的<span class="math inline">\(L_{ce}\)</span> loss指的是：teacher网络softmax层输出的概率分布和student网络softmax层输出的概率分布的交叉熵（注：MLM任务的输出）。公式如下：</p>
<blockquote>
<p>这里有点问题，在代码里使用的KL divergence，而不是CE。。。就按代码来吧。。。</p>
</blockquote>
<p><span class="math display">\[
softmax-temperature:p_i=\frac {exp(z_i/T)}{\sum_{j}exp(z_j/T)}
\]</span></p>
<p><span class="math display">\[
L_{ce}=-\sum_{i}t_ilog(s_i)
\]</span></p>
<p>其中，<span class="math inline">\(t_i、s_i\)</span>分别表示teacher model与student model的soft label，他们的计算公式是<span class="math inline">\(softmax-temperature\)</span>。</p>
<p><strong><span class="math inline">\(L_{mlm}\)</span> loss</strong></p>
<p><span class="math inline">\(L_{mlm}\)</span>损失就是BERT当中所说的MLM loss，注意：这个是student model本身的mlm。</p>
<p><strong><span class="math inline">\(L_{cos}\)</span> loss</strong></p>
<p>计算teacher hidden state和student hidden state的余弦相似度。官方代码用的是：nn.CosineEmbeddingLoss。</p>
<p><strong>我们最终的loss是三种loss的线性相加，来对student model进行pre-training。</strong></p>
<h3 id="结果">结果</h3>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/distillbert%20result.jpg"></p>
<p>从结果来看，distillBERT的表现与BERT接近，保留了97%的效果，参数量减少了40%，速度快出71%，可以说非常的优秀了。</p>
<h2 id="tinybert模型">tinyBERT模型</h2>
<p>tinyBERT是另外一种使用知识蒸馏方法的预训练模型。它提出的背景是：transformer layer中的attention机制能够提取出非常丰富的语言知识，但是在之前的distillBERT中并没有将这些语言知识让student model给学习到。所以tinyBERT提出了一个两段式的知识蒸馏的框架，来解决这个问题。</p>
<h3 id="tinybert模型架构">tinyBERT模型架构</h3>
<p>首先直接放tinyBERT的结构图，如下：</p>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/tinyBERT.jpg"></p>
<p>a图是tinyBERT的架构图，主要是对四个部分进行蒸馏，如下：</p>
<ul>
<li><strong>embedding-layer distillation：</strong>在tinyBERT模型中，因为我们想要希望模型能够学到与teacher model相似甚至是相同的语义，所以我们采用MSE损失函数来对进行蒸馏。具体公式是：假设teacher model的embedding <span class="math inline">\(E^S\)</span>的维度是<span class="math inline">\(d\)</span>，tinyBERT的embedding <span class="math inline">\(E^T\)</span>维度是<span class="math inline">\(d&#39;\)</span>，并且<span class="math inline">\(d&#39;&lt;d\)</span>，loss formation 如下：</li>
</ul>
<p><span class="math display">\[
{\cal L_{emd}}=MSE(E^SW_e,E^T)
\]</span></p>
<ul>
<li><strong>Transformer-layer distillation：</strong>对于transformer layer来说，蒸馏的有两部分：attention loss与hidden loss。attention loss主要是希望tinyBERT中的multi-head self attention得到的scores能够接近teacher model的scores，因为有研究表明attention能够学到很多语义知识，包括语法等等。公式如下：</li>
</ul>
<p><span class="math display">\[
{\cal L_{attn}}=\frac {1}{h}\sum_{i=1}^{h}MSE(A_{i}^{S},A_{i}^{T})
\]</span></p>
<p>其中，h表示head的数目。</p>
<p>hidden loss指的是希望通过一个transformer layer得到的输出结果尽可能的靠近teacher model的结果，具体是b图。公式是： <span class="math display">\[
{\cal L_{hide}}=MSE(H^SW_h,H^T)
\]</span></p>
<ul>
<li><strong>predcition-layer distillation：</strong>这是采用了交叉熵。公式如下：</li>
</ul>
<p><span class="math display">\[
{\cal L_{pred}}=-softmax(z^T)log\_softmax(z^S/t)
\]</span></p>
<p>在tinyBERT中，我们是首先在预训练阶段，使用embedding-layer distillation与transformer-layer distillation，在finetune阶段使用prediction-layer distillation。另外，需要讲解的是：tinyBERT是如何对层数进行蒸馏的？</p>
<p>在distllBERT中，我们是每两层选择一层，但是在tinyBERT中并不是这么的简单粗暴。假设teacher model有N层，我们的student model由M层，怎么进行映射呢？具体是：<span class="math inline">\(0=g(0)，N+1=g(M+1)，n=g(m)\)</span>。也就说，embedding层对应于teacher model的embedding层，prediction layer对对应于teacher model的prediction layer，对于中间的 Transformer 层，TinyBERT 采用 k 层蒸馏的方法，即 g(m) = m × N / M。例如 TinyBERT 有 4 层 Transformer，BERT 有 12 层 Transformer，则 TinyBERT 第 1 层 Transformer 学习的是 BERT 的第 3 层；而TinyBERT 第 2 层学习 BERT 的第 6 层。</p>
<h3 id="结果-1">结果</h3>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/tinyBERT%20result.jpg"></p>
<p>可以看到，在蒸馏成同样的层数的时候，tinyBERT的结果要比distillBERT要好很多，在某些数据集上甚至可以与BERT持平，还是非常非常优秀的。然后作者又做了很多消融分析，具体结果如下：</p>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/tinyBERT%20result2.jpg"></p>
<p>从结果上来说，所有的蒸馏objective都是有用的，尤其是对transfomer layer的蒸馏，当我们去掉transformer layer的时候，结果下降的非常厉害，所以，对于transformer layer的蒸馏是关键。</p>
<h2 id="fastbert模型">fastBERT模型</h2>
<p>fastBERT是今年在ACL上发布的一个新的预训练模型，个人觉得在减小模型参数与加速inference速度上是非常优雅的，值得一读。它整体的创新有两个：样本自适应机制与自蒸馏方法。具体的，下面一一介绍～</p>
<h3 id="fastbert模型结构">fastBERT模型结构</h3>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/fastBERT.jpg"></p>
<p>整体架构还是很好懂的，主要分为backbone与branch两部分，如下：</p>
<ul>
<li>backbone：与BERT-base一样，12层的transformer，在最后加了一层teacher classifier。</li>
<li>branch：branch使用的是classifier，注意这里的classifier是根据teacher classifier蒸馏来的。论文称为自蒸馏方法。即：在pre-training与fine-tuning的时候，我们都只去更新backbone的参数，之后再frozenbackbone的参数，用branch classifier来蒸馏backbone中的classifier的概率分布。</li>
</ul>
<p>整体模型的训练与inference的过程如下：</p>
<ul>
<li>pre-training：直接采用BERT、ROBERTa等模型就可以；</li>
<li>fine-tuning for backbone：给BERT加上最后一层的classifier，用标注的来进行训练，这与传统的BERT的finetuning是一样的；</li>
<li>self-distillation for branch：这里使用无标签数据就可以了。我们将backbone的分布蒸馏给branch classfier。这里使用KL散度来衡量两者概率分布，和distillBERT一样；最后的loss就是所有branch的KL散度之和；</li>
<li>Adaptive inference：根据branch的分类器的结果来对样本进行过滤，简单的样本越早给出结果，越难的样本越晚给出结果。在论文里，作者基于熵给出了不确定性度量，来衡量样本的不确定程度。公式如下：</li>
</ul>
<p><span class="math display">\[
Uncertainty=\frac {\sum_{i=1}^{N}p_s(i)logp_s(i)}{log(1/N)}
\]</span></p>
<p>在每一层，作者都是用不确定性来衡量样本是否被舍弃，样本的不确定性如果在每一层低于阈值，然后就被立即丢弃，否则的话，就保留进入下一层；另外作者做实验验证了：低不确定性的样本被正确分类的概率高，高不确定性的样本被正确分类的概率低，所以即便在第一层就被舍弃了，但是被错误分类的概率很小。<strong>这个机制叫做样本自适应机制。</strong>这样做的一个好处就是：如果我们想要大幅提高inference的速度，那么我们就可以将阈值设置的大一点，这样的话，大部分的样本的结果在早期就被输出了。</p>
<h3 id="结果-2">结果</h3>
<p><img src="/2020/07/18/NLP-distill%E7%B3%BB%E5%88%97%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/fastbert-result.jpg"></p>
<p>从结果来看，fastBERT要比distilBERT要好，其实我觉得它应该和tinybert这样的模型比较，distilbert算是很早的模型，总觉得结果不太solid。另外fastBERT模型目前只适用于分类任务，对于其他的序列标注等任务都不太适用，不过我觉得已经够了，现实中很多任务都可以被当作分类任务来做，还是有很大的应用价值的。</p>
<h2 id="mobilebert模型">MobileBERT模型</h2>
<p>mobile-BERT来源于《MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices》论文。我个人觉得整体模型并不优雅</p>
<h3 id="mobilebert模型架构">MobileBERT模型架构</h3>
<p>pass</p>
<h3 id="结果-3">结果</h3>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>distillBERT</tag>
        <tag>tinyBERT</tag>
        <tag>fastBERT</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|word2vec/GloVe/fastText模型原理详解与实战</title>
    <url>/2020/02/29/NLP-word2vec-GloVe-fastText%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>词向量是NLP中最为重要的概念。词向量的好坏直接影响到下游任务的效果。然而，即便在ELMo、BERT、ALBERT等预训练模型大行其道的当今，word2vec、GloVe、fastText仍然是目前最为流行的词向量训练方式。因此，本篇博客将具体讲解这三种词向量训练的原理，并使用gensim来展示如何使用这些词向量模型，以便得到我们想要的词向量。</p>
<a id="more"></a>
<blockquote>
<p>注意：本篇博客尽可能讲解清楚三种词向量训练模型的原理，但是很多细节的部分，需要大家自行去了解，比如huffman树的原理、softmax的原理等等。如若不尽意，可参考我在参考文献中例举的原始论文～</p>
</blockquote>
<h2 id="word2vec模型介绍">word2vec模型介绍</h2>
<p>word2vec模型其中有两种训练方式：skip-gram与CBOW，此外，还有两种加速训练的trick：hierarchical sofmtmax与negative sampling。所以，word2vec其实有4种方法。细节见<a href="https://zhuanlan.zhihu.com/p/42080556" target="_blank" rel="noopener">细节</a>、<a href="https://zhuanlan.zhihu.com/p/27234078" target="_blank" rel="noopener">skip-gram详细介绍</a></p>
<h3 id="skip-gram模型">skip-gram模型</h3>
<p>skip-gram模型是给定目标词，来预测其上下文。由于skip-gram模型是一个简单的神经网络模型。我们知道使用神经网络来训练的过程分为：</p>
<ul>
<li>确定训练数据<span class="math inline">\((X,Y)\)</span></li>
<li>确定网络架构</li>
<li>确定损失函数</li>
<li>确定优化器、迭代次数</li>
<li>存储网络</li>
</ul>
<p><strong>确定训练数据</strong></p>
<p>假设语料库只有一句话：<font face="Times New Roman"><em>The quick brown fox jumps over the lazy dog</em></font>。所以总共只有8个词。在skip-gram中，训练数据的形式其实非常简单，其实就是<strong>使用n-gram的方法生成词对。</strong>譬如</p>
<p><img src="/2020/02/29/NLP-word2vec-GloVe-fastText%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/0.png" style="zoom:80%;"></p>
<p>上图的窗口大小是2，左右各取2个词。实际上窗口大小为5时比较好的，这里只是为了展示。在得到训练数据后，我们将其用one-hot编码后，就可以输入网络了。譬如：(the, quick)单词对就表示<span class="math inline">\([(1,0,0,0,0,0,0,0),(0,1,0,0,0,0,0,0)]\)</span>，输入网络。当然，<strong>对target单词也是需要进行抽样的。</strong>链接：<a href="https://www.yuque.com/liwenju/kadtqt/gqkk0e" target="_blank" rel="noopener">对target抽样</a></p>
<p><strong>确定网络架构</strong></p>
<p><img src="/2020/02/29/NLP-word2vec-GloVe-fastText%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/1.jpg" style="zoom: 67%;"></p>
<p>上面就是skip-gram模型的架构，输入是一个单词的vector，输出是softmax后的概率分布，<strong>每一个概率值对应词汇表中的一个单词，表示词汇表中的单词出现在输入单词的上下文的概率。</strong>中间只有一个线性的hidden layer。还是紧接着我们的例子，词汇表有8个单词，所以，输入时8维的vector，输出也应该是一个8维的vector。而hidden layer的维度则是我们想要得到的词向量的维度，比如说我们使用3个hidden units，那么，权重矩阵就是8*3维的矩阵。(实际上google推荐使用300维。)注意：<strong>在网络中，其实有两个权重矩阵，我们只需要选择其中一个就可以了，因为它们互为转置。</strong>我们最后需要的额就是权重矩阵，并不需要输出层的结果。</p>
<p><strong>确定损失函数</strong></p>
<p>由于在输出层使用softmax函数，那么很自然地，损失函数就是交叉熵函数。即： <span class="math display">\[
L=\sum_{i=1}^{V}y_ilog(\hat y_i),\\
\hat y_i=\frac {e^{(w^Th_i)}}{\sum_{i=1}^{V}e^{(w^Th_i)}},\\
其中，V表示词汇表的数目
\]</span> 我们可以发现，使用sotfmax，需要计算词汇表中所有词汇的和，而一般的词汇表数目在几十万到几百万不止，那么，这个计算代价是非常昂贵的，所以，我们需要使用一些技巧来加速训练。</p>
<h3 id="negative-sampling">Negative sampling</h3>
<p>Negative sampling主要就是解决网络难以训练的问题。在skip-gram模型中，我们使用SGD来训练网络，这需要计算词汇表中所有单词的和，当词汇表很大的时候，这是非常难的。那么negative sampling，就是在在输出层中，我们抽取几个应该为0的维度，再加上为1的维度，来进行训练。这样就能大大加快训练速度。</p>
<p>譬如说，我们的词汇表为10000，hidden layer的维度300，我们nagative sampling的维度个数为5个，那么我们需要更新的维度就是6个，所以我们需要更新的权重系数为300*6=1800，这是输出层系数的0.06%。注意：<strong>在隐藏层的权重10000*300=3000000是必须需要训练的。</strong>这样一来，<strong>我们就将softmax问题转换为V个logistic二分类问题，并且我们每次只更新其中K个负样本与一个正样本的参数，从而大大降低了计算成本。</strong></p>
<p>但是问题来了：<strong>我们应该怎么抽取这5个应该为0的维度呢？</strong>具体做法是：根据单词在语料库中出现的次数来决定。出现次数越多，那么约有可能被抽中。公司如下： <span class="math display">\[
P(w_i)=\frac{f(w_i)^{\frac 34}}{\sum_{i=0}^{V}f(w_i)^{\frac34}}
\]</span> 其中，<span class="math inline">\(P(w_i)\)</span>表示单词<span class="math inline">\(w_i\)</span>被抽中的概率；<span class="math inline">\(f(w_i)\)</span>表示单词<span class="math inline">\(w_i\)</span>在语料库中出现的次数。</p>
<h3 id="hierarchical-softmax">hierarchical softmax</h3>
<p>所谓的hierarchical softmax，实际上就是采用Huffman树。huffman树的叶子结点就是词汇表中的单词，从根节点到叶子结点的路径就表示单词的概率。这样一样，我们就不需要对所有单词的得分进行求和，就大大降低了计算成本。注意：<strong>在构造huffman树的时候，常用的词的深度会更小一些，即更靠近根节点；而不常用的词会更深一点。</strong></p>
<h2 id="cbow模型">CBOW模型</h2>
<p>CBOW模型与skip-gram的训练方式相反。它是给定目标词上下文，然后来预测目标词。</p>
<p><img src="/2020/02/29/NLP-word2vec-GloVe-fastText%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/2.jpg" style="zoom: 67%;"></p>
<p>其中网络架构与skip-gram模型非常的类似。但是需要注意输入。CBOW模型的输入是多个单词的one-hot vector的和，而隐藏层需要对其求平均。如下： <span class="math display">\[
h=\frac 1CW^T(x_1+x_2+...+x_C)
\]</span> 其中，<span class="math inline">\(x_1,x_2,..,x_C\)</span>是输入的vector，<span class="math inline">\(h\)</span>是隐藏层的输出，<span class="math inline">\(C\)</span>是输入的单词的数目。</p>
<p>其余与skip-gram模型一样，输出层也是使用了softmax。此外，negative sampling与hierarchical softmax这两种trick也都可以用在CBOW模型中。</p>
<h2 id="glove模型">GloVe模型</h2>
<p>GloVe模型出自于另一篇论文《Global Vectors for Word Representation》。<strong>强力推荐大家去读一读，这是一篇非常好懂的论文！🤩</strong></p>
<p>GloVe模型的目标是：<strong>得到单词的词向量，让其尽可能的包含语义与语法信息</strong>。输入是语料库(没错，不需要去构建训练集)，输出是词向量。GloVe模型的思路是：<strong>从语料库中统计共现矩阵，然后根据共现矩阵与GloVe模型来学习词向量。</strong></p>
<p><strong>统计共现矩阵</strong></p>
<p>我们记共现矩阵为：<span class="math inline">\(X=[X_{ij}]_{N\times N}\)</span>。其中，<span class="math inline">\(X_{ij}\)</span>表示单词<span class="math inline">\(j\)</span>在单词<span class="math inline">\(i\)</span>的上下文中出现的次数，<span class="math inline">\(N\)</span>表示词汇表中的单词数目。至于如何构建的，其实就是根据统计窗口，遍历整个语料库，来进行统计。接下来，我们再引入一些符号： <span class="math display">\[
X_i=\sum_{j=1}^{N}X_{ij},\\
P_{ij}=\frac {X_{ij}}{X_i},\\
ratio_{i,j,k}=\frac{P_{ik}}{P_{jk}}
\]</span> 其中，<span class="math inline">\(P_{ij}\)</span>表示单词<span class="math inline">\(j\)</span>在单词<span class="math inline">\(i\)</span>的上下文出现的概率，<span class="math inline">\(ratio_{i,j,k}\)</span>表示单词<span class="math inline">\(i\)</span>与单词<span class="math inline">\(k\)</span>的相关度，以及单词<span class="math inline">\(j\)</span>与单词<span class="math inline">\(k\)</span>的相关度的比值。为什么要有这个东西？因为，如果我们直接看<span class="math inline">\(P_{ij}\)</span>来表示单词<span class="math inline">\(i\)</span>与单词<span class="math inline">\(j\)</span>的之间的相关度的话，其实是不够的，因为这很有可能受语料库的影响。所以，需要需要它们之间的比值。我们可以发现<span class="math inline">\(ratio_{i,j,k}\)</span>有如下性质：</p>
<ul>
<li>当<span class="math inline">\(i\)</span>与<span class="math inline">\(k\)</span>相关，<span class="math inline">\(j\)</span>与<span class="math inline">\(k\)</span>不相关的时候，那么<span class="math inline">\(ratio_{i,j,k}\)</span>的值会非常的大；</li>
<li>当<span class="math inline">\(i\)</span>与<span class="math inline">\(k\)</span>相关，<span class="math inline">\(j\)</span>与<span class="math inline">\(k\)</span>相关的时候，那么<span class="math inline">\(ratio_{i,j,k}\)</span>的值会趋于1；</li>
<li>当<span class="math inline">\(i\)</span>与<span class="math inline">\(k\)</span>不相关，<span class="math inline">\(j\)</span>与<span class="math inline">\(k\)</span>相关的时候，那么<span class="math inline">\(ratio_{i,j,k}\)</span>的值会非常的小；</li>
<li>当<span class="math inline">\(i\)</span>与<span class="math inline">\(k\)</span>不相关，<span class="math inline">\(j\)</span>与<span class="math inline">\(k\)</span>不相关的时候，那么<span class="math inline">\(ratio_{i.j.k}\)</span>的值会趋于1。</li>
</ul>
<p>那么，GloVe模型的想法是：假设我们现在以及得到了单词的词向量<span class="math inline">\(v_i,v_j,v_k\)</span>，那么，如果说我们通过某个函数，能够求得<span class="math inline">\(ratio_{i,j,k}\)</span>的话，那么就说我们的词向量与共现矩阵是一致的，也就是说我们的的词向量包含了共现矩阵中的信息。</p>
<p><strong>学习词向量</strong></p>
<p>假设我们的函数为：<span class="math inline">\(g(v_i,v_j,v_k)\)</span>，那么，我们的目标就是：<span class="math inline">\(g(v_i,v_j,v_k)=\frac {P_{ik}}{P_{jk}}\)</span>。所以，很自然地，我们可以想到： <span class="math display">\[
min\sum_{i,j,k}(g(v_i,v_j,v_k)-\frac {P_{ik}}{P_{jk}})^2
\]</span> 只要让这个式子最小，我们就能求出所有的词向量。但是，这个式子存在一个问题：那就是这个计算复杂度太高了，为：<span class="math inline">\(O(N\times N\times N)\)</span>。所以，我们必须想办法减小复杂度。</p>
<p>由于我们想衡量<span class="math inline">\(v_i\)</span>与<span class="math inline">\(v_j\)</span>的相似度，那么，我们引进<span class="math inline">\(v_i-v_j\)</span>这个是很自然的；此外，由于<span class="math inline">\(ratio_{i,j,k}\)</span>是一个标量，那么我们引入内积也是很自然的，<span class="math inline">\((v_i-v_j)^Tv_k\)</span>，最后我们在外面加一层<span class="math inline">\(exp\)</span>，从而能够简化运算。所以我们的函数可以变为如下式子： <span class="math display">\[
g(v_i,v_j,v_k)=exp((v_i-v_j)^Tv_k)
\]</span> 由于我们的目标是：<span class="math inline">\(g(v_i,v_j,v_k)=\frac {P_{ik}}{P_{jk}}\)</span>，将g函数带入，得到： <span class="math display">\[
\frac {P_{ik}}{P_{jk}}=\frac{exp(v_i^Tv_k)}{exp(v_j^Tv_k)}
\]</span> 所以，我们只要令：<span class="math inline">\(P_{ik}=exp(v_i^Tv_k)，P_{jk}=exp(v_j^Tv_k)\)</span>，即可。我们再将这两个式子统一一下，如下： <span class="math display">\[
P_{ij}=exp(v_i^Tv_j),\\
化简，\frac {X_{ij}}{X_i}=exp(v_i^Tv_j),\\
再化简，logX_{ij}-logX_i=v_i^Tv_j
\]</span> 我们可以看看最后的化简的式子，右边是对称的：<span class="math inline">\(v_i^Tv_j=v_j^Tv_i\)</span>；而左边是不对称的。所以，我们需要再化简一下： <span class="math display">\[
logX_{ij}=v_i^Tv_j+b_i+b_j
\]</span> 所以，我们的最优化式子可以化简如下： <span class="math display">\[
min\sum_{i,j}(v_i^Tv_j+b_i+b_j-logX_{ij})^2
\]</span> 此外，我们还想让出现频率高的词对有更高的权重，所以，我们需要再加一个权重项，最终如下： <span class="math display">\[
min\sum_{i,j}f(X_{ij})(v_i^Tv_j+b_i+b_j-logX_{ij})^2
\]</span> 其中，权重项函数为： <span class="math display">\[
f(x)=\begin{cases}
(\frac{x}{x_{max}})^{0.75}&amp; x&lt;x_{max}\\
1&amp; x&gt;=x_{max}
\end{cases}
\]</span> 以上就是GloVe模型的全部内容～</p>
<p><strong>GloVe模型与word2vec的对比：</strong></p>
<ul>
<li>GloVe模型最大的优点是利用了全局的信息，而word2vec中，尤其是skip-gram，只利用了目标词周边的一小部分上下文。尤其当引入negative sampling训练的时候，丧失了词与词之间的关系信息。</li>
<li>GloVe模型能够加快模型训练速度，并且能够控制词的相对权重。</li>
</ul>
<h2 id="fasttext模型">fastText模型</h2>
<p>fastText是Facebook在2016年所提出的方法。其实，整个模型架构并没有特别创新的地方，和CBOW模型非常地像。其创新的地方在于：<strong>子词嵌入的引入。</strong>（论文写的太简洁了，导致看了很久😫）</p>
<p><strong>模型架构</strong></p>
<p><img src="/2020/02/29/NLP-word2vec-GloVe-fastText%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/3.jpg" style="zoom: 67%;"></p>
<p>整个模型架构与CBOW模型非常的类似。输入是一段序列，输出是这段序列的标签类别。中间层仍然是线性的。输出层仍然是softmax。并且在训练的时候，使用hierarchical softmax来加速训练。</p>
<p><strong>子词嵌入</strong></p>
<p>这个才是fasttext模型真正创新的地方。在word2vec或者其他的模型中，相似形态的单词由不同的向量来表示。譬如，<code>老师们，老师</code>。但是，<code>老师们、老师</code>这两个单词，意思其实非常的相近，不应该被编码成不同的向量。所以，在fasttext中，引入了<strong>子词嵌入</strong>。具体做法是：我们将Apples，使用trigram，得到：<code>&lt;ap,app,ppl,ple,les,es&gt;</code>，在训练过程中，每一个n-gram都会由一个向量来表示，我们可以用这6个trigram的叠加来表示<code>Apples</code>这个单词。<strong>序列中所有单词的向量以及n-gram向量同时相加平均，作为训练的输入。</strong>譬如：一段序列中3个词，<span class="math inline">\(w_1,w_2,w_3\)</span>表示3个词的向量，<span class="math inline">\(w_{12},w_{23}\)</span>表示bigram特征的向量。那么hidden layer的输出是：<span class="math inline">\(h=\frac 15W^T(w_1+w_2+w_3+w_{12}+w_{23})\)</span>。</p>
<p><img src="/2020/02/29/NLP-word2vec-GloVe-fastText%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/4.jpg"></p>
<p>这里有一个问题需要注意：<strong>在实际中，我们往往会使用多种n-gram，所以得到的n-gram结果比单词数目多得多。</strong>所以来存储这么多的n-gram是不太现实的。所以，我们的做法是：将所有的n-gram映射到一张hash桶中，这样的话，就能够实现n-gram的vector共享。</p>
<p><strong>子词嵌入的好处：对低频词汇的词向量训练效果会更好，因为它可以和其他词共享n-gram；此外，对于UNK，我们仍然可以构建它们的字符级n-gram向量。</strong></p>
<p>理论部分就到这里，我们再回顾一下：fastText到底快在哪里？</p>
<ul>
<li>使用了hierarchical softmax，这本来就是对普通的softmax很大的加速了；</li>
<li>在hierarchical softmax中，使用Huffman树来构建，对于文本分类而言，其分类树远远小于skip-gram模型中的词表数目，所以，相比于skip-gram模型，更加快速；</li>
<li>一些trick，譬如使用提前算好exp的值。（在使用logisitic进行二分类的时候使用。）</li>
</ul>
<h2 id="实战">实战</h2>
<p>在实战部分，我主要是使用gensim。当然，如果有兴趣的小伙伴们，可以去看它们的源码，这个在github上都能够搜的到。</p>
<p><strong>fasttext</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br><span class="line">sentences=[[<span class="string">"你"</span>, <span class="string">"是"</span>, <span class="string">"谁"</span>], [<span class="string">"我"</span>, <span class="string">"是"</span>, <span class="string">"中国人"</span>]]</span><br><span class="line"><span class="comment">#训练主函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#sentences表示str类型的list的集合</span></span><br><span class="line"><span class="comment">#size表示词向量的维度</span></span><br><span class="line"><span class="comment">#window表示当前词与预测词之间的距离</span></span><br><span class="line"><span class="comment">#min_count表示低于这个数字的单词的频数，那么将被删除掉</span></span><br><span class="line"><span class="comment">#iter表示epoch</span></span><br><span class="line"><span class="comment">#min_n表示字符级的n-gram的最小的n</span></span><br><span class="line"><span class="comment">#max_n表示字符级的n-gram的最大的n</span></span><br><span class="line"><span class="comment">#word_gram：当为1的时候，会用到字符级的n-gram；为0，则不会用到字符级的n-gram，等同于word2vec。</span></span><br><span class="line">model = FastText(sentences,  size=<span class="number">4</span>, window=<span class="number">3</span>, min_count=<span class="number">1</span>, </span><br><span class="line">                 iter=<span class="number">10</span>,min_n = <span class="number">3</span> , max_n = <span class="number">6</span>,word_ngrams =<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获得词向量</span></span><br><span class="line">model.wv[<span class="string">"你"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型的保存与加载</span></span><br><span class="line">model.save(<span class="string">"fname"</span>)</span><br><span class="line">model=FastText.load(<span class="string">"fname"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意，这个只能单向保存为word2vec，无法通过load加载</span></span><br><span class="line">model.wv.save_word2vec_format(<span class="string">'test_fasttext.txt'</span>, binary=<span class="literal">False</span>)</span><br><span class="line">model.wv.save_word2vec_format(<span class="string">'test_fasttext.bin'</span>, binary=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在线更新语料库</span></span><br><span class="line"><span class="comment"># 在线更新训练 fasttext</span></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> FastText</span><br><span class="line">sentences_1 = [[<span class="string">"cat"</span>, <span class="string">"say"</span>, <span class="string">"meow"</span>], [<span class="string">"dog"</span>, <span class="string">"say"</span>, <span class="string">"woof"</span>],[<span class="string">"dude"</span>, <span class="string">"say"</span>, <span class="string">"wazzup!"</span>]]</span><br><span class="line">sentences_2 = []</span><br><span class="line"></span><br><span class="line">model = FastText(min_count=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#build.vocab表示建立字典，其中需要corpus作为参数,当update参数为true的时候，那么就更新字典</span></span><br><span class="line">model.build_vocab(sentences_1)</span><br><span class="line"><span class="comment">#train表示训练fasttext模型</span></span><br><span class="line"><span class="comment">#参数：</span></span><br><span class="line"><span class="comment">#total_examples表示字典中的使用的corpus中的句子数目一致</span></span><br><span class="line"><span class="comment">#epochs表示迭代次数</span></span><br><span class="line"><span class="comment">#total_examples=model.corpus_count, epochs=model.epochs是标准写法</span></span><br><span class="line">model.train(sentences_1, total_examples=model.corpus_count, epochs=model.epochs)</span><br><span class="line"></span><br><span class="line"><span class="comment">#model.build_vocab(sentences_2,update=True)</span></span><br><span class="line"><span class="comment">#model.train(sentences_2, total_examples=model.corpus_count, epochs=model.iter)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#获取词向量字典，两种方式</span></span><br><span class="line">model.wv.vocab</span><br><span class="line">model.wv.index2word</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取与目标词相似的词</span></span><br><span class="line">model.wv.most_similar(positive=[<span class="string">'你'</span>, <span class="string">'是'</span>], negative=[<span class="string">'中国人'</span>])</span><br><span class="line"><span class="comment">#model.wv.most_similar_cosmul(positive=['你', '是'], negative=['中国人'])</span></span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<p>1 《word2vec Parameter Learning Explained》，Xin Rong（强推， 比原始论文详细的多！）</p>
<p>2 《GloVe: Global Vectors for Word Representation》</p>
<p>3 《Bag of Tricks for Efﬁcient Text Classiﬁcation》</p>
<p>4 https://www.yuque.com/liwenju/kadtqt/ensofi</p>
<p>5 http://albertxiebnu.github.io/fasttext/</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>word2vec</tag>
        <tag>skip-gram</tag>
        <tag>CBOW</tag>
        <tag>GloVe</tag>
        <tag>fastText</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|跨语言预训练模型</title>
    <url>/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>这篇文章主要是来总结一下目前在跨语言预训练模型方面的工作，当然啦，今年ACL有很多改进的工作，之后会再写一篇。这篇主要讲解的模型是：Multingual-BERT、XLM、XLM-R、MASS、MultiFiT。</p>
<a id="more"></a>
<h2 id="multilingual-bertm-bert">Multilingual-BERT(m-BERT)</h2>
<h3 id="提出背景">提出背景</h3>
<p>m-BERT来源于《How multilingual is Multilingual BERT?》论文。主要是想探索：多语言BERT到底学到了什么？以及多语言BERT在零样本迁移学习上的表现如何？主要思路是将m-BERT在多语言上进行预训练，然后使用一个语言进行fine-tune，并在另一种语言上进行评估，fine-tune+eval在NER与POS两个任务进行了实验，最后并做了一些probing实验来探究m-BERT的跨语言迁移学习效果以及其表征能力。</p>
<h3 id="实验思路">实验思路</h3>
<p>m-BERT仍然是由12层的transformer layer组成，但是它的训练语料是104种语言的维基百科页面数据，并且所有语言共享一个词汇表。注意：多语言BERT在训练的时候，既没有使用任何输入数据的语言标注，也没有使用任何翻译机制来计算对应语言的表示。最后作者是在NER与POS两个任务上进行实验，这两个任务都是先在一种语言上进行微调，然后在另一种语言上进行评估，从而来探究m-BERT在跨语言迁移学习上的效果。具体结果如下：</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/m-BERT%20result1.jpg"></p>
<p>从结果上来看，fine-tune与eval阶段所使用的语言越相似，效果越好；结果最好的情况是fine-tune与eval使用同一种语言。</p>
<p>除了上述实验外，作者还探究了lexical overlap对结果的影响。所谓的lexical overlap，指的是：由于在m-BERT当中，所有语言共享一个词汇表，当fine-tune阶段出现了评估阶段所使用的语言的单词后，就会产生一种跨语言的迁移。作者设计了一些实验来探究了跨语言迁移学习的效果是有多大程度依赖于这个lexical overlap；以及当没有这种lexical overlap的时候，跨语言迁移学习的效果。</p>
<p>在NER任务中，为了测量lexical overlap的影响，定义了<span class="math inline">\(E_{train}\)</span>与<span class="math inline">\(E_{eval}\)</span>来分别表示训练集与测试集上的实体的集合，计算overlap的公式是： <span class="math display">\[
overlap=|E_{train}\bigcap E_{eval}|/|E_{train}\bigcup E_{eval}|
\]</span> <img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/overlap.jpg" style="zoom:67%;"></p>
<p>从上述结果中我们可以看到，英文BERT的效果非常依赖于overlap，迁移学习的效果会随着overlap的下降而下降；而对于多语言BERT而言，overlap对于其效果的影响并不大，说明多语言BERT在不同的文本上具有良好的泛化能力。</p>
<p>另外，作者还做了一些实验来探究多语言BERT在多语言文本混合（Code-switching）和音译（transliteration）的情况下表征能力。具体来说，作者进一步在UD语料库上测试了印地语（HI）和英语（EN）。多语言文本混合是指一个表达里面参杂多种语言，而音译则指将发音相似的外来词语直接通过读音翻译过来，比如酷 （cool）和迪斯科（disco）等。</p>
<h2 id="xlm">XLM</h2>
<h3 id="提出背景-1">提出背景</h3>
<p>模型来源于《Cross-lingual Language Model Pretraining》论文。它的提出背景是：向BERT、XLNet等模型都是在单一语言上进行预训练和fine-tune，换一种语言就需要重新进行预训练+fine-tune，那能不能使用多种语言同时训练一个模型，之后通过一个模型就可以在多个语言上进行下游的任务呢？这就是XLM模型。具体做法是：提出了只使用单种语言的无监督学习方法与使用跨语言的平行语料的有监督学习方法，从而在跨语言分类、监督翻译与无监督翻译中取得了SOTA的效果。</p>
<h3 id="实验思路-1">实验思路</h3>
<p>作者主要提出了三种loss fucntion：CLM、MLM、TLM。</p>
<ul>
<li>CLM：无监督单语单向LM训练任务，使用Transformer在给定前序词语的情况下预测下一个词的概率，实际上是一个transformer语言模型。</li>
<li>MLM：无监督单语双向LM训练任务，这个与BERT的MLM基本上一样的， 不同的地方在于：BERT中，输入的是句子对，并且我们是去随机MASK掉15%的token，使用的时候，80%使用 [MASK] token 替换，10%用随机 token 替换，最后10%不变；而在XLM中，使用的是由任意数量的句子组成的文本流代替成对的句子(最长为256个token)。为了均衡稀有tokens和高频tokens，借鉴来word2vec中对高频词采样的方法：文本流中的tokens是以多项式分布进行采样的，并且其权重与它们的逆文本频率的平方根成正比。另外，去掉了segment embedding，从而换用了language embedding。</li>
<li>TLM：有监督翻译LM训练任务，这个任务主要是为跨语言任务所准备的。TLM是对MLM的一种扩展。具体来说，我们将并行的翻译句子拼接在一起，然后对两个句子进行mask操作。譬如我们要做翻译的任务，那么我们通过TLM，就可以引导source与target句子的表征进行对齐。另外，这里也是去掉了segment embedding，换用了language embedding。</li>
</ul>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/XLM.jpg"></p>
<p>然后我们一般使用CLM或者MLM来进行单语言的预训练，然后根据下游任务来决定是否要加TLM。论文里主要是在跨语言分类(XNLI数据集)、有监督翻译与无监督翻译任务上进行实验。结果如下：</p>
<p><strong>XNLI</strong></p>
<p>作者是先用MLM在各个语言的单语语料上进行训练，然后再用英文训练集进行finetune，最后在多个语种上评估。结果如下：</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/XLM%20result1.jpg"></p>
<p><strong>无监督机翻</strong></p>
<p>一般的套路是：DAE+back translation。DAE指的是：譬如我们要进行英译中这样一个任务，我们搭建一个encoder-decoder框架，将英文数据+噪声输入到encoder中，然后decoder出原始的英文数据，中文也可以这样做；back translation指的是：我们可以将英文通过encoder-decoder，得翻译的中文，然后将翻译后的中文+噪声再通过encoder-decoder，还原成原始的英文。在XLM中，使用CLM/MLM来初始化encoder与decoder部分，然后按照DAE+back translation的框架来训练即可，结果如下：</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/UMT.jpg" style="zoom:50%;"></p>
<p><strong>有监督机翻</strong></p>
<p>这个就是普通的翻译，没什么可说的，结果如下：</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/MT.jpg" style="zoom:50%;"></p>
<p>论文另外还探究来模型在low-resource language上的结果以及无监督多语言情况下生成的embedding的情况。</p>
<h2 id="xlm-r">XLM-R</h2>
<p>XLM-R来源于《Unsupervised Cross-lingual Representation Learning at Scale》论文，是对XLM模型的拓展，主要是借鉴了RoBERTa模型的套路(大力出奇迹？)。具体而言就是，XLM-R模型在100种语言上使用超过2.5T的数据进行训练。相对于XLM，XLM-R使用了过滤后的CommonCrawl数据集，最终XLM-R在XNLI、MLQA、multilingual-NER这些多语言任务上均取得了最好的实验效果。主要的改进有以下几点：</p>
<ul>
<li>在模型部分，整体与XLM基本一样，与XLM模型不同的地方在于：去掉了language embedding；</li>
<li>XLM-R扩展到100种语言，与XLM-100不同的在于：在消融分析中使用了七种不同的语言：英语、法语、德语、俄语、中文、斯瓦希里语、乌尔多语，这样的话就可以将high-resource language与low-resource language都包含进来，可以比较正确的评价模型。</li>
<li>使用更多的数据，这与RoBERTa类似，更多的数据能够提升最终的效果。具体来说，对于low-resource language，我们去增加它的数目(类似于上采样？)。</li>
</ul>
<p>最后就是一些实验结果，没什么太多可说的。</p>
<h2 id="mass">MASS</h2>
<p>MASS来源于《MASS: Masked Sequence to Sequence Pre-training for Language Generation》论文。MASS模型提出的背景是：目前像BERT这样的预训练模型在各种NLU任务中表现的都已经很好了，但是在NLG任务上，却一直表现不佳，而且由于NLG任务标注数据较少，所以更加需要进行pre-train。在seq2seq架构中，一般的想法是：对encoder与decoder都进行pre-train，但是这样导致的问题就在于：这样训练得到的encoder与decoder的分布有可能会不一样，因为就是硬生生地将两个语言模型揉在一起。<strong>所以：要怎么对encoder与decoder都进行pre-train，同时还能保证它们的分布是一致的呢？</strong>这就有了MASS。</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/MASS.jpg"></p>
<p>上面是MASS的架构，它整体的训练过程如下：</p>
<ul>
<li><strong>encoder</strong>：输入是被随机mask掉之后的句子，但是与BERT不同的地方在于：MASS中是随机mask掉一段连续的token片段，而不是随机mask，这样做的好处在于：迫使模型从未被mask掉的词中提取更多的语义信息，从而用于后续decoder部分的预测。在实验中，mask的最佳比例在50%左右。</li>
<li><strong>decoder</strong>：输入是与encoder同样的句子，但是mask掉的正好和encoder相反，和翻译一样，迫使decoder通过encoder来得到的语义信息，强化attention部分，另外，只预测encoder部分别mask掉的词，能够加快训练。</li>
</ul>
<p>MASS模型的loss function就是log-似然函数，这个没太多可讲的。</p>
<p>另外，值得一提的是：作者对k(mask掉的token的长度)进行了探究，并将BERT与GPT归结于MASS模型的特例。具体来说，当k为1的时候，encoder部分就只mask掉一个token，decoder部分mask掉所有token，来预测encoder中被mask掉的token，从这个角度来看的话，encoder就像是BERT中的backbone，而decoder就像是classifier；当k等于句子长度的时候，encoder部分我们全部mask掉，decoder部分我们都不mask，这样的话，encoder部分没有输入，decoder部分就相当于GPT。架构图如下：</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/MASS-bert-GPT.jpg"></p>
<h2 id="multifit">MultiFiT</h2>
<p>MultiFiT来源于《MultiFiT: Efﬁcient Multi-lingual Language Model Fine-tuning》论文。它提出的背景是：目前大多数预训练模型都是在high source语言上进行的，譬如：英语、汉语等等，但是在那些low source语言上表现不好。所以这篇文章就提出了一个MultiFiT模型，能够在low source语言上取得较好的结果。</p>
<blockquote>
<p>这里说的high source与low source是以标注数据的多少来判断的，如果标注数据比较多，那么我们就是它是high source语言，如果标注的数据比较少，甚至没有，我们就说它是low source语言。</p>
</blockquote>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/Multifit.jpg" style="zoom:50%;"></p>
<p>上面是MultifiT模型的架构图，总共是：1个subword embedidng layer+4个QRNN layer+1个aggregation layer+2个linear layer。这是有监督的情形，对于无监督的情形，也就是没有标注数据，论文中也提出了框架，如下：</p>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/multifit-unsu.jpg" style="zoom:50%;"></p>
<p>具体步骤如下：</p>
<ul>
<li>预训练：首先在目标语言语料中进行单语预料的预训练；</li>
<li>fine-tune：使用目标语言的任务预料进行fine-tune，其实就是使用任务语料再一次进行了预训练；</li>
<li>使用跨语言分类器(图中的是LASER)，来对目标任务语料进行预测，并使用预测标签来进行单语的训练，相当于知识蒸馏。</li>
</ul>
<p><img src="/2020/07/24/NLP-%E8%B7%A8%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/multifit-result.jpg"></p>
<p>最后是在MLDoc与CLS数据集上进行了实验，可以看到，不管是在有监督的情形下还是无监督的情形下，MultiFiT模型取的结果都要比之前的结果要好。</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multilingual-BERT</tag>
        <tag>XLM</tag>
        <tag>XLM-R</tag>
        <tag>MASS</tag>
        <tag>MultiFiT</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|Bahdanau Attention与Luong Attention</title>
    <url>/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/</url>
    <content><![CDATA[<p>目前为止，已经学了很多东西，但是没有输出，总感觉似乎少了点什么。这片博客将回顾经典的Attention机制。Attention模型是深度学习领域最有影响力的工作之一，最初应用于图像领域(hard attention)，后来在NMT任务上取得巨大成功后，便开始风靡于整个深度学习社区，尤其是在NLP领域。随后提出的GPT、ELMo、transformer、BERT、GPT-2、XLNET等模型，均有Attention机制的影子。本文将详细讲解两种经典的Attention模型：Bahdanau Attention与Luong Attention，并对Attention模型进行一个小小的总结。</p>
<a id="more"></a>
<h2 id="为什么要有attention机制">为什么要有Attention机制？</h2>
<p>真正的Attention机制源于神经机器翻译（NMT）。在NMT中，最常用的模型架构是：encoder-decoder。我们将输入序列输入到encoder部分，encoder最终输出一个固定长度的vector，从而作为decoder部分的输入，最终，decoder部分输出翻译后的结果。encoder-decoder架构如下图所示。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/0.encoder-decoder.png"></p>
<p>但是上述的encoder-decoder架构存在缺陷：<strong>输入序列不论长短，都会被编码成一个固定长度的vector，作为decoder的输入，而decoder的效果会受制于该vector</strong>。也就是说，上述架构要求当前状态记录到目前为止所有的信息，但是我们无法通过一个vector来保存所有序列的信息。这会限制整个模型的性能。尤其当源输入序列非常长的时候，模型的性能会下降地越迅速。在机器翻译任务上，就表现为：当输入序列非常长的时候，翻译的文本质量会非常差。这就引进了Attention模型。</p>
<p>传统的Attention模型有两种。一个是Dzmitry Bahdanau等人在《Neural Machine Translation by Jointly Learning to Align and Translate》论文中提出的Attention模型，简称：Bahdanau Attention；另一个是Minh-Thang Luong等人在《Effective Approaches to Attention-based Neural Machine Translation》论文中提出的Attention模型，简称：Luong Attention。下面将分别详细地介绍两种Attention模型。</p>
<h2 id="bahdanau-attention">Bahdanau Attention</h2>
<p>首先给出Attention模型的架构图，如下所示。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/1.Attention机制.png"></p>
<p>整个模型的计算，可分为以下几步：</p>
<ul>
<li>encoding</li>
<li>计算attention weights</li>
<li>计算context vector</li>
<li>decoding</li>
</ul>
<p>下面将详细地介绍每一个步骤。</p>
<h3 id="encoding">Encoding</h3>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/3.图4.png"></p>
<ol type="1">
<li>输入序列：<span class="math inline">\(X=\{x_1,x_2,x_3,...,x_{T_X}\}\)</span>，其中<span class="math inline">\(T_X\)</span>表示输入序列的长度。</li>
<li>在encoder部分，使用BRNN。<span class="math display">\[\overrightarrow h_1,\overrightarrow h_2,\overrightarrow h_3,...,\overrightarrow h_{T_X}\]</span>，表示前向RNN的每一时间步的hidden state，亦可以叫做激活值；<span class="math display">\[\overleftarrow h_1,\overleftarrow h_2,\overleftarrow h_3,...,\overleftarrow h_{T_x}\]</span>，表示反向RNN的每一时间步的hidden state，亦可以叫做激活值。(不懂BRNN的话，建议直接看论文或者Andrew Ng的《Deep Learning》。)</li>
<li>使用<span class="math inline">\(h_j=(\overrightarrow h_j,\overleftarrow h_j)\)</span>表示encoder部分中j时刻的hidden state。</li>
</ol>
<h3 id="计算attention-weights">计算attention weights</h3>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/4.图5.png"></p>
<ol type="1">
<li>Attention weights <span class="math inline">\(\alpha_{t,j}\)</span>的含义是：在生成第t个输出的时候，应该在第j个encoder单元的注意力大小。Attention weights的计算公式如下：</li>
</ol>
<p><span class="math display">\[
\alpha_{t,j}=\frac {exp(e_{t,j})}{\sum_{k=1}^{T_x}exp(e_{t,k})},
\]</span></p>
<p><span class="math display">\[
e_{t,j}=a(s_{t-1},h_j)
\]</span></p>
<p>这实际上就是通过softmax得到的attention的分布。</p>
<ol start="2" type="1">
<li>需要着重理解的是对齐模型(alignment model)。在这个公式中：<span class="math inline">\(e_{t,j}=a(s_{t-1},h_j)\)</span>，a是对齐模型，用来评估当前预测词与输入序列中每一个输入词的相关度。直观地理解就是：在decoder的时候，我们更加关注于那些与预测词相关的部分。在生成一个预测词的时候，我们会考虑输入序列中每一个输入词与当前预测词的对齐关系（相关程度），对齐越好的词，我们应该给予它更大的权重，因为它对当前预测词的影响越大。</li>
</ol>
<h3 id="计算context-vector">计算context vector</h3>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/5.图6.png"></p>
<p>Context vector的计算非常简单，就是所有encoder部分的hidden state的加权平均，计算公式如下： <span class="math display">\[
c_t=\sum_{j=1}^{T_x}\alpha_{t,j}h_{j}
\]</span> 其中，<span class="math inline">\(\alpha_{t,j}\)</span>就是Attention weights，<span class="math inline">\(h_j\)</span>就是在encoder部分，计算的每一个时刻的hidden state。从这个式子可以知道，<span class="math inline">\(\alpha_{t,j}\)</span> 决定了<span class="math inline">\(h_j\)</span>对<span class="math inline">\(c_t\)</span>的影响。当<span class="math inline">\(\alpha_{t,j}\)</span> 越大，那么<span class="math inline">\(h_j\)</span>对对<span class="math inline">\(c_t\)</span>的影响就越大。</p>
<h3 id="decoding">Decoding</h3>
<p>通过上述几步，我们现在已经拥有的东西有：<span class="math inline">\(h_j\)</span>、<span class="math inline">\(c_t\)</span>、<span class="math inline">\(s_{t-1}\)</span>、<span class="math inline">\(\hat y_{t-1}\)</span>。接下来，我们就来计算t时刻的hidden state，并得到输出<span class="math inline">\(\hat y_{t}\)</span>。计算公式如下：</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/6.图7.png"> <span class="math display">\[
s_{t}=g(s_{t-1},\hat y_{t-1},c_t)
\]</span></p>
<p><span class="math display">\[
p(\hat y_{t}|y_1,y_2,y_3,...,X)=f(\hat y_t,s_t,c_t)
\]</span></p>
<p>其中，g可以是GRU或者LSTM单元等，f可以是softmax单元等。</p>
<p>当然，在预测阶段，需要使用Beam Search，从而来得到最接近原意的翻译，在这，就不展开讨论了，等有时间，再写写关于Beam Search的文章～</p>
<p>至此，Bahdanau Attention的原理就讲解完毕了，其实回顾一下，整个过程一点都不复杂。</p>
<h2 id="luong-attention">Luong Attention</h2>
<p>Luong等人提出了新的更为简单和有效的Attention模型。整个模型又分为两个模型：global attention与local attention。global attention与local attention的本质区别在于：global attention使用整个输入序列的hidden state来计算context vector，而local attention只使用输入序列的hidden state的一个子集来计算context vector。下面将具体阐述这两种模型。</p>
<h3 id="global-attention">Global Attention</h3>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/7.global%20attention概览.jpg"></p>
<p>Global Attention实际上可以看作Bahdanau Attention的简化版本，但是又有些区别。具体步骤下面将一一介绍。</p>
<p>具体步骤有：</p>
<ul>
<li>encoding</li>
<li>计算decoder的hidden state</li>
<li>计算对齐向量，其实就是注意力权重</li>
<li>计算context vector</li>
<li>计算decoder每一个时刻最终的hidden state</li>
<li>计算输出</li>
</ul>
<h4 id="encoding-1">Encoding</h4>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/8.gobal%201步.jpg"></p>
<ol type="1">
<li>输入序列：<span class="math inline">\(X=\{x_1,x_2,x_3,...,x_{T_X}\}\)</span>，其中<span class="math inline">\(T_X\)</span>表示输入序列的长度。</li>
<li>计算encoder每一个时刻的hidden state，计算公式如下：</li>
</ol>
<p><span class="math display">\[
h_t=RNN_{enc}(h_{t-1},x_{t}).
\]</span></p>
<p>其中，<span class="math inline">\(h_{t-1}\)</span>表示上一个时刻的hidden state，<span class="math inline">\(x_t\)</span>表示t时刻的输入词的word embedding。</p>
<h4 id="计算decoder的hidden-state">计算decoder的hidden state</h4>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/9.2步.jpg"></p>
<p>计算公式如下： <span class="math display">\[
s_t=RNN_{dec}(s_{t-1},\hat y_{t-1})
\]</span> 其中，<span class="math inline">\(s_t\)</span>表示decoder中t时刻的hidden state，<span class="math inline">\(s_{t-1}\)</span>表示decoder中t-1时刻的hidden state，<span class="math inline">\(\hat y_{t-1}\)</span>表示decoder中t-1时刻的输出词的word embedding。</p>
<h4 id="计算对齐向量其实就是注意力权重">计算对齐向量，其实就是注意力权重</h4>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/10.jpg"></p>
<p>计算公式如下： <span class="math display">\[
\alpha_{t,j}=\frac{exp(e_{t,j})}{\sum_{k=1}^{T_X}exp(e_{t,k})},
\]</span></p>
<p><span class="math display">\[
e_{t,j}=score(s_t,h_j).
\]</span></p>
其中，<span class="math inline">\(\alpha_{t,j}\)</span>是对齐向量，也就是注意力权重，表示生成预测词<span class="math inline">\(\hat y_{t}\)</span>，需要在<span class="math inline">\(h_j\)</span>上需要花费的注意力大小；<span class="math inline">\(e_{t,j}\)</span>表示一个分数，通过score函数得到。score函数有三种方案（general方案效果最好），如下所示： $$ score(s_t,h_j)={
<span class="math display">\[\begin{array}{rcl}
{s_t}^{T}h_j,       &amp;      &amp; dot\\
{s_t}^{T}W_ah_j,     &amp;      &amp; general\\
W_a[s_t,h_j],     &amp;      &amp; concat

\end{array}\]</span>
<p>. $$ 最后，我们会详细讲解score函数是如何工作的。</p>
<p>####计算context vector</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/10.jpg"></p>
<p>context vector的计算公式如下： <span class="math display">\[
c_t=\sum_{j=1}^{T_X}\alpha_{t,j}h_{j},
\]</span> 其中，<span class="math inline">\(\alpha_{t,j}\)</span>表示对齐向量，也就是注意力权重，<span class="math inline">\(h_j\)</span>表示encoder中j时刻的hidden state。</p>
<h4 id="计算decoder中每一个时刻最终的hidden-state">计算decoder中每一个时刻最终的hidden state</h4>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/11.jpg"></p>
<p>将context vector 和 decoder的hidden states 串起来。计算公式如下： <span class="math display">\[
\hat s_t=tanh(W_c[c_t,s_t])
\]</span></p>
<h4 id="计算输出">计算输出</h4>
<p>context vector和decoder的hidden state合起来通过一系列非线性转换以及softmax最后计算出概率。计算公式如下： <span class="math display">\[
p(\hat y_{t}|\hat y_1,\hat y_2,...,\hat. y_{t-1},X)=softmax(W_s\hat s_t)
\]</span></p>
<p>在下一个时刻，将<span class="math inline">\(s_t\)</span>、<span class="math inline">\(\hat y_t\)</span>作为第二个节点的输入，循环上述过程。在论文中，还将<span class="math inline">\(\hat s_t\)</span>作为第二个节点的输入，用来辅助当前节点的对齐决策过程。过程如下图所示。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/12.jpg"></p>
<p>关于global attention，还有一个没有讲的知识点，就是score函数是如何工作的。下面以dot与general为例，进行说明。</p>
<p><strong>dot</strong></p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/dot.jpg"></p>
<p>过程：</p>
<blockquote>
<p>注意，下面描述部分可能与前面公式不符合，是因为前面的公式的表示是用加权平均和来表示，而下面描述是用矩阵相乘来描述的，所以是没有问题的！</p>
</blockquote>
<p>输入分为两部分：1.encoder中所有的hidden state H，维度是: (hid dim1 ,sequence length)；2.decoder中在同一时刻的hidden state s，其维度是: (hid dim1,1)。</p>
<p>第一步：转置H，再与s点乘，得到维度为(sequence,1)的分数；</p>
<p>第二步：对分数进行softmax，得到和为1的<strong>权重（对齐向量）</strong>；</p>
<p>第三步：将H与第二步得到的分数相乘，得到(dim1,1)的context vector。</p>
<p><strong>general</strong></p>
<p><img src="/2020/02/17/NLP%EF%BD%9CBahdanau-Attention%E4%B8%8ELuong-Attention/general.jpg"></p>
<p>过程：</p>
<blockquote>
<p>注意，下面描述部分可能与前面公式不符合，是因为前面的公式的表示是用加权平均和来表示，而下面描述是用矩阵相乘来描述的，所以是没有问题的！</p>
</blockquote>
<p>输入分为两部分：1.encoder中所有的hidden state H，维度是: (hid dim1 ,sequence length)；2.decoder中在同一时刻的hidden state s，其维度是: (hid dim2,1)。<strong>二者维度并不一样！</strong></p>
<p>第一步：转置H为(sequence length,hid dim1)，再与<span class="math inline">\(W_a\)</span>相乘，<span class="math inline">\(W_a\)</span>的维度：(hid dim1,hid dim2)，最后再与<span class="math inline">\(s\)</span>做点乘，得到一个(sequence,1)的分数；</p>
<p>第二步：对分数进行softmax，得到和为1的<strong>权重（对齐向量）</strong>；</p>
<p>第三步：将H与第二步得到的分数做点乘，得到(hid dim2,1)的<strong>context vevtor。</strong></p>
<h3 id="local-attention">Local attention</h3>
<p>local attention是soft attention与hard attention的一种折衷方案。其在计算context vector的时候，每次只考虑encoder中所有hidden state的一个子集。由于local attention与global attention十分类似，下面将着重介绍其与global attention不同的地方，相同部分就省略了。</p>
<p>local attention在计算context vector的时候，只考虑encoder中所有hidden state的一个子集。那么如何找到这个子集呢？local attention的做法如下：</p>
<p>local attention针对t时刻的输出，生成一个他在源输入序列中的对齐位置<span class="math inline">\(p_t\)</span>，接着在源输入序列中选取一个窗口：<span class="math inline">\([p_t-D,p_t+D]\)</span>，其中<span class="math inline">\(D\)</span>根据经验给出。context vector的计算则根据窗口内所有的hidden state的加权平均得到。</p>
<p>那么如何得到这个对齐位置<span class="math inline">\(p_t\)</span>呢？local attention给出了两种方法，如下：</p>
<ol type="1">
<li>单一对齐：即简单粗暴地设置<span class="math inline">\(p_t=t\)</span>，对齐向量与context vector的计算公式，与前面的公式一样：<span class="math inline">\(\alpha_{t,j}=\frac {exp(e_{t,j})}{\sum_{k={p_t-D}}^{p_t+D}exp(e_{t,k})}\)</span>，<span class="math inline">\(c_t=\sum_{j=p_t-D}^{p_t+D}\alpha_{t,j}h_j\)</span>。</li>
<li>预测对齐：针对每个预测词，预测其在源输入序列中的对齐位置。公式如下：</li>
</ol>
<p><span class="math display">\[
p_t=S·sigmoid(v_p^Ttanh(W_ph_t))
\]</span></p>
<p><span class="math display">\[
\alpha_{t,j}=\frac {exp(e_{t,j})}{\sum_{k={p_t-D}}^{p_t+D}exp(e_{t,k})}exp(-\frac{(j-p_t)^2}{2\sigma^2})
\]</span></p>
<p>其中，<span class="math inline">\(S\)</span>表示源输入序列的长度，所以<span class="math inline">\(p_t\)</span>的大小在区间<span class="math inline">\([0,S]\)</span>之间。</p>
<p><strong>Luong attention到此就讲完了，总结一下：</strong></p>
<ol type="1">
<li><p>luong attention是bahdanau attention的简化版本，而且计算代价也更小。global attention与bandanau attention非常的类似，只是计算路径不同，global attention的计算路径是: <span class="math inline">\(s_t-&gt;\alpha_t(s)-&gt;c_t-&gt;\hat s_{t}\)</span>;bahdanau attention的计算路径是：<span class="math inline">\(s_{t-1}-&gt;\alpha_{t}(s)-&gt;c_t-&gt;s_{t}\)</span>。</p></li>
<li><p>local attention是soft attention与hard attention的折衷方案，与global 不同的地方在于：计算context vector的时候，只考虑encoder中所有hidden state的子集，其余部分一样。</p></li>
</ol>
<h2 id="attention机制总结">Attention机制总结</h2>
<p>Soft Attention：指的是在求注意力权重的时候，对于源输入序列中的每一个词都给予权重；</p>
<p>Hard Attention：直接从输入句子里面找到某个特定的单词，然后把目标句子单词和这个单词对齐，而其它输入句子中的单词硬性地认为对齐概率为0。</p>
<p>Global attention属于soft attention；</p>
<p>local Attention严格来说也属于soft attention，但是一般看作是soft和hard的一种结合；</p>
<p>还有 attention over attention、self -attention等等。</p>
<p>关于attention的应用非常多，之后有机会还会再写写关于Attention的文章～</p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>《Neural Machine Translation by Jointly Learning to Align and Translate》</li>
<li>《Effective Approaches to Attention-based Neural Machine Translation》</li>
<li>https://medium.com/<span class="citation" data-cites="shashank7.iitd/understanding-attention-mechanism-35ff53fc328e">@shashank7.iitd/understanding-attention-mechanism-35ff53fc328e</span></li>
<li>https://zhuanlan.zhihu.com/p/40920384</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>深度学习</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Summarization of Interview Problems and Solutions</title>
    <url>/2020/03/07/Summarization-of-Interview-Problems-Solutions/</url>
    <content><![CDATA[<p>This blog is aimed at collecting interview problems and solutions through nowcoder,books and so on. So, I will update this article usually. Best luck for me and everyone to find a satisfactory intern!🤩</p>
<a id="more"></a>
<h2 id="machine-learning">Machine Learning</h2>
<p><strong>ByteDance AI LAB</strong></p>
<ul>
<li>CRF讲一下？(原理与推导)</li>
</ul>
<p>答：看自己写的关于CRF的那篇博文。从HMM-&gt;MEMM-&gt;CRF。</p>
<ul>
<li>ROC曲线？AUC的计算？</li>
</ul>
<p>答：</p>
<ul>
<li>bagging vs boosting?</li>
</ul>
<p>答：1. bagging叫做重采样，譬如说，给定一个含有m个样本的训练集，我们从中随机抽取一个样本，然后再放回，依次循环，直到得到T个含有m个元素的训练集，然后，我们就在这T个训练集中训练T个模型，最后在畸形模型融合。bagging降低的是方差。值得注意的是，在RF中，我们种子进行决策树分裂的时候，我们不是从所有属性中选取最优属性，而是从一个属性子集中选取最优属性，这也是为了降低各个弱分类器之间的相关性。重采样也是为了降低各个弱分类器之间的相关性。</p>
<p>​ 2. boosting是提升方法。它的思路是每次上一次的残差作为下一次的输入。它关注的是降低偏差。因为，各个弱分类器之间具有强相关性。</p>
<ul>
<li>HMM讲一下？原理与推导</li>
<li>GBDT VS XGBoost</li>
</ul>
<ol type="1">
<li><p>GBDT的一种算法，而XGBOOst是对GBDT的一种很好的工程化的实现。</p></li>
<li><p>在以CART为基本分类器的时候，XGBoost中加入了正则化项，控制模型的复杂度，防止模型出现过拟合，而GBDT则没有。</p></li>
<li><p>XGboost中对cost function使用二阶的泰勒展开，能够同时使用一阶与二阶信息，而GBDT值使用一阶信息。</p></li>
<li><p>XGBoost中，增加了对缺失值的处理，GBDT中没有。</p></li>
<li><p>XGBoost中支持多种基分类器，譬如：gbtree、dart、linear，而GBDT只采用CART。</p></li>
</ol>
<p>解答：https://blog.csdn.net/qq_28031525/article/details/70207918</p>
<h2 id="natural-language-processing">Natural Language Processing</h2>
<p><strong>ByteDance AI LAB</strong></p>
<ul>
<li>单向双向 BERT 与BiLSTM 有什么不同？其实问的就是ELMo与BERT的区别。</li>
<li>如何解决梯度消失弥散 ？</li>
</ul>
<p>答：在一般的NN中，我们主要是在随机初始化参数的时候，需要留意一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w=np.random.randn(shape)*np.sqrt(<span class="number">1</span>/n)</span><br></pre></td></tr></table></figure>
<p>这个叫做Xavier初始化。其中，n表示前一层的神经元的数目。当然还可以替换激活函数，譬如使用relu。还有BN，正则化等等。</p>
<p>在RNN中，使用LSTM或者GRU单元来代替普通的RNN单元，解决梯度消失问题。对于梯度爆炸问题，我们使用梯度裁剪来解决。具体来说，就是当梯度超过某个阈值的时候，我们对齐进行归一化操作，使其活在某个区间范围内。在tf中，使用tf.clip_by_norm().</p>
<ul>
<li>(重要)wordembedding有哪些？（发展史以及word2vec的两种训练方法与两种加速方法）</li>
<li>transformer讲一下？（KQV position）</li>
<li>交叉熵loss公式？</li>
<li>pytorch的代码流程（我是这样回答的：预处理数据/词表-写好模型-定义损失和优化器-训练-测试）</li>
<li>Dropout的原理？</li>
<li>sgd与adam的区别？</li>
<li>L1L2正则化？</li>
</ul>
<p>答：L0正则，表示是不是0的参数的个数，公式：<span class="math inline">\(\sum_{j=1,w_j\not=0}^{m}w_j^0\)</span>；L1正则，表示所有参数的一范数的和，公式：<span class="math inline">\(\sum_{j=1}^{m}|w_j|\)</span>；L2张正则，表示所有参数的二范数的平方，公式：<span class="math inline">\(\sum_{j=1}^{m}|m_j|^2\)</span>。这些正则项实际上都是到原点的马式距离(<span class="math inline">\(L(x_i,x_j)=\sum_{k=1}^{m}{(\sqrt {(x_i^{(k)}-x_j^{(k)})^2})}^p\)</span>)。<a href="https://zhuanlan.zhihu.com/p/22505062" target="_blank" rel="noopener">正则项</a></p>
<p>L1为什么比L2更容易获得稀疏解？（可以从梯度的角度来看）<a href="https://www.zhihu.com/question/37096933?sort=created" target="_blank" rel="noopener">l1正则化</a></p>
<p><img src="/2020/03/07/Summarization-of-Interview-Problems-Solutions/l1l2.jpg"></p>
<ul>
<li>讲一下BERT？</li>
<li>SoftMax + CrossEntropy的反向梯度求导</li>
<li>sentence embedding vs docment embedding?</li>
<li>Tf-idf?</li>
<li>LR中为什么使用CE，而不是MSE？</li>
</ul>
<p>答：如果使用MSE的话，很容易出现梯度消失问题。<a href="https://www.jianshu.com/p/5d13bcd9d990" target="_blank" rel="noopener">LR中的MSE与CE选择问题</a></p>
<ul>
<li>Sigmoid，Tanh，Relu等激活函数的优缺点</li>
</ul>
<p>答：<strong>sigmoid函数的优点：</strong>1.将数据压缩到（0，1）范围内；2.sigmoid函数是光滑的，也就是它处处可导；3. sigmoid函数的值域为（0，1）,那么可以被作为概率，很好的解释模型；<strong>sigmoid函数的缺点：</strong>1.容易发生梯度消失问题；2.不是关于原点中心对称，所以模型收敛速度会比较慢。<a href="https://liam.page/2018/04/17/zero-centered-active-function/" target="_blank" rel="noopener">中心对称</a>；3. 计算量大。</p>
<p><strong>tanh函数的优点：</strong>解决了sigmoid函数不是中心对称的问题；<strong>缺点：</strong>仍然存在梯度消失的问题。</p>
<p><strong>relu函数的优点：</strong>导数稳定，计算量小；<strong>relu函数的缺点：</strong>容易发生dead relu的情况。也就是当输入小于0的时候，经过relu之后，输出为0，这样的倒数也会为0，梯度很难传递。</p>
<p><strong>leaky relu：</strong>解决了dead relu问题。但是参数是人为指定的。</p>
<p><strong>pRELU：</strong>让参数也跟着网络一块训练，得到的参数更加合理。</p>
<p><a href="https://blog.csdn.net/qq_18310041/article/details/91042085" target="_blank" rel="noopener">激活函数的比较</a></p>
<ul>
<li>讲讲subword算法？</li>
</ul>
<p>答：BPE、wordpiece。</p>
<ul>
<li>label smoothing？</li>
</ul>
<p>答：LSR通常用于softmax中。在普通的softmax中，只会关注正确标签位置的损失，而不去考虑其他位置的损失，这就就会导致模型过于关注增大正确标签位置的概率，而不去关于减小预测错误标签的概率，这样的结果，就是模型在训练集上拟合的很好，而在测试集上表现的不好，也就是会导致过拟合。</p>
<p>而在LSR中，我们引入一个平滑因子 <span class="math inline">\(\epsilon\)</span>，那么 <span class="math display">\[ \hat y=(1-\epsilon)*y+\epsilon*u \]</span>，其中 <span class="math inline">\(u\)</span> 表示一个均匀分布。实际上LSR就是引入噪声。从而，让模型不仅考虑在标签正确位置的损失，也考虑在标签错误位置的损失，让导致模型的损失增大，从而导致模型的学习能力提高。也就是说，要降低损失，就必须学习的更好，也就是迫使模型向增大正确分类的概率以及减小错误分类的概率的方向前进。<strong>LSR在transformer中有使用。</strong><a href="https://blog.csdn.net/sinat_36618660/article/details/100166957?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">LSR的介绍</a></p>
<ul>
<li>Xavier初始化和He初始化</li>
</ul>
<p>答：<a href="https://blog.csdn.net/xxy0118/article/details/84333635" target="_blank" rel="noopener">Xavier初始化和He初始化</a></p>
<ul>
<li>L1优化方法？（坐标下降法、LARS角回归）</li>
</ul>
<p>答：<a href="https://blog.csdn.net/u013802188/article/details/40476989?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">坐标轴下降法</a></p>
<ul>
<li>SVM算法核函数的选择</li>
</ul>
<p>答：1.当没有任何先验知识的时候，选择RBF核函数；2.当特征数远远大于样本数的时候，选择线性核，因为此时，如果选择高斯核的话，我们会将特征映射到更加复杂的空间中，会导致模型容易过拟合；3.当样本数远远大于特征数的时候，选择线性核；当样本数一般大小，特征数较小的时候，选择高斯核。</p>
<ul>
<li>如何·处理不平衡问题？</li>
</ul>
<p>答：1。上采样与下采样。<strong>上采样：</strong>对少样本的类别进行采样，将采样后的样本加入数据集。重复多次，从而得到数据分布更加平衡的数据集。<strong>下采样：</strong>对多样本的类别进行采样，将采样后的样本移出数据集，重复多次，从而得到数据分布更加均匀的数据集。</p>
<p>2.修改权重 3。bagging；4. 多任务联合学习。5.对于二分类问题来说，激情问题转化为异常检测。<a href="https://blog.csdn.net/weixin_37801695/article/details/86243998" target="_blank" rel="noopener">答案</a></p>
<p>Skip-gram的loss？</p>
<p><img src="/2020/03/07/Summarization-of-Interview-Problems-Solutions/skip-gram-loss.jpg" style="zoom:50%;"></p>
<h2 id="mathematics">Mathematics</h2>
<p><strong>ByteDance AI LAB</strong></p>
<ul>
<li>x, y是独立的随机变量，方差期望已知，那么如何求 xy 的方差？</li>
</ul>
<p><span class="math inline">\(D(XY)=E[(XY)^2]-[E(XY)]^2=E(X^2)E(Y^2)-[E(X)E(Y)]^2\)</span></p>
<p>常见的公式：</p>
<ol type="1">
<li><span class="math inline">\(D(X)=E(X^2)-[E(X)]^2\)</span></li>
<li><span class="math inline">\(如果X与Y独立，那么 D(X+Y)=D(X)+D(Y)\)</span></li>
<li><span class="math inline">\(如果X与Y独立，那么 E(XY)=E(X)E(Y)\)</span></li>
<li><span class="math inline">\(方差的定义：D(X)=E[[X-E(X)]^2]\)</span></li>
<li><span class="math inline">\(设X与Y是两个随机变量，那么D(X+Y)=D(X)+D(Y)+2cov(X,Y)\)</span></li>
<li><span class="math inline">\(协方差：Cov(X,Y)=E\{[X-E(X)][Y-E(Y)]\}=E(XY)-E(X)E(Y)\)</span></li>
</ol>
<p><a href="https://www.cnblogs.com/jfdwd/p/11274056.html" target="_blank" rel="noopener">期望，方差，协方差</a></p>
<h2 id="algorithm">Algorithm</h2>
<p><strong>ByteDance AI LAB</strong></p>
<ul>
<li>topk问题</li>
</ul>
<p>topk问题一律使用最小堆解决。</p>
<p>解答：https://blog.csdn.net/zyq522376829/article/details/47686867?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task</p>
<ul>
<li>一道编程题： 给一个数组A，如何变成数组B，B要满足这个形式 B0 &gt;= B1&lt;= B2 &gt;= B3 &lt;= B4….【leetcode324】 （不需要对数组排序，只需要将数组A按照中位数分成两集合，大集合内的数放奇数位，小集合内的数放偶数位。分成两集合的方法：先得到数组长度len，然后使用快排剪枝或者使用堆来选出两个大小为len/2的集合，可看作topk问题）</li>
<li>最快速度最小空间求一个数组的第k小</li>
<li>如何找到一个无序数组的中位数</li>
</ul>
<p>解答：1.使用排序算法，这是最直接的想法；2.建立最小堆。这里要分情况讨论：当数组元素数目n为奇数的时候，我们取(n+1)/2个元素来建立最小堆；之后，遍历剩余的元素，比堆顶元素大，就替换堆顶元素，最后，堆顶元素就是中位数。</p>
<ul>
<li>已知数组a和正整数m，从数组a中找到一个连续子序列，使得连续子序列的和&gt;=m，求满足条件的最短的连续子序列的长度。</li>
<li>海量数据处理题目</li>
</ul>
<p>解答：https://blog.csdn.net/v_july_v/article/details/6279498/</p>
<ul>
<li>最长公共子序列.</li>
</ul>
<p>解答：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">longestCommonSubsequence</span><span class="params">(<span class="built_in">string</span> text1, <span class="built_in">string</span> text2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(text1.<span class="built_in">size</span>()==<span class="number">0</span> || text2.<span class="built_in">size</span>()==<span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">       <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; <span class="title">dp</span><span class="params">(text1.<span class="built_in">size</span>()+<span class="number">1</span>,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(text2.<span class="built_in">size</span>()+<span class="number">1</span>,<span class="number">0</span>))</span></span></span><br><span class="line"><span class="function">      </span></span><br><span class="line"><span class="function">       <span class="title">for</span><span class="params">(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=text1.<span class="built_in">size</span>();i++)</span></span>&#123;</span><br><span class="line">           <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;=text2.<span class="built_in">size</span>();j++)&#123;</span><br><span class="line">               <span class="keyword">if</span>(text1[i<span class="number">-1</span>]==text2[j<span class="number">-1</span>])&#123;</span><br><span class="line">                   dp[i][j]=dp[i<span class="number">-1</span>][j<span class="number">-1</span>]+<span class="number">1</span>;</span><br><span class="line">               &#125;</span><br><span class="line">               <span class="keyword">else</span>&#123;</span><br><span class="line">                   dp[i][j]=<span class="built_in">max</span>(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>]);</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[text1.<span class="built_in">size</span>()][text2.<span class="built_in">size</span>()];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>10亿int，4GB内存，找出所有不重复数字？</p>
<p><a href="https://blog.csdn.net/jacke121/article/details/78336275/" target="_blank" rel="noopener">答案</a></p></li>
</ul>
<p>统计词频</p>
<p>bash命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat words.txt | tr -s <span class="string">' '</span> <span class="string">'\n'</span> | sort | uniq -c | sort -r | awk <span class="string">'&#123;print $2" "$1&#125;'</span></span><br></pre></td></tr></table></figure>
<h2 id="iq-tests">IQ Tests</h2>
<p>现在有三枚硬币，一个是一正一反，一个是两面都是正，一个是两面都是反，现在随机抛出一枚硬币是正面，那么这枚硬币的反面也是正面的概率；（2/3）</p>
<p>解答：<span class="math inline">\(\frac {1/3}{1/3*1/2+1/3}=2/3\)</span></p>
<h2 id="pythonc">Python/C++</h2>
<ul>
<li><p>python的多线程？(假的多线程)</p></li>
<li><p>python的浅拷贝与深拷贝。</p></li>
<li><p>yield？<a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856/" target="_blank" rel="noopener">yield的使用</a></p></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#yield的使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_number</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        print(<span class="string">f"<span class="subst">&#123;i&#125;</span>"</span>)</span><br><span class="line">        <span class="keyword">yield</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意a=[i for i in range(n)]，当改成：a=(i for i in range(n)),就变成一个generator了！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">f"starting......."</span>)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        res=<span class="keyword">yield</span> <span class="number">4</span></span><br><span class="line">        print(<span class="string">f"res: <span class="subst">&#123;res&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    a=print_number(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> a: <span class="keyword">continue</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'*'</span>*<span class="number">20</span>)</span><br><span class="line">    g=foo()</span><br><span class="line">    print(next(g))</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'*'</span>*<span class="number">20</span>)</span><br><span class="line">    print(next(g))</span><br></pre></td></tr></table></figure>
<ul>
<li>python中的genertor？<a href="https://www.cnblogs.com/zywscq/p/5774567.html" target="_blank" rel="noopener">generator总结</a></li>
<li>rand7生成rand10？</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">rand7</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="comment">//https://www.cnblogs.com/xiaokang01/p/9786751.html</span></span><br><span class="line">	<span class="comment">//srand保证每次产生的随机数是不一样的</span></span><br><span class="line">	srand((<span class="keyword">int</span>)time(<span class="number">0</span>));</span><br><span class="line">	</span><br><span class="line">  <span class="keyword">return</span> rand()%<span class="number">8</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">rand10</span><span class="params">()</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">do</span>&#123;</span><br><span class="line">    	i=(rand7()<span class="number">-1</span>)*<span class="number">7</span>+rand7();</span><br><span class="line">  	&#125;<span class="keyword">while</span>(i&gt;<span class="number">40</span>);</span><br><span class="line"></span><br><span class="line">	  <span class="keyword">return</span> i%<span class="number">10</span>+<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;   </span><br><span class="line">  <span class="keyword">int</span> x=rand10();</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,x);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>AUC计算</strong></p>
<p>https://www.bioinfo-scrounger.com/archives/767/</p>
<h2 id="参考文献">参考文献</h2>
<p>1 https://blog.csdn.net/qq_28031525/article/details/80028055?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task</p>
]]></content>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow2的零碎笔记</title>
    <url>/2020/04/25/Tensorflow2%E7%9A%84%E9%9B%B6%E7%A2%8E%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>这篇文章主要是记录使用tensorflow2的一些笔记吧，因为有时候是真的容易忘。</p>
<a id="more"></a>
<p><strong>tf.keras.Embedding(input_dim=vocab_size,ouput_dim=embedding_size,input_length=input_length)</strong></p>
<blockquote>
<p>这是嵌入层，只能作为第一层。并且输入的形状只能是(batch_size,seq_length).</p>
</blockquote>
<p><strong>tf.keras.prerocessing.sequence.pad_sequences(x_train,maxlen=maxlen)</strong></p>
<blockquote>
<p>x_train只能是一个二维tensor，maxlen表示最终要得到的桔句子长度。</p>
</blockquote>
<p><strong>tf.keras.layers.Bidirectional(layer,merge_mode="concat,None,mul,sum")</strong></p>
<blockquote>
<p>比如传入LSTM的话，那么就可以得到biLSTM。如果是None的话，那么就返回两个值。</p>
</blockquote>
<p><strong>t f.keras.layers.LSTM(units,acitavtion,return_sequences,return_state,go_barckwards)</strong><a href="https://huhuhang.com/post/machine-learning/lstm-return-sequences-state" target="_blank" rel="noopener">链接</a></p>
<blockquote>
<p>units表示LSTM输出的维度；acitvation表示激活函数，默认是tanh；return_sequences表示是否返回整个输出序列，默认为false；return_state表示除了输出外，是否输出最后一个状态，默认false。如果return_sequences与return_state均是True的话，那么返回值有三个：整个输出序列，最后输出、最后一个状态。go_backwards为True，表示将输入给LSTM的输入反向。<a href="https://stackoverflow.com/questions/49946942/keras-lstm-go-backwards-usage" target="_blank" rel="noopener">见stackoverflow的解答</a></p>
<p>举个🌰：👇</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = np.random.random([<span class="number">32</span>, <span class="number">10</span>, <span class="number">8</span>]).astype(np.float32)</span><br><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">output = lstm(inputs)  <span class="comment"># The output has shape `[32, 4]`.</span></span><br><span class="line"></span><br><span class="line">lstm = tf.keras.layers.LSTM(<span class="number">4</span>, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># whole_sequence_output has shape `[32, 10, 4]`.</span></span><br><span class="line"><span class="comment"># final_memory_state and final_carry_state both have shape `[32, 4]`.</span></span><br><span class="line">whole_sequence_output, final_memory_state, final_carry_state = lstm(inputs)</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>tf.keras.layers.MaxPool2D(pool_size=(),strides=(),padding="valid")</strong></p>
<blockquote>
<p>输入的维度是4维的(batch_size,rows,cols,channels)，并且pool_size与strides都是tuple，这一点要记住，最后输出的维度是(batch_size,new_rows,new_cols,channels)。</p>
</blockquote>
<p><strong>callbacks的写法</strong></p>
<ul>
<li><strong>tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=3)</strong></li>
</ul>
<blockquote>
<p>其中，monitor表示要检测的指标，min_delta表示在被监测的数据中被认为是提升的最小变化，即绝对变化小于min_delta，将被视为没有提升；patience表示没有进步的训练轮数，在这之后训练就会被停止。</p>
</blockquote>
<p><strong>tf.keras.layers.Conv1D(filters=250,kernel_size=3,padding='same',strides=1)</strong></p>
<blockquote>
<p>一维卷积，输入的维度为（batch_size, steps, input_dim），输出维度为（batch_size, new_steps, filters）。</p>
</blockquote>
<p><strong>tf.keras.layers.MaxPool1D(pool_size=2,strides=1,padding='valid')</strong></p>
<blockquote>
<p>输入维度为(batch_size, steps, features)，输出维度为(batch_size, downsampled_steps, features)。</p>
</blockquote>
<p><strong>argparse的使用</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser=argparse.ArgumentParser(description=<span class="string">""</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--name"</span>,type=int/str,help=<span class="string">"name"</span>,default=xxx,required=<span class="literal">True</span>/<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">args=parser.parse_args()</span><br><span class="line">print(args.name)</span><br></pre></td></tr></table></figure>
<p><strong>范数的概念</strong></p>
<p>0范数，向量中非零元素的个数。</p>
<p>1范数，为绝对值之和。</p>
<p>2范数，就是通常意义上的模。</p>
<p>无穷范数，就是取向量的最大值</p>
<p><img src="/2020/04/25/Tensorflow2%E7%9A%84%E9%9B%B6%E7%A2%8E%E7%AC%94%E8%AE%B0/范数。png.png" style="zoom:80%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算1范数</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a=tf.norm(x,ord=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>np.argsort的使用</strong></p>
<p>y=np.argsort(X)的作用是：<strong>将X中的元素从小到大排序后，提取对应的索引index，然后输出到y。</strong></p>
<p>[::-1]是从最后一个元素到第一个元素复制一遍，即倒序。</p>
<p><strong>json的使用</strong><a href="https://www.cnblogs.com/linwow/p/10693781.html" target="_blank" rel="noopener">链接</a></p>
<blockquote>
<p>推荐使用json，而不是pickle，因为json得到的文件所有通用，而pickle得到的文件只能是python使用！</p>
</blockquote>
<p><strong>json.dumps(obj)</strong>:序列化成字符串</p>
<p><strong>json.dump(obj):</strong>序列化字符串到文件中</p>
<p><strong>json.loads(json_str):</strong>将json_str反序列化成原本的对象</p>
<p><strong>json.load(file):</strong>读取json文件，并反序列化成原本的对象</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">a=&#123;<span class="string">"a"</span>: <span class="number">1</span>, <span class="string">"c"</span>: <span class="number">0</span>, <span class="string">"b"</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="comment">#将a序列化成字符串</span></span><br><span class="line">json_str=json.dumps(a)</span><br><span class="line"><span class="comment">#序列化成字符串到文件中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"a.json"</span>.<span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">  json.dump(a,f)</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取json文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"a.json"</span>,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">  a=json.load(f)</span><br></pre></td></tr></table></figure>
<p>pickle的使用方法与json一致！但是写文件的时候，要使用"wb"，读文件的时候，要使用“rb”。</p>
<p><strong>导入上级模块</strong><a href="https://www.cnblogs.com/ipersevere/p/10916803.html" target="_blank" rel="noopener">链接</a>、<a href="https://zhuanlan.zhihu.com/p/64893308" target="_blank" rel="noopener">链接2</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第一步：在要访问的文件所在文件夹下，建立空的__init__.py文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#第二步：在要写的文件中写入下列代码：</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">".."</span>)<span class="comment">#返回到上级目录</span></span><br><span class="line"><span class="keyword">from</span> a.ss <span class="keyword">import</span> dasj</span><br></pre></td></tr></table></figure>
<p><strong>获取文件目录结构：tree</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">├── .DS_Store</span><br><span class="line">├── CDSSM</span><br><span class="line">│   └── CDSSM.py</span><br><span class="line">├── CompAgg</span><br><span class="line">│   ├── 1.py</span><br><span class="line">│   └── CompAgg.py</span><br><span class="line">├── DSSM</span><br><span class="line">│   └── DSSM.py</span><br><span class="line">├── DecAtt</span><br><span class="line">│   ├── DecAtt.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   └── DecAtt.cpython-37.pyc</span><br><span class="line">│   └── train.py</span><br><span class="line">├── ESIM</span><br><span class="line">│   ├── .DS_Store</span><br><span class="line">│   ├── 1.py</span><br><span class="line">│   ├── ESIM.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   └── ESIM.cpython-37.pyc</span><br><span class="line">│   └── train.py</span><br><span class="line">├── HCAN</span><br><span class="line">│   ├── 1.py</span><br><span class="line">│   └── HCAN.py</span><br><span class="line">├── InferSent</span><br><span class="line">│   ├── InferSent.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   └── InferSent.cpython-37.pyc</span><br><span class="line">│   └── train.py</span><br><span class="line">├── MatchPyramid</span><br><span class="line">│   ├── DynamicMaxPool2D.py</span><br><span class="line">│   ├── MatchPyramid.py</span><br><span class="line">│   ├── MatchingLayer.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   ├── DynamicMaxPool2D.cpython-37.pyc</span><br><span class="line">│   │   ├── MatchPyramid.cpython-37.pyc</span><br><span class="line">│   │   └── MatchingLayer.cpython-37.pyc</span><br><span class="line">│   └── train.py</span><br><span class="line">├── SIamLSTM</span><br><span class="line">│   ├── SiamLSTM.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   └── SiamLSTM.cpython-37.pyc</span><br><span class="line">│   └── train.py</span><br><span class="line">├── SSE</span><br><span class="line">│   └── SSE.py</span><br><span class="line">├── SiamBILSTM</span><br><span class="line">│   ├── SiamBILSTM.py</span><br><span class="line">│   └── train.py</span><br><span class="line">├── SiamCNN</span><br><span class="line">│   ├── SiamCNN.py</span><br><span class="line">│   ├── __pycache__</span><br><span class="line">│   │   └── SiamCNN.cpython-37.pyc</span><br><span class="line">│   └── train.py</span><br><span class="line">├── __init__.py</span><br><span class="line">└── __pycache__</span><br><span class="line">    └── __init__.cpython-37.pyc</span><br></pre></td></tr></table></figure>
<p><strong>tf.norm</strong></p>
<p>最近频繁涉及到什么余弦相似度的计算之类的，所以计算tensor的模是必须的，这里小小的总结一下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a=np.ones((<span class="number">100</span>,<span class="number">60</span>,<span class="number">30</span>))</span><br><span class="line"><span class="comment">#下面两种计算方法都是可以的，结果一样。</span></span><br><span class="line">b=tf.norm(a,ord=<span class="number">2</span>,axis<span class="number">-1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">c=tf.math.sqrt(tf.math.reduce_sum(tf.math.square(a),axis=<span class="number">-1</span>,keepdims=<span class="literal">True</span>))</span><br><span class="line">print(b==c)</span><br></pre></td></tr></table></figure>
<p>tf.norm是用来计算tensor的模的，具体来说是：tf.norm(tensor,ord=2,axis=-1,keepdims=True).其中，ord=1表示计算其l1范数，等于2表示计算l2范数；axis=-1表示从最后一维计算；keepdims=True表示计算之后的tensor的维度的个数不变。</p>
<p><strong>关于padding与mask</strong></p>
<p>看到一篇关于mask的应用场景与方案，写的很好，记录一下🤩链接：<a href="https://www.jianshu.com/p/7b408a60ac35" target="_blank" rel="noopener">padding与mask</a></p>
<p><strong>关于encode与decode</strong></p>
<p><a href="https://blog.csdn.net/wz947324/article/details/80625533" target="_blank" rel="noopener">参考链接</a></p>
<p>总的来说，如果str是字符串，如果我们要直接对它进行操作的话，就不需要encode或者decode，除非我们要将str保存到文件，那么我们就需要对其进行encode，在python代码里，具体是:<code>with codecs.open(foel_patj,"w",encoding="utf-8") as f:</code>。</p>
<p>如果我们str是bytes，那么我们要洗那个对其进行操作的话，那么我们就需要对其进行decode，从而得到字符串，具体就是：<code>u1=str.decode("utf-8")</code>。</p>
<p>我们具体在使用的时候，我们可以这样：（text是一个对象）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> six.PY3:<span class="comment"># 如果是python3的话 </span></span><br><span class="line">  <span class="keyword">if</span> isinstance(text, str):<span class="comment"># 如果是str的话，我们就直接返回使用即可</span></span><br><span class="line">      <span class="keyword">return</span> text</span><br><span class="line">    <span class="keyword">elif</span> isinstance(text, bytes): <span class="comment"># 如果是bytes，那么我们就是需要进行decode，得到字符串来使用。</span></span><br><span class="line">      <span class="keyword">return</span> text.decode(<span class="string">"utf-8"</span>, <span class="string">"ignore"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"Unsupported string type: %s"</span> % (type(text)))</span><br><span class="line"><span class="keyword">elif</span> six.PY2: <span class="comment"># 如果是python2的话</span></span><br><span class="line">  <span class="keyword">if</span> isinstance(text, str):</span><br><span class="line">    <span class="keyword">return</span> text.decode(<span class="string">"utf-8"</span>, <span class="string">"ignore"</span>)</span><br><span class="line">  <span class="keyword">elif</span> isinstance(text, unicode):</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line">  <span class="keyword">else</span>:</span><br></pre></td></tr></table></figure>
<p><strong>关于TFRrecord的使用</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_tfrecord</span><span class="params">(file_path, word_dict_path, max_utterance_num=<span class="number">10</span>, max_utterance_len=<span class="number">50</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    建立TFRecord文件，方便读取</span></span><br><span class="line"><span class="string">    :param file_path: 数据集的文件路径</span></span><br><span class="line"><span class="string">    :param word_dict_path: word dict的文件路径</span></span><br><span class="line"><span class="string">    :param max_utterance_num: context中utterance的最大数目</span></span><br><span class="line"><span class="string">    :param max_utterance_len: 句子的最大长度</span></span><br><span class="line"><span class="string">    :return:无</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    data = load_data(file_path)</span><br><span class="line">    word_dict = load_dictionary(word_dict_path)</span><br><span class="line">    print(<span class="string">"start bulid TFRecord!"</span>)</span><br><span class="line"></span><br><span class="line">    base_path = os.path.dirname(file_path)</span><br><span class="line">    base_name = os.path.basename(file_path)</span><br><span class="line">    target_path = base_path + <span class="string">"/%s.tfrecord"</span> % base_name</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建tfrecord的文件，名字为target_path</span></span><br><span class="line">    writer = tf.io.TFRecordWriter(target_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        context_indexes_new, context_len_new = context_to_index(item[<span class="string">"context"</span>], word_dict, max_utterance_num,max_utterance_len)</span><br><span class="line">        response_indexes_new, response_len_new = response_to_index(item[<span class="string">"response"</span>], word_dict, max_utterance_len)</span><br><span class="line">        label = int(item[<span class="string">"label"</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将数据汇总，构建特征</span></span><br><span class="line">        features = &#123;</span><br><span class="line">            <span class="string">"context"</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_indexes_new.tostring()])),</span><br><span class="line">            <span class="string">"context_len"</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_len_new.tostring()])),</span><br><span class="line">            <span class="string">"response"</span>: tf.train.Feature(bytes_list=tf.train.BytesList(value=[response_indexes_new.tostring()])),</span><br><span class="line">            <span class="string">"resonse_len"</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[response_len_new])),</span><br><span class="line">            <span class="string">"label"</span>: tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        tf_features = tf.train.Features(feature=features)</span><br><span class="line">        <span class="comment"># 把数据写入example</span></span><br><span class="line">        tf_example = tf.train.Example(features=tf_features)</span><br><span class="line">        <span class="comment"># example序列化</span></span><br><span class="line">        tf_serialized = tf_example.SerializeToString()</span><br><span class="line">        writer.write(tf_serialized)</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tfrecord_parser</span><span class="params">(max_utterance_num,max_utterance_len)</span>:</span></span><br><span class="line">   <span class="string">'''</span></span><br><span class="line"><span class="string">    解析tfrecord文件</span></span><br><span class="line"><span class="string">    :param max_utterance_num:  context中utterance的最大数目</span></span><br><span class="line"><span class="string">    :param max_utterance_len: 句子的最大长度</span></span><br><span class="line"><span class="string">    :return: parser</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parser</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">        feature=&#123;</span><br><span class="line">            <span class="string">"context"</span>:tf.io.FixedLenFeature(shape=[],dtype=tf.string)</span><br><span class="line">            <span class="string">"context_len"</span>:tf.io.FixedLenFeature(shape=[],dtype=tf.string)</span><br><span class="line">            <span class="string">"response"</span>:tf.io.FixedLenFeature(shape=[],dtype=tf.string)</span><br><span class="line">            <span class="string">"reponse_len"</span>:tf.io.FixedLenFeature(shape=[],dtype=tf.int64)</span><br><span class="line">            <span class="string">"label"</span>:tf.io.FixedLenFeature(shape=[],dtype=tf.int64)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        parsed_example=tf.io.parse_single_example(serialized=example_proto,features=feature)</span><br><span class="line">        context=tf.reshape(tf.io.decode_raw(parsed_example[<span class="string">"context"</span>],tf.int32),shape=[max_utterance_num,max_utterance_len])</span><br><span class="line">        context_len=tf.reshape(tf.io.decode_raw(parsed_example[<span class="string">"context_len"</span>],tf.int32),shape=[max_utterance_num])</span><br><span class="line">        response=tf.reshape(tf.io.decode_raw(parsed_example[<span class="string">"response"</span>],tf.int32),shape=[max_utterance_len])</span><br><span class="line">        response_len=parsed_example[<span class="string">"response_len"</span>]</span><br><span class="line">        label=parsed_example[<span class="string">"label"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context,context_len,response,response_len</span><br><span class="line">    <span class="keyword">return</span> _parser</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch_dataset</span><span class="params">(tfrecord_file,parser,batch_size,is_test=False)</span>:</span></span><br><span class="line">     <span class="string">'''</span></span><br><span class="line"><span class="string">    建立batch的数据</span></span><br><span class="line"><span class="string">    :param tfrecord_file: tfrecord文件</span></span><br><span class="line"><span class="string">    :param parser: parser</span></span><br><span class="line"><span class="string">    :param batch_size: batch size的大小</span></span><br><span class="line"><span class="string">    :return: batch dataset</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> is_test:</span><br><span class="line">        dataset=tf.data.TFRecordDataset(tfrecord_file).map(parser).batch(batch_size)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataset=tf.data.TFRecordDataset(tfrecord_file).map(parser).batch(batch_size)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>
<p><strong>np.random.choice</strong></p>
<p>参考链接：<a href="https://blog.csdn.net/ImwaterP/article/details/96282230" target="_blank" rel="noopener">链接</a></p>
<p><strong>fzf的使用</strong></p>
<ul>
<li>ctrl+r：显示所用历史命令</li>
<li>tt：快速浏览当前目录下的文件，(具体可以自己设置)</li>
<li>ctrl+t：快速选择当前目录下的文件</li>
<li>cd ** ：模糊查找文件</li>
</ul>
<p><strong>tensorflow中获取shape的方法</strong></p>
<p>在tensorflow2中，获取tensor的shape的大致分为两类：静态方法与动态方法</p>
<p>所谓的静态方法是：a.shape.as_list()，a.get_shape;</p>
<p>所谓的动态方法是：tf.shape(a).numpy()</p>
<p>推荐使用tf.shape来获取tensor的维度，因为如果使用静态方法来做的话，如果之后要使用tf.reshape这样的op，那么会报错。</p>
<p><strong>tf.slice(a,begin=[],size=[])</strong></p>
<p>begin的个数与a的维度的个数相同，表示从这个维度第几个开始，size表示在每一个维度取得的数目</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">data=tf.reshape(tf.range(<span class="number">0</span>,<span class="number">9</span>),shape=[<span class="number">3</span>,<span class="number">3</span>])<span class="comment">#[3,3]</span></span><br><span class="line"><span class="comment"># begin=[1,1]表示第0维从下标为1的开始算起，第1维从下标为1的开始算起；size=[1,2]表示第0维取1个，第1维取2个</span></span><br><span class="line">sliced_data=tf.slice(data,begin=[<span class="number">1</span>,<span class="number">1</span>],size=[<span class="number">1</span>,<span class="number">2</span>]) </span><br><span class="line">print(sliced_data)</span><br><span class="line"><span class="comment"># 结果如下：</span></span><br><span class="line"><span class="comment"># tf.Tensor([[4 5]], shape=(1, 2), dtype=int32)</span></span><br></pre></td></tr></table></figure>
<p><strong>tf.gather(tensor,[1,2,3])</strong></p>
<p>表示从tensor里面取出索引为[1,2,3]的tensor，例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">data=tf.reshape(tf.range(<span class="number">0</span>,<span class="number">9</span>),shape=[<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">print(data)</span><br><span class="line">e=tf.gather(data,[<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">print(e)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tf.Tensor(</span></span><br><span class="line"><span class="string">[[0 1 2]</span></span><br><span class="line"><span class="string"> [3 4 5]</span></span><br><span class="line"><span class="string"> [6 7 8]], shape=(3, 3), dtype=int32)</span></span><br><span class="line"><span class="string">tf.Tensor(</span></span><br><span class="line"><span class="string">[[0 1 2]</span></span><br><span class="line"><span class="string"> [3 4 5]], shape=(2, 3), dtype=int32)</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tensorflow2</tag>
      </tags>
  </entry>
  <entry>
    <title>保研PKU之路</title>
    <url>/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/</url>
    <content><![CDATA[<p>我的保研之路于2018年9月28日确认北大的录取通知的那一刻，就正式宣告结束了。现在算算，其实已经过去了一年半了。最近突然心血来潮，想记录一下那段兵慌马乱却又无比充实的日子。一方面，趁自己对这段记忆还清晰的时候，记录下来勉励自己；另一方面，也是希望能够对素昧谋面的保研er们能够有所帮助～</p>
<a id="more"></a>
<h2 id="个人情况">个人情况</h2>
<p><strong>本科学校：</strong>末流985</p>
<p><strong>绩点排名：</strong>2/70（最终1/70）</p>
<p><strong>六级成绩：</strong>500边缘</p>
<p><strong>科研情况：</strong>一篇中文核心，一篇英文在投（目前已发表）</p>
<p><strong>获奖情况：</strong>3次国家奖学金，省部级及以上奖项5项</p>
<p><strong>社会实践：</strong>2项</p>
<h2 id="夏令营经历2018年3月1日2018年7月27日">夏令营经历（2018年3月1日——2018年7月27日）</h2>
<p>全国高校的夏令营从3月份开学就陆陆续续地开始了，而且越好的学校开始时间越早，6月到8月是夏令营开营时间。我同学在3月初的时候，拿到了清华的offer，就结束了他的推免之路了。。。膜(sang)拜(xin)巨(bing)佬(kuang)😷。所以及时获取信息很重要。我主要是在保研论坛以及各个学校学院的官网寻找夏令营信息。之后按照各个夏令营的要求，准备好简历、推荐信、获奖证书、科研论文等等，邮寄或者网上申报。</p>
<p>关于选择夏令营，我采取了“题海战术”，其实投了挺多学校的。但是现在回过头来看，其实并不推荐大家这样做。原因有两个：一方面是，大三下学期还有很多课程与考试，其实根本没有那么多的时间去参加很多的夏令营；另一方面，参加夏令营并没有大家想象地那么轻松，参营准备过程其实非常地耗费精力；此外，还有准备夏令营的材料，也是非常劳民伤财的事情。所以大家在参加夏令营的时候，一定要有所选择。</p>
<p>我收到的夏令营参营通知挺多的，从中选择参加了几个自己喜欢的夏令营。</p>
<h3 id="人大信院2018年7月8日2018年7月10日">人大信院(2018年7月8日——2018年7月10日)</h3>
<p>人大信院是我最先收到夏令营参营通知(6月19日)，正好那段时间没有考试，所以就抱着兴奋的心情去参加了。我是7月8日到的人大，当时被安排住在品园，说实话有点失望，因为品园的宿舍真的极其地破，以至于我现在回忆起来仍然是记忆犹新......</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/人大安排.jpeg" style="zoom:50%;"></p>
<center>
<font face="Times New Roman">1.人大信院夏令营时间安排</font>
</center>
<p>9号上午有开营仪式，主要是介绍学院的概况、科研团队以及学院就业情况等等，总之就是一顿夸。</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/人大信院开营仪式.jpeg" style="zoom:50%;"></p>
<center>
<font face="Times New Roman">2.人大信院开营仪式</font>
</center>
<p>9号下午是正式的面试。面试分为英文面试+中文面试。英文面试会简单地问一些大学生活，基本不会涉及到科研等方面；中文面试就比较严苛，会根据简历来提问。比如，因为我当时简历上有论文，所以老师直接问了论文的内容，而且老师问的非常的深，甚至要我介绍论文中一些公式的变量是什么意思，起了什么作用，为什么要加这个变量......</p>
<p>10号上午，首先是笔试。主要是涉及到C++、JAVA中的基础内容，不过最后有几道大题，需要手写代码，我记得其中有一个是：给定一段话，统计其词频。总体来说，难度中等。笔试完后，就是机试。人大信院机试所用的平台是北大的POJ，当时好像是三道题，具体是啥我忘记了，我当时AC了一道最简单关于字符串的题目，还是太菜了·......</p>
<p>最后，人大信院的offer我没拿到。我觉得主要有两个原因：1.人大信院的夏令营录取率太低了，39个人只录了3个人，7.6%的录取率，是我听说过的所有夏令营最低的......而且选人标准很迷啊，当时和我一起住的两个哥们，也都没有收到人大信院的offer，但是到最后，一个去了中科院信工所，一个去了北大叉院，对，没错，就是叉院......2.机试发挥不好，就AC了一道，还是自己太菜了。</p>
<h3 id="央财信院2018年7月12日2018年7月15日">央财信院(2018年7月12日——2018年7月15日)</h3>
<p>央财是我第二个决定参加的夏令营。当时其实还接到了天大的通知，而且在天大夏令营参营名单公示之前，老师主动发邮件表示对我很感兴趣，要我去参加。不过我权衡再三，还是拒绝了天大，选择参加央财的夏令营，可能还是喜欢北京吧。（后来我才知道央财信院在沙河...）</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/央财信院安排.jpg" style="zoom:50%;"></p>
<center>
<font face="Times New Roman">3.央财信院夏令营时间安排</font>
</center>
<p>7月12日晚上，破冰仪式，这让我印象很深刻，因为其他的夏令营都没有这个环节，都是直接介绍学院概况，然后面试、笔试之类的。当时老师们开展了很多游戏让我们彼此熟悉，瞬间让我在这个残酷的保研夏令营中，感觉到了一丝的温暖。</p>
<p>7月13日上午，学院概况宣讲。下午，笔试。笔试内容主要来源于《管理信息系统》这本书，当然还有开放题，我印象比较深的是：如何看待普惠金融？</p>
<p>7月14日上午，学生成果展示。主要是在3分钟之内，通过PPT展示自己的科研情况，这时候有论文简直不要太爽哈哈。下午，面试环节。这个面试比较有意思，面试问题不是由老师来提问，而是像抽签一样由学生抽取问题，而且会有1-2分钟的思考时间。我当时抽到的问题是：什么是经济新常态？表述没有时间限制。</p>
<p>7月15日，离营。</p>
<p>信院的offer是我后来才收到的，所以我当时就拒掉了。当然，也不只是这个原因，更多的还是觉得，信院和自己想象中的差距比较大，所以毅然决然地拒掉了。</p>
<h3 id="南大工管院2018年7月25日2018年7月27日">南大工管院(2018年7月25日——2018年7月27日)</h3>
<p>南大工管院是我参加的第三个夏令营。在参加这个夏令营之前，我还收到了中山以及清华的通知，但是纠结再三(当时因为拒掉了央财，手里没有一个offer，比较慌)，还是参加了南大。(所以真的不要报太多的夏令营，选择的时候真的非常的纠结，尤其是拒掉了清华的时候😷)</p>
<p>我参加的是二期的夏令营，一期的夏令营举办时间和考试时间撞了，非常地遗憾，所以只能参加二期。但是建议大家能参加一期，还是要去参加一期，因为后来我才知道，一期二期差别待遇非常大。参加一期的同学不仅住的酒店非常豪华，而且还给报销路费，最后录取率还极其地高；参加二期的同学，啥都没有......</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/南大安排.jpg" style="zoom:50%;"></p>
<center>
<font face="Times New Roman">4.南大工管院夏令营时间安排</font>
</center>
<p>7月25日，报道。</p>
<p>7月26日上午，笔试。内容是根据参营通知邮件中的论文，回答相关问题。每一个人发的论文都不一样，我的是这一篇论文👇；下午是由一个滴滴的科学家做的学术报告；晚上是学院以及学科介绍。</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/南大论文.jpg" style="zoom:50%;"></p>
<center>
<font face="Times New Roman">5.论文截图</font>
</center>
<p>7月27日上午，面试。面试内容是老师根据你简历来进行提问。因为我简历上有些科研经历，老师也就针对论文开始提问了(所以有科研经历真的非常加分) 。整个面试过程非常地顺利，面试完后，我就知道我肯定能拿到offer。</p>
<p>最后也是不出意料地拿到了offer～</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/南大录取.jpg" style="zoom:50%;"></p>
<center>
<font face="Times New Roman">6.南大工管院夏令营offer</font>
</center>
<p>##预推免——征战北大（2018年9月21日）</p>
<p>其实夏令营拿到了南大的offer，其实还是比较满意的，所以预推免阶段就没有花太多心思，不过我好好准备了北大的预推免。原因有两个：1.在夏令营的时候，本来想报北大信科和叉院，但是报名阶段网站崩了，导致没报上，总觉得是遗憾，虽然报了也不一定录得上哈哈哈；2.自身还是稍微有点名校情结的，对北大又比较有好感，所以预推免阶段就好好地准备了北大的预推免。</p>
<p>我报的是北大软微院，有两个原因：一方面是当时通过夏令营拿到了信科offer的同学了解到，我想找的信科的导师已经没有名额了；另一方面，我之后的规划是不出意外的情况下，大概率是读研之后准备工作，所以软微院也比较符合我的期待。(9月18日收到面试通知👇)</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/北大推免通知.jpg"></p>
<center>
<font face="Times New Roman">7.北大软微院面试通知</font>
</center>
<p>9月21日，面试。面试有三位老师（我现在回想起来有一位是沈老师）。老师人都非常和蔼，不会给人压力。面试内容还是根据简历内容来进行提问。因为我简历上有科研经历，所以老师问到了我论文的idea以及innovation（有论文真的非常非常加分～）。另外，由于我的论文是关于大规模群决策共识问题的研究，而且老师似乎是做区块链方向的，所以老师就问到了我论文的共识与区块链中的共识的区别。幸运的是，大三上学期的时候，我在另一个做区块链的课题组待过一段时间，所以对这个还了解一些，还给老师大讲特讲了闪电网络、RSMC、HTLC等等。我也不知道当时我哪来的勇气，仅凭零星的记忆就敢去讲这些🤣。不过似乎老师们都比较满意，面了将近30分钟，我就出来了。（最后的得分还比较高哈哈哈）</p>
<p>9月26日，正式收到了北大的offer，lucky～</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/预推免成功.jpg"></p>
<center>
<font face="Times New Roman">8.北大软微院录取offer</font>
</center>
<h2 id="注定是难忘的日子2018年9月28日">注定是难忘的日子——2018年9月28日</h2>
<p>9月28日，我在研招网推免系统中确认了北大录取通知，我的保研之路也就正式结束了，我也正式迈入北大，感恩❤️～</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/1.复试确认.png"></p>
<center>
<font face="Times New Roman">9.确认复试通知</font>
</center>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/2.待录取确认.png"></p>
<center>
<font face="Times New Roman">10.接受待录取通知</font>
</center>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/3.成功录取.png"></p>
<center>
<font face="Times New Roman">11.成功录取</font>
</center>
<h2 id="年6月27日">2019年6月27日</h2>
<p>收到北大录取通知书～</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/录取通知书正面.jpeg" style="zoom: 33%;"></p>
<center>
<font face="Times New Roman">12.录取通知书</font>
</center>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/朋友圈.jpeg" style="zoom: 33%;"></p>
<center>
<font face="Times New Roman">13.朋友圈</font>
</center>
<h2 id="年9月6日">2019年9月6日</h2>
<p>开学啦❤️PKU～</p>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/北大开学1.jpeg" style="zoom: 33%;"></p>
<center>
<font face="Times New Roman">14.开学典礼</font>
</center>
<p><img src="/2020/02/17/%E4%BF%9D%E7%A0%94PKU%E4%B9%8B%E8%B7%AF/北大开学.jpeg" style="zoom: 33%;"></p>
<center>
<font face="Times New Roman">15.朋友圈</font>
</center>
<h2 id="现在">现在</h2>
<p>现在正在努力学习，想拥有的太多，自身还仍有不足，任重而道远，努力努力再努力❤️～</p>
<p>祝愿每个人都能成为想要的自己❤️～</p>
]]></content>
      <tags>
        <tag>PKU</tag>
        <tag>保研</tag>
      </tags>
  </entry>
  <entry>
    <title>强化学习|GAN models</title>
    <url>/2020/06/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-GAN-models/</url>
    <content><![CDATA[<p>由于需要，最近开始看强化学习的东西啦，大概一周时间搞完DRL基础，然后开始探索DRL在NLP中的应用。🤩这篇博客主要是记录关于GAN的原理、其背后的的理论以及其各种变体模型(especially for NLP)的内容，感觉还是挺有意思的。</p>
<a id="more"></a>
<h1 id="生成对抗网络gan">生成对抗网络（GAN）</h1>
<p>目前来看，GAN模型不是一个新鲜的技术了，它在神经风格迁移，人脸变换等等方面都有很大的应用。先来讲解什么是GAN。</p>
<blockquote>
<p>本篇博客的大部分内容来源于李宏毅教授的GANs课程。</p>
</blockquote>
<p>在GAN中，有两个部分：Generator与Discriminator。原论文使用的是银行与印钞团伙的例子。在这里，我用一个比较peace的比喻就是：Generator好比于学生，它输出的是学生自己完成的作业，DIscriminator好比于老师，来判断学生的作业是否达到标准要求。将标准的作业与学生的作业都做为Discriminator的输入，老师希望自己判别标准作业以及学生的作业的能力越来越强，而学生则希望自己完成的作业越来越靠近标准的作业，以此来通过老师的判定。最终，当老师无法判定标准作业以及学生的作业之间的区别的时候，就说明学生自己的作业就达到了标准作业的要求，通过了老师的判定。</p>
<p>具体来说，Generator用来生成”假样本“，将“假样本”与真实样本同时输入Discriminator中，Discriminator需要判断哪些是真实样本，哪些是“假样本“。如果是真实样本，我们给它记为1，如果是”假样本“，我们记为0。这样不断循环，Generator不断生成更加能够“骗过”Discriminator的“假样本”，而Discriminator则不断提高自己识别真实样本与”假样本“的能力，最终，知道Discriminator无法辨别真实样本与”假样本“的时候，就结束。我们最终使用的是Generator。下面给出公式化的表达，如下：</p>
<p><img src="/2020/06/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-GAN-models/GAN.jpg"></p>
<h2 id="gan的原理">GAN的原理</h2>
<p>放图～</p>
<p><img src="/2020/06/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-GAN-models/ganyuanli.jpg"></p>
<p>原始的GAN实际上就是在minimizeGenerator生成的data的分布<span class="math inline">\(P_G\)</span>与真实的data分布<span class="math inline">\(P_{data}\)</span>的JS divergence。下面具体分析一下～</p>
<ul>
<li>for Discriminator：它的作用是衡量得到<span class="math inline">\(P_G\)</span>与<span class="math inline">\(P_{data}\)</span>的JS divergence。它优化的目标是要maximize真实data的概率，同时minimize Gennerator生成的data的概率，这实际上就是和LR是一样的。我们可以将真实data看到positive sample，Generator生成的data看作negative sample，可以看到与LR是等价的。所以，Discriminator的loss就等价于 minimize CE。用公式来表示的话，就是：</li>
</ul>
<p><span class="math display">\[
D^*=argmaxV(G,D)
\]</span></p>
<ul>
<li>For Generator：它的作用是minimize 得到的JS divergence。用公式来表示就是：</li>
</ul>
<p><span class="math display">\[
G^*=arg min \ Div(P_g,P_{data})=argmin \ max \ V(G,D)
\]</span></p>
<p>另外，我们在更新Generator的时候，我们会使用<span class="math inline">\(V=\frac 1m\sum_{i=1}^{m}-log(D(G(z_i)))\)</span>，因为相比于<span class="math inline">\(log(1-D(G(z_i)))\)</span>，在训练的开始，梯度下降的比较快。</p>
<h2 id="fgan任意的divergence">fGAN——任意的Divergence</h2>
<p>在原始的GAN中，我们是去衡量并minimize Generator生成的data的分布与真实data的分布的JS divergence，那我们很容易想到以下两个问题：</p>
<ol type="1">
<li>GAN中是否可以换成的其他的divergence呢？</li>
<li>换成的其他的divergence，是否会有提高呢？也就是说JS divergence是否是最优的divergence呢？</li>
</ol>
<p>对于第一个问题，答案是可以的！结果就是这一小节讲的f-divergence；对于第二个问题，答案是：不知道，因为换用其他的divergence，效果似乎没有更好，但是也没有更差，很多GAN的变体中用到了其他的divergence，之后会讨论。</p>
<h1 id="wgan">WGAN</h1>
<h1 id="vae-gan">VAE-GAN</h1>
<h1 id="bigan">BIGAN</h1>
<h1 id="gan的实现">GAN的实现</h1>
<h2 id="实现代码">实现代码</h2>
<p>在这里，我采用tensorflow2来实现DCGAN模型。需要注意：在Generator部分，我们是需要从低维转到高维，所以使用反卷积操作，但是需要注意的是：如果使用反卷积，势必要使用填充，这样的话，如果我们要生成满意的图片的话，必须要保留位置信息，所以，在实现的时候，我们要抛弃会损失位置信息的池化层以及最后的全连接层，此外，我们要使用多个反卷积，从而使得宽度与高度不断变大，深度不断变小，从而达到我们预期的维度，除此之外，由于叠加多层反卷积层，所以我们需要添加BatchNorm，来改善模型的训练效果，同时我们还需要是用leakyRelu，来使得梯度保持稳定。对于Disciriminator，我们使用多个卷积层，其他的与Generator一样，注意，我们最后我们仍然不使用全连接层，而是扁平化后直接送入sigmoid层。具体如下：</p>
<p>首先是模型定义部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    generator部分</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.fc=layers.Dense(<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>,use_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.transpose_conv2d1=layers.Conv2DTranspose(filters=<span class="number">256</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.bn1=layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">        self.transpose_conv2d2=layers.Conv2DTranspose(filters=<span class="number">128</span>,kernel_size=(<span class="number">5</span>,<span class="number">5</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.bn2=layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">        self.transpose_conv2d3=layers.Conv2DTranspose(filters=<span class="number">3</span>,kernel_size=(<span class="number">4</span>,<span class="number">4</span>),strides=(<span class="number">3</span>,<span class="number">3</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2d=layers.Conv2D(filters=<span class="number">1</span>,kernel_size=(<span class="number">9</span>,<span class="number">9</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        x=self.fc(inputs)</span><br><span class="line">        x=tf.reshape(x,shape=[<span class="number">-1</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">512</span>])</span><br><span class="line">        x=tf.nn.leaky_relu(x)</span><br><span class="line"></span><br><span class="line">        x=self.transpose_conv2d1(x)</span><br><span class="line">        x=self.bn1(x,training=training)</span><br><span class="line">        x=tf.nn.leaky_relu(x)</span><br><span class="line"></span><br><span class="line">        x=self.transpose_conv2d2(x)</span><br><span class="line">        x=self.bn2(x,training=training)</span><br><span class="line">        x=tf.nn.leaky_relu(x)</span><br><span class="line"></span><br><span class="line">        x=self.transpose_conv2d3(x)</span><br><span class="line">        x=self.conv2d(x)</span><br><span class="line">        x=tf.nn.tanh(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    discriminator部分</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Discriminator, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv2d1=layers.Conv2D(filters=<span class="number">64</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.bn1=layers.BatchNormalization()</span><br><span class="line">        self.conv2d2=layers.Conv2D(filters=<span class="number">128</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.bn2=layers.BatchNormalization()</span><br><span class="line">        self.conv2d3=layers.Conv2D(filters=<span class="number">256</span>,kernel_size=(<span class="number">2</span>,<span class="number">2</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.bn3=layers.BatchNormalization()</span><br><span class="line"></span><br><span class="line">        self.flatten=layers.Flatten()</span><br><span class="line"></span><br><span class="line">        self.fc=layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None, mask=None)</span>:</span></span><br><span class="line">        x=self.conv2d1(inputs)</span><br><span class="line">        x=self.bn1(x,training=training)</span><br><span class="line"></span><br><span class="line">        x=self.conv2d2(x)</span><br><span class="line">        x=self.bn2(x,training=training)</span><br><span class="line"></span><br><span class="line">        x=self.conv2d3(x)</span><br><span class="line">        x=self.bn3(x,training=training)</span><br><span class="line"></span><br><span class="line">        x=self.flatten(x)</span><br><span class="line">        logits=self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<p>最后是训练过程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> GAN <span class="keyword">import</span> Discriminator, Generator</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义参数</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">buffer_size = <span class="number">256</span></span><br><span class="line">noise_dim=<span class="number">100</span></span><br><span class="line">num_sample=<span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 产生数据</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train=x_train[:num_sample,...]</span><br><span class="line">print(x_train.shape)</span><br><span class="line">x_train = tf.reshape(tf.cast((x_train - <span class="number">127.5</span>) / <span class="number">127.5</span>, dtype=tf.float32), shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">data = tf.data.Dataset.from_tensor_slices(x_train).shuffle(buffer_size).batch(batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator_loss</span><span class="params">(real_output, generator_output)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    discriminator的损失计算</span></span><br><span class="line"><span class="string">    :param real_output: 真实object输入到discriminator之后产生的结果</span></span><br><span class="line"><span class="string">    :param generator_output: generator产生的结果输入到discriminator之后产生的结果</span></span><br><span class="line"><span class="string">    :return:discriminator的损失</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">    discriminator_real_loss = loss_fn(tf.ones_like(real_output), real_output)</span><br><span class="line">    discriminator_fake_loss = loss_fn(tf.zeros_like(generator_output), generator_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> discriminator_real_loss + discriminator_fake_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_loss</span><span class="params">(generator_output)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    generator的损失计算</span></span><br><span class="line"><span class="string">    :param generator_output: generator输出的结果</span></span><br><span class="line"><span class="string">    :return: generator的损失</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">    generator_loss = loss_fn(tf.ones_like(generator_output), generator_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> generator_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">discriminator=Discriminator()</span><br><span class="line">generator=Generator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">d_optimizer=tf.keras.optimizers.Adam()</span><br><span class="line">g_optimizer=tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义单个的train step</span></span><br><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(batch_data)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    train step</span></span><br><span class="line"><span class="string">    :param batch_data: batch data</span></span><br><span class="line"><span class="string">    :return: 无</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    noise=tf.random.normal(shape=[batch_size,noise_dim])</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> discriminator_tape:</span><br><span class="line"></span><br><span class="line">        real_output=discriminator(batch_data,training=<span class="literal">True</span>)</span><br><span class="line">        generator_data=generator(noise,training=<span class="literal">True</span>)</span><br><span class="line">        generator_output=discriminator(generator_data,training=<span class="literal">True</span>)</span><br><span class="line">        d_loss=discriminator_loss(real_output,generator_output)</span><br><span class="line"></span><br><span class="line">    discriminator_gradient=discriminator_tape.gradient(d_loss,discriminator.trainable_variables)</span><br><span class="line">    d_optimizer.apply_gradients(zip(discriminator_gradient, discriminator.trainable_variables))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> generator_tape:</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        注意，使用 with as 结构一定要注意，当跳出这个结构后，里面的东西会失效，相当于关闭了，所以最好重新写一遍！</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        generator_data = generator(noise, training=<span class="literal">True</span>)</span><br><span class="line">        generator_output = discriminator(generator_data, training=<span class="literal">True</span>)</span><br><span class="line">        g_loss=generator_loss(generator_output)</span><br><span class="line"></span><br><span class="line">    generator_gradient=generator_tape.gradient(g_loss,generator.trainable_variables)</span><br><span class="line">    g_optimizer.apply_gradients(zip(generator_gradient, generator.trainable_variables))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d_loss,g_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_plot_image</span><span class="params">(gen_model,test_noise,epoch)</span>:</span></span><br><span class="line">    pre_images = gen_model(test_noise,training=<span class="literal">False</span>)</span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(pre_images.shape[<span class="number">0</span>]):</span><br><span class="line">        plt.subplot(<span class="number">4</span>,<span class="number">4</span>,i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow((pre_images[i,:,:,<span class="number">0</span>] + <span class="number">1</span> )/<span class="number">2</span>,cmap=<span class="string">'gray'</span>)</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.savefig(<span class="string">'image_at_epoch_&#123;:04d&#125;.png'</span>.format(epoch))</span><br><span class="line">    plt.close()</span><br><span class="line"></span><br><span class="line">num_exp_to_generate = <span class="number">16</span></span><br><span class="line">seed = tf.random.normal([num_exp_to_generate,noise_dim])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(data,epochs)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    训练</span></span><br><span class="line"><span class="string">    :param data: 数据</span></span><br><span class="line"><span class="string">    :param epochs: epoch的数据</span></span><br><span class="line"><span class="string">    :return: 无</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> index,batch_data <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            d_loss,g_loss=train_step(batch_data)</span><br><span class="line">            print(<span class="string">f"epoch:<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>,batch step:<span class="subst">&#123;index+<span class="number">1</span>&#125;</span>,d_loss:<span class="subst">&#123;d_loss&#125;</span>,g_loss:<span class="subst">&#123;g_loss&#125;</span>.\n"</span>)</span><br><span class="line">        generate_plot_image(generator, seed, epoch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    train(data,epochs)</span><br></pre></td></tr></table></figure>
<h2 id="结果">结果</h2>
<p>这是第一次epoch训练结束后，使用测试数据得到的结果：</p>
<p><img src="/2020/06/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-GAN-models/image_at_epoch_0000.png"></p>
<p>这是第五次epoch训练结束后，使用测试数据得到的结果：</p>
<p><img src="/2020/06/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-GAN-models/image_at_epoch_0004.png"></p>
<p>可以看到结果有很大的提升，当然了，我这里只训练了5个epochs，所以最后的结果也不是很好，可以试着训练几千轮，应该效果会好很多。到此为止，基础的GAN的知识就讲到这。其实GAN实现起来简单，但是GAN存在的很多的问题，需要关注，关于GAN背后的理论知识还是需要多去了解一点，才能够真正的用到其他的领域，譬如：Discriminator的本质是得到generator distribution与real distribution之间的某种divergence(原始的GAN是得到JS divergence)，而Generator的本质是去minimize 得到的divergence的值等等，这些理论都是需要去了解的，包括GAN需要注意的地方，譬如：优化饱和，mode collapse，model dropping问题。之后有时间再更新关于GAN的部分吧～</p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>tensorflow2</tag>
        <tag>强化学习</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习|XGBoost模型原理详解与实战</title>
    <url>/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<p>XGBoost模型是集成学习中最为著名的一个模型。不用我说，我相信只要做ML/DM/DL领域的筒子们，都会听说过这个模型。这篇博客将详细地讲解XGBoost模型的原理，并且使用XGBoost库来实践。注意：<strong>在看XGBoost之前，请务必要读懂CART与GBDT，具体可参看我之前的文章：<a href="https://codewithzichao.github.io/2020/02/27/统计学习方法-决策树模型原理详解与实现/">CART</a>、<a href="https://codewithzichao.github.io/2020/02/27/统计学习方法-AdaBoost模型原理详解与实现/#more">GBDT</a>。</strong></p>
<a id="more"></a>
<h2 id="xgboost模型介绍">XGBoost模型介绍</h2>
<p><strong>XGBoost模型是对GBDT很好地工程化的实现。</strong>所以，XGBoost模型也是加法模型，对于给定的样本<span class="math inline">\(x_i\)</span>，我们使用XGBoost对其分数<span class="math inline">\(\hat y_i\)</span>进行预测，可以表示为：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/0.jpg" style="zoom:80%;"></p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/1.jpg" style="zoom: 50%;"></p>
<p>其中，<span class="math inline">\(K\)</span>表示总共有<span class="math inline">\(K\)</span>个决策树，<span class="math inline">\(f_k\)</span>表示第<span class="math inline">\(k\)</span>个决策树，在XGBoost中使用CART树；<span class="math inline">\(\cal F\)</span>表示假设假设空间，即所有决策树的集合。那么，接下来，我们需要给出XGBoost模型的损失函数(严格来讲，叫做 cost function)，以便求出所有的<span class="math inline">\(f_k\)</span>。那么，如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/2.jpg" style="zoom:80%;"></p>
<p>其中，<span class="math inline">\(\sum_{i}l(\hat y_i,y_i)\)</span>是损失函数，表示的是模型对于训练集的拟合程度，<span class="math inline">\(\sum_k \Omega(f_k)\)</span>表示正则项，也叫做惩罚项，XGBoost中对每一个回归树进行惩罚，让模型的复杂度不过高，从而不会轻易过拟合。而衡量树的复杂度的指标有很多，譬如：树的深度、叶子结点的数目、内部节点数目等等。<strong>在XGBoost中，采用了叶子结点的数目来衡量树的复杂度</strong>。其中，<span class="math inline">\(T\)</span>表示的是叶子结点的数目，<span class="math inline">\(w\)</span>表示的是叶子结点的分数，也就是回归树预测的值。实际上，这个正则化项也相当于剪枝了。</p>
<h3 id="cost-function的推导">cost function的推导</h3>
<p>接下来，重点来了，我们需要对损失函数这一块进行泰勒展开！怎么做呢？始终要记住一句话：<strong>XGBoost模型是对GBDT模型很好地工程化的实现。</strong>所以，XGBoost模型仍然是<strong>前向分步算法</strong>。我们需要<strong>对损失函数进行二阶泰勒展开</strong>。如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/3.jpg" style="zoom: 67%;"></p>
<p>所以，通过泰勒展开和化简，我们得到最优的cost function如下： <span class="math display">\[
L^{*(t)}=-\frac{1}{2}\mathop{\sum}\limits_{j=1}^{T}\frac{G_j^2}{H_j+\lambda}+\gamma T
\]</span> 所以，从上式可以看出，<strong>只要我们确定了回归树的结构，那么我们就能得到最小的损失</strong>。那么，怎么去确定回归树的结构呢？</p>
<h3 id="确定回归树的结构">确定回归树的结构</h3>
<p>关于回归树的学习，一个很直接的想法，就是遍历所有特征的所有切分点，找到最佳切分变量与切分点，从而确定回归树的结构。那么，关键是：我们使用什么样的评价准则来评判切分的质量好坏？在XGBoost中，我们使用如下式子来评判分裂的好坏：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/4.jpg" style="zoom:67%;"></p>
<p>当<span class="math inline">\(\cal L_{split}\)</span>越大，那么说明叶子结点分裂的质量越好，最终的损失函数的值也会越小。所以，我们就可以依次所有特征的所有切分点，从中找到最大的<span class="math inline">\(\cal L_{split}\)</span>的切分点和所对应的值。这种方法叫做：<strong>精确贪心算法</strong>。</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/5.jpg" style="zoom: 67%;"></p>
<p>但是，这种分裂算法，在特征数目以及其对应的取值太多的时候，效率会非常的慢，所以，在XGBoost中，还提出的近似贪心算法。<strong>核心思想是：我们不需要依次遍历所有特征值的所有切分点，我们只需要找到并遍历有可能的切分点，从中找到<span class="math inline">\(\cal L_{split}\)</span>的值最大的切分变量与切分点就可以了。</strong></p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/6.jpg" style="zoom:67%;"></p>
<p>不过，在近似算法中，需要注意的是，我们并不是根据样本数目来进行分位，而是以每一个样本的二阶导数作为权重，以此排序，来进行分位。那么，为什么可以采用二阶导数来做权重呢？推导如下（参看链接：<a href="https://zhuanlan.zhihu.com/p/75217528" target="_blank" rel="noopener">xgboost</a>）：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/8.jpg" style="zoom: 33%;"></p>
<h2 id="稀疏值的处理">稀疏值的处理</h2>
<p>稀疏值有三种：<strong>大量的0值、大量的类别one-hot编码、缺失值。</strong>XGBoost可以自动学习出稀疏值的节点分裂方向。如下：</p>
<p><img src="/2020/02/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-XGBoost%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E6%88%98/7.jpg" style="zoom:67%;"></p>
<ul>
<li>首先需要注意到输入的两个集合：<strong>一个是<span class="math inline">\(I\)</span>，其包含所有的样本(包括含空缺值的样本)；另一个是<span class="math inline">\(I_k\)</span>，是不包含空缺值样本的集合。在计算总的<span class="math inline">\(G\)</span>和<span class="math inline">\(H\)</span>的时候，用的是<span class="math inline">\(I\)</span>。</strong></li>
<li>此外，可以看到内层循环里面有两个for。<strong>第一个for是从把特征取值从小到大排序。</strong>这个时候，在计算<span class="math inline">\(G_R\)</span>的时候，是用总的<span class="math inline">\(G\)</span>减去<span class="math inline">\(G_L\)</span>，计算<span class="math inline">\(H_R\)</span>也是使用<span class="math inline">\(H\)</span>减去<span class="math inline">\(H_L\)</span>。这就相当于将稀疏值归到了右节点。</li>
<li><strong>第二个for是从把特征取值从大到小排序。</strong>这个时候，在计算<span class="math inline">\(G_L\)</span> 的时候，是用总的<span class="math inline">\(G\)</span>减去<span class="math inline">\(G_R\)</span>，计算<span class="math inline">\(H_L\)</span>也是使用<span class="math inline">\(H\)</span>减去<span class="math inline">\(H_R\)</span>。这就相当于将稀疏值归到了左节点。</li>
<li><strong>我们只要比较最大的增益出现在第一个for，还是第二个for，我们就能知道稀疏值的分裂方向是在左还是右。</strong>这就是XGBoost模型处理稀疏值的方法。</li>
</ul>
<h2 id="two-tricks">two tricks</h2>
<p>在XGBoost模型中，除了通过添加正则化项来防止过拟合之外，还用了另外两个用来防止过拟合的trick：<strong>shrinkage(缩减)、column subsampling(列抽样)。</strong>我们一一介绍。<strong>shrinkage</strong>。所谓的shrinkage，就是将学习速率减小，增加迭代次数，从而起到正则化的作用。<strong>column subsampling</strong>。这个和RF中是一样的，没什么可说的。除此之外，XGBoost模型还有一些工程化的东西，像cache的设计等等。这些感兴趣的筒子们，可以去看原始paper🎉～</p>
<h2 id="xgboost模型实战">XGBoost模型实战</h2>
<p>当然了，我们并不会从头去去实现一个XGBoost模型，那样十分复杂，而且个人觉得意义不是很大哈。感兴趣的通知可以去直接去阅读源码细节，这样反而更好些🎉在这里，我将使用xgb库来对XGBoost模型进行实践。(夜深了，明天写吧🥱)</p>
<p>附上代码👇不过说实话，XGBoost的参数真的多😣。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">准确率：1.0</span></span><br><span class="line"><span class="string">时间：53.53627586364746</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">print(pd.__version__)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">path = os.getcwd()</span><br><span class="line"></span><br><span class="line">start = time.time()</span><br><span class="line"></span><br><span class="line">train = pd.read_csv(path + <span class="string">"/MnistData/mnist_train.csv"</span>, names=list(i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">784</span>)))</span><br><span class="line">test = pd.read_csv(path + <span class="string">"/MnistData/mnist_test.csv"</span>, names=list(i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">784</span>)))</span><br><span class="line"></span><br><span class="line">train_data = train.iloc[:, <span class="number">1</span>:]</span><br><span class="line">train_label = train.iloc[:, <span class="number">0</span>]</span><br><span class="line">print(train_data.shape)</span><br><span class="line">print(train_label.shape)</span><br><span class="line">test_data = test.iloc[:, <span class="number">1</span>:]</span><br><span class="line">test_label = test.iloc[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'booster'</span>: <span class="string">'gbtree'</span>,</span><br><span class="line">    <span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>,  <span class="comment"># 多分类的问题</span></span><br><span class="line">    <span class="string">'num_class'</span>: <span class="number">10</span>,  <span class="comment"># 类别数，与 multisoftmax 并用</span></span><br><span class="line">    <span class="string">'gamma'</span>: <span class="number">0.1</span>,  <span class="comment"># 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。</span></span><br><span class="line">    <span class="string">'max_depth'</span>: <span class="number">12</span>,  <span class="comment"># 构建树的深度，越大越容易过拟合</span></span><br><span class="line">    <span class="string">'lambda'</span>: <span class="number">2</span>,  <span class="comment"># 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。</span></span><br><span class="line">    <span class="string">'subsample'</span>: <span class="number">0.7</span>,  <span class="comment"># 随机采样训练样本</span></span><br><span class="line">    <span class="string">'colsample_bytree'</span>: <span class="number">0.7</span>,  <span class="comment"># 生成树时进行的列采样</span></span><br><span class="line">    <span class="string">'min_child_weight'</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="comment"># 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言</span></span><br><span class="line">    <span class="comment"># ，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。</span></span><br><span class="line">    <span class="comment"># 这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。</span></span><br><span class="line">    <span class="string">'silent'</span>: <span class="number">0</span>,  <span class="comment"># 设置成1则没有运行信息输出，最好是设置为0.</span></span><br><span class="line">    <span class="string">'eta'</span>: <span class="number">0.007</span>,  <span class="comment"># 如同学习率</span></span><br><span class="line">    <span class="string">'seed'</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">'nthread'</span>: <span class="number">7</span>,  <span class="comment"># cpu 线程数</span></span><br><span class="line">    <span class="comment"># 'eval_metric': 'auc'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">num_rounds = <span class="number">5000</span>  <span class="comment"># 迭代次数</span></span><br><span class="line"></span><br><span class="line">X_train, X_validation, Y_train, Y_validation = train_test_split(train_data, train_label, test_size=<span class="number">0.3</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># random_state is of big influence for val-auc</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xgb_val = xgb.DMatrix(X_validation, label=Y_validation)</span><br><span class="line">xgb_train = xgb.DMatrix(X_train, label=Y_train)</span><br><span class="line">xgb_test = xgb.DMatrix(test_data)</span><br><span class="line"></span><br><span class="line">watchlist = [(xgb_train, <span class="string">'train'</span>), (xgb_val, <span class="string">'val'</span>)]  <span class="comment"># 允许查看train set与dev set的误差表现</span></span><br><span class="line"><span class="comment"># training model</span></span><br><span class="line"><span class="comment"># early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练</span></span><br><span class="line">model = xgb.train(params, xgb_train, num_rounds, watchlist, early_stopping_rounds=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">model.save_model(path + <span class="string">"/xgboost/xgb.model"</span>)  <span class="comment"># 用于存储训练出的模型</span></span><br><span class="line">print(<span class="string">"best best_ntree_limit"</span>, model.best_ntree_limit)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"跑到这里了model.predict"</span>)</span><br><span class="line">preds = model.predict(xgb_test, ntree_limit=model.best_ntree_limit)</span><br><span class="line"></span><br><span class="line">accuracy_test = accuracy_score(preds, test_label)</span><br><span class="line">print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy_test&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="comment"># 输出运行时长</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">f"xgboost success! cost time:<span class="subst">&#123;end - start&#125;</span>(s)."</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<p>1 《XGBoost: A Scalable Tree Boosting System》</p>
<p>2 https://www.jianshu.com/p/ac1c12f3fba1</p>
<p>3 https://zhuanlan.zhihu.com/p/86816771</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>集成学习</tag>
        <tag>XGBoost</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|朴素贝叶斯模型详解与实现</title>
    <url>/2020/02/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>朴素贝叶斯模型(naive bayes)属于分类模型，也是最为简单的概率图模型，对于之后理解HMM、CRF等模型，大有裨益。本篇博客将对朴素贝叶斯模型的原理进行详细的讲解，并采用纯python实现以及调用scikit-learn库实现，这两种方式对朴素贝叶斯模型进行实现。</p>
<a id="more"></a>
<h2 id="朴素贝叶斯模型概述">朴素贝叶斯模型概述</h2>
<p>在朴素贝叶斯中，我们最终的目标是：给定实例的特征向量<span class="math inline">\(X=x\)</span>，求得实例的类别标签<span class="math inline">\(Y\)</span>，即<strong>求令$ P(Y=c_k|X) <span class="math inline">\(值最大的类别标签\)</span>c_k<span class="math inline">\(。**那么怎么做呢？我们会通过求**先验概率**\)</span>P(Y)<span class="math inline">\(与**条件概率**\)</span>P(X|Y)<span class="math inline">\(，得到联合概率分布(也就是说朴素贝叶斯模型属于生成模型)，最后根据**贝叶斯定理**求得\)</span>P(Y|X)<span class="math inline">\(，从中选择\)</span>P(Y=c_k|X)<span class="math inline">\(的值最大的类别标签\)</span>c_k<span class="math inline">\(，整个过程就结束了。如果我们仔细观察这个过程的话，我们会发现有几个问题需要解决：**1.先验概率\)</span>P(Y)<span class="math inline">\(与条件概率\)</span>P(X|Y)$怎么求出的？ 2. 如果实例的特征向量特别多的话，会造成计算量特别大，怎么解决的？</strong>下面我们将一一介绍。</p>
<h2 id="朴素贝叶斯的两大前提条件">朴素贝叶斯的两大前提条件</h2>
<p><strong>贝叶斯定理</strong>。贝叶斯定理，相信大家应该特别熟悉。假设<span class="math inline">\(X\)</span>是特征向量，<span class="math inline">\(Y\)</span>是类别标签，有<span class="math inline">\(K\)</span>种取值，我们要通过<span class="math inline">\(X\)</span>来预测<span class="math inline">\(Y\)</span>。那么如下： <span class="math display">\[
P(Y|X)=\frac {P(X|Y)P(Y)}{P(X)}.
\]</span> 其中，<span class="math inline">\(P(Y)\)</span>叫做先验概率，<span class="math inline">\(P(X|Y)\)</span>叫做条件概率，<span class="math inline">\(P(Y|X)\)</span>叫做后验概率。</p>
<p><strong>条件独立性假设</strong>。假设每个实例特征向量有<span class="math inline">\(n\)</span>维的话，第<span class="math inline">\(j\)</span>个特征有<span class="math inline">\(S_j\)</span>个取值。我们假设特征之间都是相互独立的，那么有 <span class="math display">\[
P(X=x|Y=c_k)=\prod _{i=1}^{n}P(X^{(i)}=x^{(i)}|Y=c_k)
\]</span> 条件独立性假设是非常强的假设，主要目的就是为了简化运算，实际上，特征之间应该是有关系的才更合理一些。而如果没有这个假设的话，那么上式的参数实际有:<span class="math inline">\(K\prod_{j=1}^{n}S_j\)</span>，当特征维度特别大的时候，这个参数量是非常大的。当采用了这个假设之后，参数量就变为：<span class="math inline">\(K\sum_{j=1}^{n}S_j\)</span>，这样就小很多。如果采用概率图来表示的话，如下图：</p>
<p><img src="/2020/02/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpeg" style="zoom: 25%;"></p>
<p>我们可以简单理解为：给定<span class="math inline">\(Y=c_k\)</span>，两个特征之间，由于被<span class="math inline">\(Y\)</span>给阻断了，所以彼此独立。</p>
<p>那么，根据上面两大前提条件，我们可以得到朴素贝叶斯模型。实际上我们就是要最大化后验概率，从而得到类别标签。如下： <span class="math display">\[
P(Y=c_k|X=x)=\frac {P(X=x|Y=c_k)P(Y=c_k)}{P(X=x)}=\frac {P(Y=c_k)\prod_{j=1}^{n}P(X=x^{(i)}|Y=c_k)}{P(X=x)}
\]</span> 由于<span class="math inline">\(P(X=x)\)</span>对于所有的类别标签来说的话，都是一样的，所以可以去掉，最终得到的公式如下: <span class="math display">\[
y=argmax_{c_k}P(Y=c_k){\prod_{j=1}^{n}P(X=x^{(i)}|Y=c_k)}
\]</span> 这就是朴素贝叶斯公式，是不是非常简单～</p>
<h2 id="先验概率与条件概率的计算">先验概率与条件概率的计算</h2>
<p>当得到了朴素贝叶斯的公式后，那么其中的<span class="math inline">\(P(Y=c_k)\)</span>与<span class="math inline">\(P(X=x|Y=c_k)\)</span>怎么求呢？在这里，我们需要分情况讨论：得看特征本身是离散的还是连续的。当特征是离散的时候，我们使用极大似然估计，叫做<strong>多项式模型</strong>；当特征是连续的时候，我们让其满足高斯分布，叫做<strong>高斯模型</strong>。下面就一一介绍。</p>
<h3 id="多项式模型">多项式模型</h3>
<p>当特征是离散的时候，我们使用极大似然估计去得到先验概率与条件概率。（公式太难敲了，我就直接贴图了😭图片来源：《统计学习方法》）</p>
<p><img src="/2020/02/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom: 50%;"></p>
<p>但是，在计算先验概率与条件概率的时候，我们会做一些平滑处理，以防出现为0的情况，从而影响到后验概率的计算。这种操作叫做laplace平滑。（图片来源：《统计学习方法》）</p>
<p><img src="/2020/02/18/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.jpg" style="zoom:50%;"></p>
<h3 id="高斯模型">高斯模型</h3>
<p>当特征为连续值的时候，我们就不能采取多项式模型来估计先验概率与条件概率了，因为会导致很多<span class="math inline">\(P(X=x_i|Y=c_k)\)</span>等于0。所以需要采用高斯模型。高斯模型的思想是：<strong>让特征的每一维都满足高斯分布(正态分布)，从而来处理连续特征</strong>。注意，先验概率的计算与多项式模型相同。公式如下： <span class="math display">\[
P(X^{(j)}=a|Y=c_k)=\frac {1}{\sqrt {2\pi \sigma^{2}_{c_k,j}}}e^{-\frac {({a-\mu_{c_k,j}})^2}{2\sigma^{2}_{c_k,j}}}
\]</span> 其中，<span class="math inline">\(\sigma^{2}_{c_k,j}\)</span>表示类别是<span class="math inline">\(c_k\)</span>的实例中，第<span class="math inline">\(j\)</span>维特征的方差，<span class="math inline">\(\mu_{c_k,j}\)</span>表示类别是<span class="math inline">\(c_k\)</span>的实例中，第<span class="math inline">\(j\)</span>维特征的均值。</p>
<p>当求出先验概率与条件概率之后，再带入到朴素贝叶斯公式中，就可以得到实例的类别标签了。到此，朴素贝叶斯的理论部分就讲完啦，相较于HMM、CRF等模型，真的可以说是非常简单了🎉～</p>
<h2 id="朴素贝叶斯模型的实现">朴素贝叶斯模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现朴素贝叶斯模型：纯python实现以及调用scikit-learn库来实现。我的github里面可以下载到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"><span class="comment"># Date:2019-12-30</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">准确率：0.8433</span></span><br><span class="line"><span class="string">时间：130.05937218666077</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy  <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    data_list = []</span><br><span class="line">    label_list = []</span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            label_list.append(int(curline[<span class="number">0</span>]))</span><br><span class="line">            data_list.append([int(int(feature) &gt; <span class="number">128</span>) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])  <span class="comment"># 二值化，保证每一个特征只能取到0和1两个值</span></span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Naive_Bayes</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_data, train_label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        :param train_data:训练集的特征向量</span></span><br><span class="line"><span class="string">        :param train_label: 训练集的类别标签</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.train_data = train_data</span><br><span class="line">        self.train_label = train_label</span><br><span class="line">        self.input_num, self.feature_num = self.train_data.shape  <span class="comment"># input_num、feature_num表示训练集数目、特征数目</span></span><br><span class="line">        self.classes_num = self.count_classes()</span><br><span class="line">        self.p_y, self.p_x_y = self.get_probabilities()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count_classes</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算类别数目</span></span><br><span class="line"><span class="string">        :return:类别数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        s = set()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.train_label:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> s:</span><br><span class="line">                s.add(i)</span><br><span class="line">        <span class="keyword">return</span> len(s)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_probabilities</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算先验概率与条件概率</span></span><br><span class="line"><span class="string">        :return: 返回先验概率与条件概率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        print(<span class="string">"start training"</span>)</span><br><span class="line">        p_y = np.zeros(self.classes_num)</span><br><span class="line">        p_x_y = np.zeros((self.classes_num, self.input_num, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算先验概率p_y</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.classes_num):</span><br><span class="line">            p_y[i] = (np.sum((self.train_label == i)) + <span class="number">1</span>) / (self.input_num + self.classes_num)</span><br><span class="line">        p_y = np.log(p_y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算条件概率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_num):</span><br><span class="line">            label = self.train_label[i]</span><br><span class="line">            x = self.train_data[i]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.feature_num):</span><br><span class="line">                p_x_y[label][j][x[j]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.classes_num):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.feature_num):</span><br><span class="line">                p_x_y_0 = p_x_y[i][j][<span class="number">0</span>]</span><br><span class="line">                p_x_y_1 = p_x_y[i][j][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                p_x_y[i][j][<span class="number">0</span>] = np.log((p_x_y_0 + <span class="number">1</span>) / (p_x_y_0 + p_x_y_1 + <span class="number">2</span>))</span><br><span class="line">                p_x_y[i][j][<span class="number">1</span>] = np.log((p_x_y_1 + <span class="number">1</span>) / (p_x_y_0 + p_x_y_1 + <span class="number">2</span>))</span><br><span class="line">        print(<span class="string">"finished training."</span>)</span><br><span class="line">        <span class="keyword">return</span> p_y, p_x_y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">naive_bayes_predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        预测单个实例x的类别标签</span></span><br><span class="line"><span class="string">        :param x: 特征向量</span></span><br><span class="line"><span class="string">        :return: x的类别标签</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        p = np.zeros(self.classes_num)</span><br><span class="line">        p_y, p_x_y = self.p_y, self.p_x_y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.classes_num):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.feature_num):</span><br><span class="line">                p[i] += p_x_y[i][j][x[j]]</span><br><span class="line">            p[i] += p_y[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.argmax(p)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_model</span><span class="params">(self, test_train, test_label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        在整个测试集上测试模型</span></span><br><span class="line"><span class="string">        :param test_train: 测试集的特征向量</span></span><br><span class="line"><span class="string">        :param test_label: 测试集的类别标签</span></span><br><span class="line"><span class="string">        :return: 准确率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        print(<span class="string">"start testing"</span>)</span><br><span class="line">        error = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_label)):</span><br><span class="line">            <span class="keyword">if</span> (self.naive_bayes_predict(test_train[i]) != test_label[i]):</span><br><span class="line">                error += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        accuarcy = (len(test_label) - error) / (len(test_label))</span><br><span class="line">        print(<span class="string">"finished testing."</span>)</span><br><span class="line">        <span class="keyword">return</span> accuarcy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start loading data."</span>)</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    a = Naive_Bayes(train_data, train_label)</span><br><span class="line">    accuracy = a.test_model(test_data, test_label)</span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="sckit-learn实现">sckit-learn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"><span class="comment"># Date:2019-12-30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    data_list = []</span><br><span class="line">    label_list = []</span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            label_list.append(int(curline[<span class="number">0</span>]))</span><br><span class="line">            data_list.append([int(int(feature) &gt; <span class="number">128</span>) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])  <span class="comment"># 二值化，保证每一个特征只能取到0和1两个值</span></span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start loading data."</span>)</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    clf=MultinomialNB()</span><br><span class="line">    clf.fit(train_data,train_label)</span><br><span class="line"></span><br><span class="line">    accuracy=clf.score(test_data,test_label)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    test_predict=clf.predict(test_data)</span><br><span class="line">    print(classification_report(test_label, test_predict))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>naive bayes</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|隐马尔可夫模型原理详解与实现</title>
    <url>/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>隐马尔可夫模型(HMM)，是一个可用来解决标注问题的生成模型，在自然语言处理、语音识别等领域有着广泛的应用。本篇博客将详细地介绍HMM模型的三个问题与解决算法，包括：前向-后向算法、Baum-Welch算法以及大名鼎鼎的Viterbi算法。最后，并采用python以及hmmlearn库这两种方式，来对HMM模型进行实现。</p>
<a id="more"></a>
<h2 id="隐马尔可夫模型介绍">隐马尔可夫模型介绍</h2>
<p>隐马尔可夫模型(HMM)，属于概率图模型。它的原理，总结起来就是三句话：<strong>一个模型，两个假设，三个问题(算法)</strong>。下面就讲一介绍。</p>
<h3 id="一个模型">一个模型</h3>
<p>HMM模型描述的是一个隐藏的马尔可夫链生成不可观测的状态序列，再由状态序列生成观测序列的过程。在这里，我们需要定义一些符号。我们将状态序列记为：<span class="math inline">\(I=\{i_1,i_2,...,i_T\}\)</span>，其中<span class="math inline">\(T\)</span>表示序列的长度，所有的状态变量的取值集合记做：<span class="math inline">\(Q=\{q_1,q_2,...,q_N\}\)</span>，其中<span class="math inline">\(N\)</span>表示状态取值的总数。我们将观测序列记为：<span class="math inline">\(O=\{o_1,o_2,...,o_T\}\)</span>，其中<span class="math inline">\(T\)</span>表示序列的长度，所有的观测变量的取值集合记做：<span class="math inline">\(V=\{v_1,v_2,...,v_M\}\)</span>。记HMM模型的参数为：<span class="math inline">\(\lambda=(\pi,A,B)\)</span>。其中，<span class="math inline">\(\pi\)</span>表示初始概率分布，<span class="math inline">\(A\)</span>表示状态转移矩阵；<span class="math inline">\(B\)</span>表示观测概率分布。具体它的概率图如下：</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg" style="zoom:50%;"></p>
<h3 id="两个假设">两个假设</h3>
<p>HMM模型中有两个假设：<strong>齐次马尔可夫假设</strong>、<strong>观测独立性假设</strong>。</p>
<p><strong>齐次马尔可夫假设</strong>。在给定<span class="math inline">\(i_t\)</span>的情况下，<span class="math inline">\(i_{t+1}\)</span>与<span class="math inline">\(i_{t-1}\)</span>以及之前的状态变量无关。即： <span class="math display">\[
P(i_{t+1}|i_t,i_{t-2},...,i_1,o_1,o_2,...,o_t)=P(i_{t+1}|i_t)
\]</span> <strong>观测独立性假设</strong>。在给定<span class="math inline">\(i_t\)</span>的情况下，<span class="math inline">\(o_t\)</span>与之前的状态与观测变量都无关。即： <span class="math display">\[
P(o_t|o_1,o_2,...,o_{t-1},i_1,i_2,...,i_t)=P(o_t|i_t)
\]</span></p>
<h3 id="三个问题算法">三个问题(算法)</h3>
<p>HMM模型主要要解决三个问题：<strong>Evaluation、Learning、Decoding</strong>。</p>
<p><strong>Evaluation</strong>。即：在给定模型参数<span class="math inline">\(\lambda\)</span>的情况，生成观测序列<span class="math inline">\(O\)</span>的概率<span class="math inline">\(P(O|\lambda)\)</span>。</p>
<p><strong>Learning</strong>。即：求<span class="math inline">\(\lambda\)</span>，即：<span class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}P(O|\lambda)\)</span>。</p>
<p><strong>Decoding</strong>。即：给定模型参数与观测序列的情况下，求概率最大的状态序列。即：<span class="math inline">\(I=\mathop {argmax} \limits_{I}P(I|O)\)</span>。</p>
<p>关于Evaluation问题，我们使用前向算法或者后向算法进行求解；关于Learning问题，我们使用Baum-Welch算法来求解；关于Decoding问题，我们使用Viterbi算法来求解。下面将一一介绍这些算法。</p>
<h2 id="evaluation问题的求解">Evaluation问题的求解</h2>
<p>关于Evaluation问题的求解，有两种算法：<strong>前向算法与后向算法。</strong>其实，最后结果是一样，只是求解过程不太一样。在这里，我将直接给出推导过程。</p>
<p>首先需要化简<span class="math inline">\(P(O|\lambda)\)</span>。如下：</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom: 50%;"></p>
<p>所以我们可以看到，公式：<span class="math inline">\(P(O|\lambda)=\mathop{\sum}\limits_{I}\prod_{t=1}^{T}b_{i_t}(o_t) \pi (a_{i_1})\prod_{t=2}^{T}a_{i_{t-1},i_{t-2}}\)</span>。其计算复杂度为：<span class="math inline">\(O(TN^T)\)</span>。当T很大的时候，计算复杂度是以指数形式增长的，所以近乎是不可解的。所以，我们必须找新的算法来解决这个问题。</p>
<h3 id="前向算法">前向算法</h3>
<p>使用前向算法，得出递推公式，如下：</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg" style="zoom: 67%;"></p>
<p>所以，我们可以得到递推公式：<span class="math inline">\(\alpha_{t+1}(i)=\mathop{\sum}\limits_{j=1}^{N}\alpha_t(j)a_{ji}b_i(o_{t+1})\)</span>。又<span class="math inline">\(\alpha_1(i)=\pi_ib_i(o_1)\)</span>，<span class="math inline">\(P(O|\lambda)=\mathop{\sum}\limits_{i=1}^{N}\alpha_T(i)\)</span>。那么我们就可以求出<span class="math inline">\(p(O|\lambda)\)</span>的值了。</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/4.jpg" style="zoom:50%;"></p>
<h3 id="后向算法">后向算法</h3>
<p>除了使用前向算法，后向算法也是求解<span class="math inline">\(P(O|\lambda)\)</span>的一种方法。递推公式的推导如下：</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/5.jpg" style="zoom:67%;"></p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/6.jpg" style="zoom:67%;"></p>
<p>所以，我们得到了递推公式：<span class="math inline">\(\beta_t(i)=\mathop{\sum}\limits_{j=1}^{N}b_j(o_{t+1})\beta_{t+1}(j)\)</span>。又<span class="math inline">\(\beta_T(i)=1\)</span>，<span class="math inline">\(P(O|\lambda)=\mathop{\sum}\limits_{i=1}^{N}\pi_ib_i(o_1)\beta_1(i)\)</span>。那么我们就可以求出<span class="math inline">\(P(O|\lambda)\)</span>的值了。</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/7.jpg" style="zoom:50%;"></p>
<h2 id="learning问题的求解">Learning问题的求解</h2>
<p>所谓的Learning问题，其实就是求<span class="math inline">\(\lambda=\mathop{argmax}\limits_{\lambda}P(O|\lambda)\)</span>。那么，直接求的话，我们会发现这个式子是没有解析解的。所以，在HMM模型中，采用Baum-Welch算法，也就是EM算法来解决这个问题。</p>
<h3 id="baum-welch算法em算法">Baum-Welch算法(EM算法)</h3>
<p>Baum-Welch算法推导如下：</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/8.jpg" style="zoom:67%;"></p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/11.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/10.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/9.jpg" style="zoom:50%;"></p>
<h2 id="decoding问题的求解">Decoding问题的求解</h2>
<p>所谓的decoding问题，就是：给定模型参数<span class="math inline">\(\lambda\)</span>与观测序列<span class="math inline">\(O\)</span>，求最大概率的状态变量<span class="math inline">\(I\)</span>。即：<span class="math inline">\(I=\mathop{argmax}\limits_{I}P(I|O)\)</span>。在HMM中，我们使用Viterbi算法来求解，实际上是一种动态规划的思想，非常简单。</p>
<h3 id="viterbi算法">Viterbi算法</h3>
<p>我们定义<span class="math inline">\(\delta_t(i)\)</span>为第t时刻状态为<span class="math inline">\(q_i\)</span>的所有单个路径中的概率最大值。公式如下： <span class="math display">\[
\delta_t(i)=\mathop{max}\limits_{i_1,i_2,i_3,...,i_{t-1}}P(i_t=q_i,i_1,...,i_{t-1},o_1,o_2,...,o_t|\lambda).
\]</span> 那么，很容易就能得到递推公式： <span class="math display">\[
\delta_{t+1}(i)=\mathop{max}\limits_{i_1,i_2,i_3,...,i_{t}}P(i_{t+1}=q_i,i_1,...,i_{t},o_1,o_2,...,o_{t+1}|\lambda)\\
=\mathop{max}\limits_{1&lt;=j&lt;=N}\delta_{t}(j)a_{ji}b_i(o_{t+1})
\]</span> 其中，<span class="math inline">\(i=1,2,...,N\)</span>。根据这个递推公式，我们很快就能得到所有路径中概率的最大值。那么怎么求得概率最大的路径呢？</p>
<p>我们定义<span class="math inline">\(\psi_t(i)\)</span>表示t时刻状态为你<span class="math inline">\(q_i\)</span>的所有单个路径中概率最大的路径的第<span class="math inline">\(t-1\)</span>个节点。公式如下： <span class="math display">\[
\psi_t(i)=\mathop{argmax}\limits_{1&lt;=j&lt;=N}\delta_{t-1}(j)a_{ji}
\]</span> 整理如下：</p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/12.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/23/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/13.jpg" style="zoom:50%;"></p>
<p>至此，HMM模型，理论部分就讲完了🎉～</p>
<h2 id="hmm模型的实现">HMM模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了python以及hmmlearn库这两种方式来实现HMM模型，我的github里面可以下在到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下</p>
<h3 id="使用hmmlearn库实现">使用hmmlearn库实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># E-mail:lzichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">hmmlearn中有两个模型：高斯HMM与多项式HMM，分别对应于：变量是连续的与离散的。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> hmmlearn <span class="keyword">import</span> hmm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 个盒子状态</span></span><br><span class="line">states = [<span class="string">'box1'</span>, <span class="string">'box2'</span>, <span class="string">'box3'</span>]</span><br><span class="line"><span class="comment"># 2 个球观察状态</span></span><br><span class="line">observations = [<span class="string">'red'</span>, <span class="string">'white'</span>]</span><br><span class="line"><span class="comment"># 初始化概率</span></span><br><span class="line">start_probability = np.array([<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>])</span><br><span class="line"><span class="comment"># 转移概率</span></span><br><span class="line">transition_probability = np.array([</span><br><span class="line">    [<span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.3</span>],</span><br><span class="line">    [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.2</span>],</span><br><span class="line">    [<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.5</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发射状态概率</span></span><br><span class="line">emission_probability = np.array([</span><br><span class="line">    [<span class="number">0.5</span>, <span class="number">0.5</span>],</span><br><span class="line">    [<span class="number">0.4</span>, <span class="number">0.6</span>],</span><br><span class="line">    [<span class="number">0.7</span>, <span class="number">0.3</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 建立模型，设置参数</span></span><br><span class="line">    model = hmm.MultinomialHMM(n_components=len(states))</span><br><span class="line">    model.startprob_ = start_probability</span><br><span class="line">    model.transmat_ = transition_probability</span><br><span class="line">    model.emissionprob_ = emission_probability</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测问题,执行Viterbi算法</span></span><br><span class="line">    seen = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    logprob, box = model.decode(seen.reshape(<span class="number">-1</span>, <span class="number">1</span>), algorithm=<span class="string">'viterbi'</span>)</span><br><span class="line">    print(<span class="string">'The ball picked:'</span>, <span class="string">','</span>.join(map(<span class="keyword">lambda</span> x: observations[x], seen)))</span><br><span class="line">    print(<span class="string">'The hidden box:'</span>, <span class="string">','</span>.join(map(<span class="keyword">lambda</span> x: states[x], box)))</span><br><span class="line"></span><br><span class="line">    box_pre = model.predict(seen.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    print(<span class="string">'The ball picked:'</span>, <span class="string">','</span>.join(map(<span class="keyword">lambda</span> x: observations[x], seen)))</span><br><span class="line">    print(<span class="string">'The hidden box predict:'</span>, <span class="string">','</span>.join(map(<span class="keyword">lambda</span> x: states[x], box_pre)))</span><br><span class="line">    <span class="comment"># 观测序列的概率计算问题</span></span><br><span class="line">    <span class="comment"># score函数返回的是以自然对数为底的对数概率值</span></span><br><span class="line">    <span class="comment"># ln0.13022≈−2.0385</span></span><br><span class="line">    print(model.score(seen.reshape(<span class="number">-1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"-------------------------------"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 学习问题</span></span><br><span class="line">    states = [<span class="string">"box1"</span>, <span class="string">"box2"</span>, <span class="string">"box3"</span>]</span><br><span class="line">    n_states = len(states)</span><br><span class="line"></span><br><span class="line">    observations = [<span class="string">"red"</span>, <span class="string">"white"</span>]</span><br><span class="line">    n_observations = len(observations)</span><br><span class="line"></span><br><span class="line">    model = hmm.MultinomialHMM(n_components=n_states)</span><br><span class="line">    <span class="comment"># 三个观测序列，用来训练</span></span><br><span class="line">    X = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">    model.fit(X)  <span class="comment"># 训练模型</span></span><br><span class="line">    print(model.startprob_)  <span class="comment"># 得到初始概率矩阵</span></span><br><span class="line">    print(model.transmat_)  <span class="comment"># 得到状态转移矩阵</span></span><br><span class="line">    print(model.emissionprob_)  <span class="comment"># 得到观测概率分布</span></span><br><span class="line">    <span class="comment"># 概率计算</span></span><br><span class="line">    print(model.score(X))</span><br></pre></td></tr></table></figure>
<h3 id="python实现">Python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf8</span></span><br><span class="line"><span class="comment"># Author:codeiwithzichao</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N, M)</span>:</span></span><br><span class="line">        self.A = np.zeros((N, N))  <span class="comment"># 状态转移概率矩阵</span></span><br><span class="line">        self.B = np.zeros((N, M))  <span class="comment"># 观测概率矩阵</span></span><br><span class="line">        self.Pi = np.array([<span class="number">1.0</span> / N] * N)  <span class="comment"># 初始状态概率矩阵</span></span><br><span class="line"></span><br><span class="line">        self.N = N  <span class="comment"># 可能的状态数</span></span><br><span class="line">        self.M = M  <span class="comment"># 可能的观测数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_probality</span><span class="params">(self, O)</span>:</span></span><br><span class="line">        self.T = len(O)</span><br><span class="line">        self.O = O</span><br><span class="line"></span><br><span class="line">        self.forward()</span><br><span class="line">        <span class="keyword">return</span> sum(self.alpha[self.T - <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        前向算法</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.alpha = np.zeros((self.T, self.N))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 公式 10.15</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">            self.alpha[<span class="number">0</span>][i] = self.Pi[i] * self.B[i][self.O[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 公式10.16</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, self.T):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">                sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">                    sum += self.alpha[t - <span class="number">1</span>][j] * self.A[j][i]</span><br><span class="line">                self.alpha[t][i] = sum * self.B[i][self.O[t]]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        后向算法</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.beta = np.zeros((self.T, self.N))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 公式10.19</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">            self.beta[self.T - <span class="number">1</span>][i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 公式10.20</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">                    self.beta[t][i] += self.A[i][j] * self.B[j][self.O[t + <span class="number">1</span>]] * self.beta[t + <span class="number">1</span>][j]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_gamma</span><span class="params">(self, i, t)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        公式 10.24</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        numerator = self.alpha[t][i] * self.beta[t][i]</span><br><span class="line">        denominator = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">            denominator += self.alpha[t][j] * self.beta[t][j]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> numerator / denominator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_ksi</span><span class="params">(self, i, j, t)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        公式 10.26</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        numerator = self.alpha[t][i] * self.A[i][j] * self.B[j][self.O[t + <span class="number">1</span>]] * self.beta[t + <span class="number">1</span>][j]</span><br><span class="line">        denominator = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">                denominator += self.alpha[t][i] * self.A[i][j] * self.B[j][self.O[t + <span class="number">1</span>]] * self.beta[t + <span class="number">1</span>][j]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> numerator / denominator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        随机生成 A，B，Pi</span></span><br><span class="line"><span class="string">        并保证每行相加等于 1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">import</span> random</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">            randomlist = [random.randint(<span class="number">0</span>, <span class="number">100</span>) <span class="keyword">for</span> t <span class="keyword">in</span> range(self.N)]</span><br><span class="line">            Sum = sum(randomlist)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">                self.A[i][j] = randomlist[j] / Sum</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">            randomlist = [random.randint(<span class="number">0</span>, <span class="number">100</span>) <span class="keyword">for</span> t <span class="keyword">in</span> range(self.M)]</span><br><span class="line">            Sum = sum(randomlist)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.M):</span><br><span class="line">                self.B[i][j] = randomlist[j] / Sum</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, O, MaxSteps=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.T = len(O)</span><br><span class="line">        self.O = O</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        self.init()</span><br><span class="line"></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 递推</span></span><br><span class="line">        <span class="keyword">while</span> step &lt; MaxSteps:</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">            print(step)</span><br><span class="line">            tmp_A = np.zeros((self.N, self.N))</span><br><span class="line">            tmp_B = np.zeros((self.N, self.M))</span><br><span class="line">            tmp_pi = np.array([<span class="number">0.0</span>] * self.N)</span><br><span class="line"></span><br><span class="line">            self.forward()</span><br><span class="line">            self.backward()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># a_&#123;ij&#125;</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">                    numerator = <span class="number">0.0</span></span><br><span class="line">                    denominator = <span class="number">0.0</span></span><br><span class="line">                    <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">                        numerator += self.cal_ksi(i, j, t)</span><br><span class="line">                        denominator += self.cal_gamma(i, t)</span><br><span class="line">                    tmp_A[i][j] = numerator / denominator</span><br><span class="line"></span><br><span class="line">            <span class="comment"># b_&#123;jk&#125;</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.N):</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(self.M):</span><br><span class="line">                    numerator = <span class="number">0.0</span></span><br><span class="line">                    denominator = <span class="number">0.0</span></span><br><span class="line">                    <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T):</span><br><span class="line">                        <span class="keyword">if</span> k == self.O[t]:</span><br><span class="line">                            numerator += self.cal_gamma(j, t)</span><br><span class="line">                        denominator += self.cal_gamma(j, t)</span><br><span class="line">                    tmp_B[j][k] = numerator / denominator</span><br><span class="line"></span><br><span class="line">            <span class="comment"># pi_i</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">                tmp_pi[i] = self.cal_gamma(i, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            self.A = tmp_A</span><br><span class="line">            self.B = tmp_B</span><br><span class="line">            self.Pi = tmp_pi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(self, length)</span>:</span></span><br><span class="line">        <span class="keyword">import</span> random</span><br><span class="line">        I = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># start</span></span><br><span class="line">        ran = random.randint(<span class="number">0</span>, <span class="number">1000</span>) / <span class="number">1000.0</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> self.Pi[i] &lt; ran <span class="keyword">or</span> self.Pi[i] &lt; <span class="number">0.0001</span>:</span><br><span class="line">            ran -= self.Pi[i]</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        I.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成状态序列</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length):</span><br><span class="line">            last = I[<span class="number">-1</span>]</span><br><span class="line">            ran = random.randint(<span class="number">0</span>, <span class="number">1000</span>) / <span class="number">1000.0</span></span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> self.A[last][i] &lt; ran <span class="keyword">or</span> self.A[last][i] &lt; <span class="number">0.0001</span>:</span><br><span class="line">                ran -= self.A[last][i]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            I.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成观测序列</span></span><br><span class="line">        Y = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            k = <span class="number">0</span></span><br><span class="line">            ran = random.randint(<span class="number">0</span>, <span class="number">1000</span>) / <span class="number">1000.0</span></span><br><span class="line">            <span class="keyword">while</span> self.B[I[i]][k] &lt; ran <span class="keyword">or</span> self.B[I[i]][k] &lt; <span class="number">0.0001</span>:</span><br><span class="line">                ran -= self.B[I[i]][k]</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line">            Y.append(k)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triangle</span><span class="params">(length)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    三角波</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    X = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(length)]</span><br><span class="line">    Y = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">        x = x % <span class="number">6</span></span><br><span class="line">        <span class="keyword">if</span> x &lt;= <span class="number">3</span>:</span><br><span class="line">            Y.append(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y.append(<span class="number">6</span> - x)</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sin</span><span class="params">(length)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    三角波</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">import</span> math</span><br><span class="line">    X = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(length)]</span><br><span class="line">    Y = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">        x = x % <span class="number">20</span></span><br><span class="line">        Y.append(int(math.sin((x * math.pi) / <span class="number">10</span>) * <span class="number">50</span>) + <span class="number">50</span>)</span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_data</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    plt.plot(x, y, <span class="string">'g'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">     hmm = HMM(<span class="number">15</span>,<span class="number">101</span>)</span><br><span class="line">     sin_x, sin_y = sin(<span class="number">40</span>)</span><br><span class="line">     show_data(sin_x, sin_y)</span><br><span class="line">     hmm.train(sin_y)</span><br><span class="line">     y = hmm.generate(<span class="number">100</span>)</span><br><span class="line">     x = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>)]</span><br><span class="line">     show_data(x,y)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>HMM</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|K近邻算法原理详解与实现</title>
    <url>/2020/02/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BD%9C%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>KNN算法是一种较为简单的分类算法（也可用于回归问题），其可以实现多分类问题。本篇博客将详细地讲解KNN算法，并采用python与scikit-learn库两种方式，对KNN算法进行实现。</p>
<a id="more"></a>
<h2 id="knn算法介绍">KNN算法介绍</h2>
<p>KNN算法是比较简单的。它的整个应用过程如下：</p>
<p>输入：给定训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)</span>，其中，<span class="math inline">\(x_i=\{x_i^{(1)},x_i^{(2)},...,x_i^{(n)}\}\)</span>，即每一个特征向量都有n维，<span class="math inline">\(y_i\in \{c_1,c_2,...,c_k\}\)</span>，即总共有<span class="math inline">\(k\)</span>个标签类别；待预测实例<span class="math inline">\(x\)</span>。</p>
<p>输出：输出实例<span class="math inline">\(x\)</span>所属的标签类别<span class="math inline">\(y\)</span>。</p>
<ul>
<li>计算实例<span class="math inline">\(x\)</span>与所有训练集实例的距离，并从中选取<span class="math inline">\(K\)</span>个最近的实例，这<span class="math inline">\(K\)</span>个点所组成的区域叫做<span class="math inline">\(x\)</span>的邻域。</li>
<li>根据分类决策规则，确定<span class="math inline">\(x\)</span>所属的标签类别<span class="math inline">\(y\)</span>。</li>
</ul>
<p>是不是很简单？🤩从上述过程中，我们可以看到，KNN算法中，最重要的三个要素是：<strong>K值的选择、距离的计算、分类决策规则</strong>。下面就讲一一介绍。</p>
<h2 id="k值的选择">K值的选择</h2>
<p>K值的选择，可以说是整个模型的核心。但是在实际上，并没有具体的公式精确地告诉我们，K值的选择。不过我们可以来分析一下：</p>
<ul>
<li>当K值非常大的时候(譬如K=N)，我们会发现，不管输入的<span class="math inline">\(x\)</span>是什么，结果都不会变，都会偏向于训练集中大多数的那个标签类别，模型过于简单。</li>
<li>当K值非常小的时候(譬如K=1)，那么，只有与输入实例相近的点才会起作用，那么如果恰巧是噪声的话，就会预测错误，所以，我们可以看到，K值太小的话，模型会变得复杂，容易发生过拟合。</li>
</ul>
<p>所以，在实际中，会让K值取一个适中的值，或者采用交叉验证的方式来选取最优的K值。</p>
<h2 id="距离的计算">距离的计算</h2>
<p>在KNN算法中，一般我们会使用欧式距离，但是也有其他的距离，具体如下： <span class="math display">\[
{\text{欧式距离}}\ \ \ \ \  L_2(x_i,x_j)=(\sum_{k=1}^{n}|x_i^{(k)}-x_j^{(k)}|^2)^{\frac 12}\\
{\text{曼哈顿距离}}\ \ \ \ \ \ L_1(x_i,x_j)=(\sum_{k=1}^{n}|x_i^{(k)}-x_j^{(k)}|)\\
{\text{切比雪夫距离}}\ \ \ \ \ \ L_\infty(x_i,x_j)=\mathop{max}\limits_{k=1}^{n}|x_i^{(k)}-x_j^{(k)}|
\]</span> 在KNN中，使用不同的距离度量，所得到的最近邻点是不一样的。</p>
<h2 id="分类决策规则">分类决策规则</h2>
<p>在KNN中，分类决策规则一般都是多数表决。即：<strong>由K个实例中的多数标签类别决定。</strong>使用公式表达如下： <span class="math display">\[
y=argmax_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j)
\]</span> 其中，<span class="math inline">\(I\)</span>是指示函数。</p>
<h2 id="kd-tree">kd tree</h2>
<p>其实，介绍完三要素后，KNN算法算是完成了。但是我们来思考一下KNN算法。我们每预测一个实例，都需要计算其与训练集所有实例的距离，并且从中取出K个实例。如果说，数据是高维的，并且训练集非常的大(mnist训练数据集中，有60000个)，那么计算就会变得非常的慢。如果说测试集也非常大的话，那么所花费的时候就会更长。为了解决这个问题，就有人提出了kdtree。<strong>注意：这里的k说的是对k维空间进行划分，与K值不是一个概念！</strong></p>
<p><strong>所谓的kdtree，就是将特征空间划分为多个区域，如果说一些区域离待预测的实例的距离太远，那么就可以放弃掉这些区域，这样就大大减小了比较次数，提高了计算效率。</strong></p>
<p>那我们具体来剖析一下kdtree。它其实是BST的变体。BST，相信只要学过数据结构，应该都非常熟悉。<strong>BST的左子树都比起父节点的值要小，其右子树都比父节点的值要大。</strong>但是在BST中，所有节点都是一维数据，而在，kdtree中，所有节点都是多维数据。下面举个例子来说明kdtree是怎么工作的(《统计学习方法》中其实并没有讲地特别清楚😩)：</p>
<p><img src="/2020/02/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BD%9C%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg"></p>
<p>譬如说有集合<span class="math inline">\(\{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)\}\)</span>。那么kdtree构建过程如下：</p>
<ul>
<li>构建根节点时，此时的切分维度为<span class="math inline">\(x\)</span>，集合在x维从小到大排序为<span class="math inline">\((2,3)，(4,7)，(5,4)，(7,2)，(8,1)，(9,6)\)</span>；它的中值为(7,2)。**注意：2,4,5,7,8,9在数学中的中值为(5 + 7)/2=6，但因该算法的中值需在点集合之内，所以中值计算用的是 $len(points)//2=3, points[3]=(7,2) <span class="math inline">\(）。**所以，\)</span>(2,3)，(4,7)，(5,4)<span class="math inline">\(挂在\)</span>(7,2)<span class="math inline">\(节点的左子树，\)</span>(8,1)，(9,6)<span class="math inline">\(挂在\)</span>(7,2)$节点的右子树。</li>
<li>构建<span class="math inline">\((7,2)\)</span>节点的左子树时，点集合<span class="math inline">\((2,3)，(4,7)[\)</span>]()，(5,4)此时的切分维度为y，中值为<span class="math inline">\((5,4)\)</span>作为分割平面，<span class="math inline">\((2,3)\)</span>挂在其左子树，<span class="math inline">\((4,7)\)</span>挂在其右子树。</li>
<li>构建<span class="math inline">\((7,2)\)</span>节点的右子树时，点集合<span class="math inline">\((8,1)，(9,6)\)</span>此时的切分维度也为y，中值为<span class="math inline">\((9,6)\)</span>作为分割平面，<span class="math inline">\((8,1)\)</span>挂在其左子树。至此kd tree构建完成。使用二维空间画出来的话，如下：</li>
</ul>
<p><img src="/2020/02/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BD%9C%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg"></p>
<p>OK，当构建完kdtree之后，那么<strong>如何对其进行搜索呢？</strong></p>
<ul>
<li><p>我们来查找点<span class="math inline">\((2.1,3.1)\)</span>，在<span class="math inline">\((7,2)\)</span>点测试到达<span class="math inline">\((5,4)\)</span>，在<span class="math inline">\((5,4)\)</span>点测试到达<span class="math inline">\((2,3)\)</span>，然后search_path中的结点为&lt;(7,2), (5,4), (2,3)&gt;，从search_path中取出<span class="math inline">\((2,3)\)</span>作为当前最佳结点nearest, dist为<span class="math inline">\(0.141\)</span>；</p></li>
<li><p>然后回溯至<span class="math inline">\((5,4)\)</span>，以<span class="math inline">\((2.1,3.1)\)</span>为圆心，以<span class="math inline">\(dist=0.141\)</span>为半径画一个圆，并不和超平面<span class="math inline">\(y=4\)</span>相交，所以不必跳到结点<span class="math inline">\((5,4)\)</span>的右子空间去搜索，因为右子空间中不可能有更近样本点了。</p></li>
<li><p>于是在回溯至<span class="math inline">\((7,2)\)</span>，同理，以<span class="math inline">\((2.1,3.1)\)</span>为圆心，以<span class="math inline">\(dist=0.141\)</span>为半径画一个圆并不和超平面<span class="math inline">\(x=7\)</span>相交，所以也不用跳到结点<span class="math inline">\((7,2)\)</span>的右子空间去搜索。</p></li>
<li><p>至此，search_path为空，结束整个搜索，返回nearest<span class="math inline">\((2,3)\)</span>作为<span class="math inline">\((2.1,3.1)\)</span>的最近邻点，最近距离为0.141。</p></li>
</ul>
<p><strong>注意：上述是最近邻的过程，得出一个最靠近待预测实例的点，如果是K近邻的话，同样地，我们可以通过这样的方法，来得到K个最近邻的点。</strong>至此，KNN算法的理论部分就讲完啦🎉</p>
<h2 id="knn算法的实现">KNN算法的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现KNN模型：python实现以及调用scikit-learn库来实现。我的github里面可以下载到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#Author:codewithzichao</span></span><br><span class="line"><span class="comment">#E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist数据集(只使用了1000来训练，只使用了1000来测试。)</span></span><br><span class="line"><span class="string">结果(准确率)：0.738</span></span><br><span class="line"><span class="string">时间：28.6643168926239</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName: 数据路径</span></span><br><span class="line"><span class="string">    :return: 返回特征向量与标签类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list=[]</span><br><span class="line">    label_list=[]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline=line.strip().split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">            data_list.append([int(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line">            label_list.append(int(curline[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">        data_matrix=np.array(data_list)</span><br><span class="line">        label_matrix=np.array(label_list)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data_matrix,label_matrix</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,train_data,train_label,K)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        :param train_data: 训练集的特征向量</span></span><br><span class="line"><span class="string">        :param train_label: 训练集的类别向量</span></span><br><span class="line"><span class="string">        :param K: 指定的K值</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.train_data=train_data</span><br><span class="line">        self.train_label=train_label</span><br><span class="line">        self.input_num=self.train_data.shape[<span class="number">0</span>]</span><br><span class="line">        self.feature=self.train_data.shape[<span class="number">1</span>]</span><br><span class="line">        self.K=K</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_distance</span><span class="params">(self,x1,x2)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算两个样本之间的距离，使用欧式距离</span></span><br><span class="line"><span class="string">        :param x1: 第一个样本</span></span><br><span class="line"><span class="string">        :param x2: 第二步样本</span></span><br><span class="line"><span class="string">        :return: 样本之间的距离</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> np.sqrt(np.sum(np.square(x1-x2)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_K</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        dist_group=np.zeros(self.input_num)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_num):</span><br><span class="line">            x1=self.train_data[i]</span><br><span class="line">            dist=self.cal_distance(x,x1)</span><br><span class="line">            dist_group[i]=dist</span><br><span class="line"></span><br><span class="line">        topK=np.argsort(dist_group)[:self.K]<span class="comment">#升序排序</span></span><br><span class="line"></span><br><span class="line">        labeldist=np.zeros(<span class="number">10</span>)<span class="comment">#10个标签，在每一个标签对应的位置上加1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(topK)):</span><br><span class="line">            labeldist[int(self.train_label[topK[i]])]+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.argmax(labeldist)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self,test_data,test_label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        在测试集上测试</span></span><br><span class="line"><span class="string">        :param test_data: 测试集的特征向量</span></span><br><span class="line"><span class="string">        :param test_label: 测试集的标签向量</span></span><br><span class="line"><span class="string">        :return: 准确率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        error=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">        test_num=test_data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(test_num):</span><br><span class="line">            print(<span class="string">f"the current sample is <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>,the total samples is<span class="subst">&#123;test_num&#125;</span>."</span>)</span><br><span class="line">            x=test_data[i]</span><br><span class="line">            y=self.get_K(x)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(y!=test_label[i]):</span><br><span class="line">                error+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">        accuracy=(test_num-error)/test_num</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    start=time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start load data."</span>)</span><br><span class="line">    train_data,train_label=loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data,test_label=loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    a=KNN(train_data[:<span class="number">1000</span>],train_label[:<span class="number">1000</span>],<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"finished training."</span>)</span><br><span class="line"></span><br><span class="line">    accuracy=a.test(test_data[:<span class="number">1000</span>],test_label[:<span class="number">1000</span>])</span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    end=time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end-start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="scikit-learn实现">scikit-learn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#Author:codewithzichao</span></span><br><span class="line"><span class="comment">#E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist数据集(只使用了1000来训练，只使用了1000来测试。)</span></span><br><span class="line"><span class="string">结果(准确率)：0.799</span></span><br><span class="line"><span class="string">时间：16.832828998565674</span></span><br><span class="line"><span class="string">---------------------------</span></span><br><span class="line"><span class="string">果然，自己写的python没有编写kdtree等部分，效果与时间上都比不上sklearn。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName: 数据路径</span></span><br><span class="line"><span class="string">    :return: 返回特征向量与标签类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list=[]</span><br><span class="line">    label_list=[]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline=line.strip().split(<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">            data_list.append([int(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line">            label_list.append(int(curline[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">        data_matrix=np.array(data_list)</span><br><span class="line">        label_matrix=np.array(label_list)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data_matrix,label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start load data."</span>)</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    knn=KNeighborsClassifier(n_neighbors=<span class="number">10</span>)</span><br><span class="line">    knn.fit(train_data[:<span class="number">1000</span>],train_label[:<span class="number">1000</span>])</span><br><span class="line"></span><br><span class="line">    prediction=knn.predict(test_data[:<span class="number">1000</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        print(<span class="string">f"predict is <span class="subst">&#123;prediction[i]&#125;</span>,the true is <span class="subst">&#123;test_label[i]&#125;</span>."</span>)</span><br><span class="line">    accuracy=knn.score(test_data[:<span class="number">1000</span>],test_label[:<span class="number">1000</span>])</span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    end=time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end-start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|Cross-Lingual NER</title>
    <url>/2020/10/19/NLP-Cross-Lingual-NER/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要讲解一下cross-lingual NER，解读今年MSRA发表的三篇论文：《Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources》、《Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language》、《UniTrans : Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data》，以及附带的《GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition》。</p>
<a id="more"></a>
<h2 id="enhanced-meta-learning-for-cross-lingual-named-entity-recognition-with-minimal-resourcesaaai2020">Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources(AAAI2020)</h2>
<h3 id="background">background</h3>
<p>当我们想要在target language上进行NER任务的时候，但是如果没有足够的labled data的话，cross-lingual NER是一种有效的方式。所谓的cross-lingual NER指的是，我们有labeled source language data与unlabeled target language data，我们希望模型在labeled source language上学习到的知识能够迁移给unlabeled target language，从而使得target language data在没有label的情况下，也能够取得很好的结果。目前已有的cross-lingual NER model主要分为两种方式：<strong>methods based on direct transfer 与methods based on annotation projection</strong>。</p>
<ul>
<li>所谓的methods based on direct transfer指的是：我们使用source language data去训练一个NER model，然后直接在target language data上进行test，这种方法的关键在于：怎么得到并在模型中融入一些language-independent feature，常用的是：cross-lingual word embedding，word cluster等等。</li>
<li>所谓的methods based on annotation projection指的是：我们将source language data在word level/phrase level上给他翻译成target language data，当然label也是copy过去，然后模型使用翻译后的target language data进行训练，最后在原有unlabeled target language data进行test。</li>
</ul>
<p>本篇paper提出的方法属于第一种，因为作者认为这种方法还有提升的空间。为什么呢？因为最近有paper显示：<strong>通过建立一个cross-lingual encoder，任何句子都能够被encode到同一个feature space</strong>。另外，在mBERT中，还给出了这样一个结论：通过mBERT，我们可以通过简单的余弦相似度来计算不同语言的句子的similarity，这样的话，给定一个target language example，我们就能够在source language data中召回出一些与其在结构/语义上相似的句子。所以在正式对unlabeled language data进行test之前，我们是不是可以在source language data中找到与给定的unlabeled language data中相似的那些sample，从而使用这些sample来对模型进行fine-tuning，来提升效果？答案是肯定的！此外，召回的example会是一个比较少的数目，从而避免引入额外的噪音，伤害模型的性能。到此，cross-lingual NER任务就转化为了：<strong>模型需要在一个小数目的训练集上进行训练，并在新的target language上取得比较好的结果。</strong>这就很自然地引入了meta learning。</p>
<h3 id="model">model</h3>
<p>首先简单的介绍一下什么是meta learning。所谓的meta learning，也叫做LTL(学会学习)，它适用于：模型只需要使用少量数据进行训练，就能够快速在适用于一个新的任务，<strong>它强调的是fast adaptation。</strong>目前常用的meta learning算法，主要分为三类：leanrning a metric space 、learning a good parameters initialization、learning an optimizer，本篇paper采用的MAML算法属于第二类。OK，点到为止，下面将讲解如何利用meta learning来进行cross-lingual NER任务。</p>
<ol type="1">
<li><p><strong>问题定义</strong></p>
<p>我们把labeled source language data记做： <span class="math inline">\(D^S_{train}=\{x^{(i)}\}_{i=1}^{N}\)</span> ，unlabeled target language data记做：<span class="math inline">\(D^T_{test}=\{x^{(j)}\}_{j=1}^{M}\)</span>。我们的目标是：希望使用<span class="math inline">\(D^S_{train}\)</span>来训练一个模型，从而让模型能够在<span class="math inline">\(D^S_{test}\)</span>有着很好的效果。</p></li>
<li><p><strong>Base model：</strong>使用预训练的mBERT，同时添加一个softmax用于token解码，损失函数采用CE。</p></li>
<li><p><strong>enhanced meta learning approach：</strong>在这篇paper里，使用MAML算法来进行meta learning。不了解的MAML算法请看这个<a href="https://zhuanlan.zhihu.com/p/57864886" target="_blank" rel="noopener">link</a>。整个算法其实分为三部分：task的构建、meta-training(在MAML中就是预训练阶段)与adaptation(在MAML中就是微调阶段)。</p>
<ol type="1">
<li><p><strong>task的构建</strong></p>
<p>在执行MAML算法之前，我们需要构建task，因为在MAML中，一个task就相当于一个样本。对于<span class="math inline">\(D^S_{train}\)</span>中每一个样本<span class="math inline">\(x^{(i)}\)</span>，我们将其本身当作meta task <span class="math inline">\(\tau_i\)</span>的测试集<span class="math inline">\(D^{\tau_i}_{test}\)</span>(其实就是query set)，从source language data召回的与<span class="math inline">\(x^{(i)}\)</span>最相似的样本的集合作为meta task <span class="math inline">\(\tau_i\)</span>的训练集<span class="math inline">\(D^{\tau_i}_{train}\)</span>(其实就是support set)，所以每一个meta task <span class="math inline">\(\tau_i\)</span>可以表示为： <span class="math display">\[
\tau_i=(D^{\tau_i}_{test},D^{\tau_i}_{test}),i\in 1,2,3,4,5,...,N
\]</span> 具体怎么召回呢？其实就是通过mBERT得到<span class="math inline">\(x^{(i)}\)</span>的sentence presentation，然后计算<span class="math inline">\(x^{(i)}\)</span>与<span class="math inline">\(x^{(j)}\)</span>的余弦相似度，选取最相似的K个样本即可。</p></li>
<li><p><strong>meta-training</strong></p>
<p>这一步可以看作是预训练，模型在多个不同task进行训练，从而能够让模型获得很强的泛化能力，之后在目标task当中，我们只需要fine-tuning很少的次数，就可以得到很好的结果，也就是<strong>fast adaption</strong>。这一步核心的东西叫做：<strong>gradient by gradient</strong>。具体就是：我们随机sample一些task作为一个batch来训练模型的参数<span class="math inline">\(\theta\)</span>，在这一个batch的参数更新的时候，是一个样本一个样本来进行参数更新的，等到一个batch结束后，我们就得到了一个新的参数<span class="math inline">\(\theta^{&#39;}\)</span>，然后用<span class="math inline">\(\theta^{&#39;}\)</span>来更新最原始的参数<span class="math inline">\(\theta\)</span>。当然了，预训练可以执行很多次。</p></li>
<li><p><strong>adaptation</strong></p>
<p>这一步是模型在target language data上进行微调。当然了，具体的task的构建方法也是第一步说的一样，不过使用的样本全部来自于target language data。<strong>值得注意的是，这一步中我们只有一次的参数更新。</strong>整体的架构图如下：</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/meta-learning.jpg"></p>
<p>当然了，由于不同语言的句子之间的对齐很重要，为了让模型能够学习到这种对齐，model还在token-level上对entity进行了mask。还有一个max loss的操作，具体就是：传统的CE loss，它对每一个token 的loss都是平等对待，但是这是不对的，不同的token它对整体的loss贡献是不一样的，所以我们可以减去所有token loss中最大值，从而让那些loss 比较高的token能够学习的更加充分。当然在本paper里没有使用，因为本来训练数据就少，没有必要，而且这样搞很容易过度学习了，从而伤害模型的性能。</p></li>
</ol></li>
</ol>
<h3 id="experiment">experiment</h3>
<ol type="1">
<li><p>数据集：CoNLL-2002/2003、Europeana Newspapers、MSRA，en作为source language，其他语言作为target language。</p></li>
<li><p>实验结果</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/meta-learning%20result.jpg"></p>
<p>从结果来看，还是不错的。个人认为这种思路还是蛮新颖的。</p></li>
</ol>
<h2 id="single-multi-source-cross-lingual-ner-via-teacher-student-learning-on-unlabeled-data-in-target-languageacl2020">Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language(ACL2020)</h2>
<h3 id="background-1">background</h3>
<p>这篇论文假设的任务更加的严苛：之前的cross-lingual NER都是有可用的labeled source language data，那如果没有可用的labeled source language data，要怎么样才能在unlabeled target language data上取得好的效果呢？它首先对在cross-lingual NER上两种方法进行了对比：</p>
<ul>
<li><strong>methods based on direct transfer</strong>缺点的在于：没有很好地利用unlabeled target language data，并且模型效果非常依赖language-independent feature；</li>
<li><strong>methods based on annotation projection</strong>的缺点在于：需要parallel text of target language data，一般都是通过对source language data进行翻译得到，这样以来，难免会引入噪音，对最终模型的性能存在损害，同时这种方法对于zero-source cross-lingual NER的情况不适用。</li>
</ul>
<p>对两种方法进行分析之后，<strong>我们希望构建的模型，能够处理zero-source cross-lingual NER这种情况，同时能够很好地利用unlabeled target language data中的信息。</strong>具体的做法是：采用teacher-student learning(借鉴了知识蒸馏)。</p>
<h3 id="model-1">model</h3>
<p>整个模型分为两大情况：single source cross-lingual NER和multi-source cross-lingual NER，都是采用teacher-student learning的方式来进行训练。</p>
<ol type="1">
<li><p><strong>single source cross-lingual NER</strong></p>
<ol type="1">
<li><p>training：对于single source cross-lingual NER，我们把在source language上训练好的模型作为teacher model。具体在这篇paper里面，选取mBERT作为teacher model，student model可以与teacher model一样的结构，也可以是不一样的结构。我们把使用后unlabeled target language data去训练student model，让其去模拟teacher model输出的实体标签的分布。损失函数采用MSE，解码使用softmax。<strong>注意，训练过程中，我们不去更新teacher model的参数。</strong>模型图如下：</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/single-source%20model.jpg"></p></li>
<li><p>inference：我们只使用srtudent model 来进行inference，解码仍然采用softmax。</p></li>
</ol></li>
<li><p><strong>multi-source cross-lingual NER</strong></p>
<ol type="1">
<li><p>training：对于multi-source cross-lingual NER，我们把K个在source language data上训练好的模型当作teacher model(注意，共有K个model，每一个模型在一种source language上训练过)，sutdent model的结构仍然可以相同或者不同。但是与single source cross-lingual NER不同的在于，student model是要拟合teacher model的输出的分布，而teacher model有多个，需要对多个teacher models进行融合，实际上就是多个teacher model的输出的概率分布进行加权融合，权重就是不同teacher models的相对重要性。公式如下： <span class="math display">\[
\tilde p(x_i^{&#39;},\theta_T)=\sum_{k=1}^{K}\alpha_k\tilde p(x_i^{&#39;},\theta_T^{(k)})
\]</span> 其中，<span class="math inline">\(\tilde p(x_i^{&#39;},\theta_T^{(k)})\)</span>表示第<span class="math inline">\(k\)</span>个teacher model的输出概率分布，<span class="math inline">\(\alpha_k\)</span>表示第<span class="math inline">\(k\)</span>个teacher model的权重。关键是<span class="math inline">\(\alpha_k\)</span>怎么计算呢？在这篇paper中，设计一个language identiﬁcation auxiliary task，来计算<span class="math inline">\(\alpha_k\)</span>。</p>
<p><strong>language identiﬁcation auxiliary task</strong></p>
<p>这个任务通过学习得到不同language的language embedding，来计算source language与target language的相似度，从而给各个teacher models分配权重。假设第k种source language的数据集表示为：<span class="math inline">\(D^{(k)}_{src}=\{(u^{(k)},k)\}\)</span>，我们要做的是：去学习所有的 source language embedding vector：<span class="math inline">\(\mu^{(k)}\in R^m，k\in 1,2,3,..,K\)</span>。具体公式如下： <span class="math display">\[
{\cal L}(P,M)=-\frac{1}{Z}\sum_{(u^{(k)},k)\in D_{src}}CE(softmax(g^T(u)MP),k)+\gamma||PP^T-I||_F^2
\]</span> 其中，<span class="math inline">\(P\in R^{m\times K}=\{\mu^{(1)},\mu^{(2)},...,\mu^{(K)}\}\)</span>，K表示K种语言，m表示其embedding dimension，<span class="math inline">\(g^T(u)\)</span>表示得到<span class="math inline">\(u\)</span>的sentence embedding，<span class="math inline">\(k\)</span>我个人觉得是一个one-hot vector，要不然不对劲。得到<span class="math inline">\(P\)</span>之后，我们就可以计算<span class="math inline">\(\alpha\)</span>了，如下： <span class="math display">\[
\alpha_k=\frac{1}{|D_{tgt}|}\sum_{x^{&#39;}\in D_{tgt}}\frac{exp(g^T(x^{&#39;})M\mu^{(k)}/\tau)}{\sum_{i=1}^Kexp(g^T(x^{&#39;})M\mu^{(i)}/\tau)}
\]</span> 其中，<span class="math inline">\(|D_{tgt}|\)</span>表示target language data中的样本数目，<span class="math inline">\(\tau\)</span>设置为所有的<span class="math inline">\(g^T(x^{&#39;})M\mu^{(k)}/\tau)\)</span>的方差，从而保证得到的<span class="math inline">\(\alpha\)</span>不会为0也不会为1。总的来看，这个任务还是蛮简单的，但是很work。值得一提的是，<strong>在实际实现的过程中，对<span class="math inline">\(M\)</span>使用了低秩近似，从而减少参数量，加快训练。</strong></p></li>
<li><p>inference：还是只使用student model来进行inference，解码使用softmax。</p></li>
</ol></li>
</ol>
<h3 id="experiment-1">experiment</h3>
<ol type="1">
<li><p>数据集：CoNLL-2002、CoNLL-2003</p></li>
<li><p>hypoparameters的设置参考原始论文</p></li>
<li><p>实验结果</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/single-result.jpg" style="zoom:50%;"></p></li>
</ol>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/multi-result.jpg"></p>
<p>从结果来看，效果是惊人的，在single source cross-lingual NER中，仅仅使用unlabeled target language data，就超越了meta learning那篇的结果；对于multi-source cross-lingual NER中，效果更是大幅提高，说明teacher-student learning这种方法有着很大的潜力。之后可以再在这上面做一些探索。</p>
<h2 id="unitrans-unifying-model-transfer-and-data-transfer-for-cross-lingual-named-entity-recognition-with-unlabeled-dataijcai2020">UniTrans : Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data(IJCAI2020)</h2>
<h3 id="background-2">background</h3>
<p>这篇同样也是研究zero-resource cross-lingual NER。目前在cross-lingual NER常用的两种方法的优缺点在上面应提到过了，这里不再赘述。这篇paper希望能够对两种方法进行结合，从而充分利用两者的优点，同时避免其缺点。所以就提出了unitrans模型。额外说一句，这篇paper应该是zero-resource cross-lingual NER的SOTA，结果大幅超越之前所讲解的两篇paper，非常优秀的论文，值得一读。</p>
<h3 id="model-2">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/unitrans.jpg"></p>
<p>unitrans整体的架构是：1.我们首先确定一个base model，然后使用source language training data <span class="math inline">\(D_{src}\)</span>来训练base model得到新的参数为<span class="math inline">\(\theta_{src}\)</span> 的 model；对source language data通过word-to-word translation来得到翻译后的target language data <span class="math inline">\(D_{trans}\)</span>，基于<span class="math inline">\(\theta_{src}\)</span>,使用翻译后的<span class="math inline">\(D_{trans}\)</span>来对参数为<span class="math inline">\(\theta_{src}\)</span>的NER model进行fine-tune，并将fine-tuning后的NER model作为teacher model(参数：<span class="math inline">\(\theta_{teach}\)</span>)；除此之外，我们使用<span class="math inline">\(D_{trans}\)</span>来训练base model，得到参数为<span class="math inline">\(\theta_{trans}\)</span>的模型(不基于<span class="math inline">\(\theta_{src}\)</span>)；2.我们得到三个模型（<span class="math inline">\(\theta_{src}、\theta_{teach}、\theta_{trans}\)</span>）之后，通过设计一种voting机制，去得到unlabeled target language data的pseudo label，然后使用pseudo label来训练student model。下面依次讲解其中重要的compoment。</p>
<p><strong>base model：</strong>在这篇paper中，就是一个mBERT+softmax，loss就是entity-level的ce loss。</p>
<p><strong>word-to-word translation：</strong>这一部分借鉴的是《Word translation without parallel data》paper中的做法。具体做法是：假设我们已经得到source language与target language的word embedding：<span class="math inline">\(S，T\in R^{d\times D}\)</span>，并且得到了D对word的词典，我们希望能够找到一个变化，使得两个语言的word可以相互转换。公式是： <span class="math display">\[
P=argmin_{P^{&#39;}}||P^{&#39;}S-T||_F
\]</span> 得到<span class="math inline">\(P\)</span>之后，对于任何一个source language word，我们可以通过最近邻的方法来找到对应的翻译。但是关键是怎么找到最近邻呢？方法是定义CSLS方法来衡量不同语言的word之间的相似度。具体可以参看<a href="https://zhuanlan.zhihu.com/p/30218451" target="_blank" rel="noopener">link</a>。</p>
<p><strong>knowleage distillation：</strong>得到三个模型(<span class="math inline">\(\theta_{src}、\theta_{teach}、\theta_{trans}\)</span>)之后，我们需要将这些模型的知识蒸馏给student。具体做法是如下：</p>
<ol type="1">
<li>首先，我们将unlabeled target language data输入到<span class="math inline">\(\theta_{teach}\)</span>与<span class="math inline">\(\theta_{stu}\)</span>中，来让<span class="math inline">\(\theta_{stu}\)</span>模型去拟合<span class="math inline">\(\theta_{teach}\)</span>的输出的分布，loss使用MSE；这样做，不仅可以将<span class="math inline">\(\theta_{teach}\)</span>的知识蒸馏给<span class="math inline">\(\theta_{stu}\)</span>，同时还能让<span class="math inline">\(\theta_{stu}\)</span>学习到unlabeled language data中的信息；</li>
<li>我们希望进一步提升student model的效果，采用的方式是：我们希望能够得到unlabel target language data的pseudo label，然后使用pseudo label来训练student model。具体怎么得到pesudo label呢？我们设计了一种voting 机制，我们将unlabel target langugae输入到三个模型当中，只有当三个模型的输出的结果完全相等时，我们才给其标上label，然后使用这些数据来训练sutdent model。</li>
<li>综合第一步与第二步，knowledge distillation的总的loss，表示如下：</li>
</ol>
<p><span class="math display">\[
{\cal L}(\theta_{stu})=\frac{1}{|{\cal D}_T|}\sum_{\tilde x\in {\cal D}_T}(\eta{\cal L}_{hard}^{\tilde x}+{\cal L}_{soft}^{\tilde x})
\]</span></p>
<p>其中，<span class="math inline">\({\cal L}_{hard}^{\tilde x}\)</span>是第二步的loss，<span class="math inline">\({\cal L}_{soft}^{\tilde x}\)</span>是第一步的loss，<span class="math inline">\(\eta=1\)</span>。</p>
<p><strong>inference：</strong>我们只使用student model，另外注意，解码使用CRF。</p>
<h3 id="experiment-2">experiment</h3>
<ol type="1">
<li><p>数据集：CoNLL-2002、CoNLL-2003、NoDaLiDa-2019</p></li>
<li><p>结果</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/unitrans-result.jpg"></p>
<p>从结果中我们可以看到，效果提升非常大，这个还可以通过teacher ensembling进一步提升效果。作者也做了一些消融实验，如下：</p>
<p><img src="/2020/10/19/NLP-Cross-Lingual-NER/ablation.jpg"></p>
<p>这篇论文结果真的太强了，甚至给我感觉使用teacher-student learning这种方式来做cross-lingual NER，这篇paper到顶了。</p></li>
</ol>
<p>##GRN model(not for cross-lingual NER)</p>
<h3 id="background-3">background</h3>
<p>GRN模型是2019年MSRA发表在AAAI上的《GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition》。在NER任务中，由于其对word的位置信息极其敏感，所以常用的编码器常常使用RNN，但是RNN系列的编码器最大的缺点就在于并行效率太低，而CNN相比RNN来说，最大的优势在于：并行效率高，但是其缺点在于CNN只能对word周边的信息进行建模，捕捉局部context information，但是CNN对global context information的捕捉能力太弱，基于CNN的NER模型(IDCNN)在NER任务上的结果都弱于RNN系列的NER模型，<strong>所以，是否能够设计某种机制，让CNN能够捕捉到global context information呢？</strong>在GRN里，通过使用gating机制对sentence中的任意两个word之间的relation进行建模，从而让CNN捕捉global context information的能力大大提高，这就是GRN提出的背景。</p>
<blockquote>
<p>吐槽一下，这篇说实话很一般，idea真的一般，2019年，BERT都出来了，在GRN的实验里，都没有使用BERT来进行对比，结果也没啥说服力。</p>
</blockquote>
<h3 id="grn-model">GRN model</h3>
<p>整个GRN模型分为四层：<strong>representation layer、context layer、relation layer、CRF layer。</strong></p>
<ol type="1">
<li><p><strong>representation layer：</strong>假设输入的sentence表示为：<span class="math inline">\(s=\{s_1,s_2,...,s_T\}\)</span>，每一个token对应的标签表示为：<span class="math inline">\(y=\{y_1,y_2,..,y_T\}\)</span>。整个layer的输出有两部分：由GloVe初始化的word embedding(update in training stage)+char embedding通过kernel=3的CNN+max-over-time pooling，从而得到char feature。第一部分表示为：<span class="math inline">\(w_i=E(s_i)\)</span>；第二部分：<span class="math inline">\(c_i\)</span>，输出是两者concat的结果：<span class="math inline">\(z_i=[c_i,w_i]\)</span>。</p></li>
<li><p><strong>context layer：</strong>这一层主要是捕捉word的局部信息。具体做法是：使用多个不同kernel size的CNN(等长卷积，kernel size=1，3，5)来得到encoding的输出，卷积的激活函数使用<span class="math inline">\(tanh\)</span>，然后对使用激活函数之后的结果进行concat，再使用max pooling(不是global max pooling，所以维度数目不变)。</p></li>
<li><p><strong>relation layer：</strong>这一层的目的主要是用来对sentence中的任意两个word之间的关系进行modeling，并利用gating机制来获取全局内容信息，具体公式如下(其实还是attention，没啥创新，不过这个layer的实现code还是可以去看看的)： <span class="math display">\[
\alpha_{i,j}=\sigma(W[x_i;x_j]+b_x) \\
r_i=\frac{1}{T}\sum_{j=1}^{T}\alpha_{ij}*x_j \\
p_i=tanh(r_i)
\]</span> 其中，<span class="math inline">\(r_i\)</span>就是word <span class="math inline">\(x_i\)</span>的全局fusion feature，当然了，为了增加非线性，对<span class="math inline">\(r_i\)</span>使用tanh进行变换，得到<span class="math inline">\(p_i\)</span>，这就是最终输入到CRF的输入。</p></li>
<li><p><strong>CRF layer：</strong>就是标准的CRF layer。</p></li>
</ol>
<h3 id="experiment-3">experiment</h3>
<ol type="1">
<li>数据集：CoNLL-2003 English NER、OntoNotes 5.0</li>
<li>hypoparameters的设置请参考原始论文，具体结果就不放了，反正就在当时达到了SOTA，anyway。</li>
</ol>
<h2 id="references">references</h2>
<p>《Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources》</p>
<p>code：<strong>https://github.com/microsoft/vert-papers/tree/master/papers/Meta-Cross</strong></p>
<p>《Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language》、</p>
<p>code：<strong>https://github.com/microsoft/vert-papers/tree/master/papers/SingleMulti-TS</strong></p>
<p>《UniTrans : Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data》</p>
<p>code： <strong>https://github.com/microsoft/vert-papers/tree/master/papers/UniTrans</strong></p>
<p>《GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition》。</p>
<p>code：<strong>https://github.com/HuiChen24/NER-GRN</strong></p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>cross-lingual NER</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|Multimodal NER</title>
    <url>/2020/10/21/NLP-Multimodal-NER/</url>
    <content><![CDATA[<p>近期会更新一系列NER的paper解读，计划2周时间将NER的重要的论文刷完，有一个想做的事情嘻嘻😁。这篇博客主要解读一下目前多模态NER上质量比较好的论文：《Adaptive Co-Attention Network for Named Entity Recognition in Tweets》、《Visual Attention Model for Name Tagging in Multimodal Social Media》、《Multimodal Named Entity Recognition for Short Social Media Posts》、《Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer》。</p>
<a id="more"></a>
<h2 id="adaptive-co-attention-network-for-named-entity-recognition-in-tweetsaaai2018">Adaptive Co-Attention Network for Named Entity Recognition in Tweets(AAAI2018)</h2>
<h3 id="background">background</h3>
<p>NER是NLP最基础且重要的任务之一，但是传统的NER都是只针对与textual content进行建模。在社交媒体上，<strong>很多的推文都是文字+图片的形式，并且社交媒体上的文字往往比较口语化、语法也比较informal、字数比较少、很多文字内容只有结合image才能理解，</strong>单纯对text进行建模，往往得不到很好的结果，而image对于NER提供了很好的辅助信息，所以如何利用image信息来进行NER，是一个非常重要的问题。这篇paper正是针对这样一个问题，构建了twitter数据集，并提出了ACN模型。</p>
<h3 id="model">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/ACN_model.jpg"></p>
<p>ACN模型主要分为三部分：<strong>feature extractor、adaptive co-attention netowrk、CRF tagging</strong>。</p>
<ul>
<li><p><strong>feature extractor：</strong>这一部分分为两个小部分：image feature extraction与text feature extraction。</p>
<ul>
<li>对于image，采用VGG16来对iamge进行encoding，我们抽取最后一层的输出作为image的representation，表示为：<span class="math inline">\(\tilde v_I=\{\tilde v_i|\tilde v_i\in R^{d_v},i=1,...,N\}\)</span>，其中<span class="math inline">\(d_v\)</span>表示每一个region的维度，共有7x7个region，即：<span class="math inline">\(N=49\)</span>，所以一张image输出的维度是：<span class="math inline">\(N\times d_v\)</span>。为了能够与之后的text进行交互，维度与text相同，对其进行了非线性变换，并使用tanh进行激活，得：<span class="math inline">\(v_I=tanh(W_I\tilde v_I+B+I)\)</span>，<span class="math inline">\(v_I\)</span>是最终的image的representation。</li>
<li>对于text，是character-level word embedding与word embedding的concat。首先是character embedding，使用多个不同的cnn来对其进行编码，并使用tanh进行激活，然后对每一个结果使用max-over-time pooling，最后全部进行concat(常规操作了，没啥新颖的。)，得到character-level word embedding：<span class="math inline">\(w^{&#39;}\)</span>；将<span class="math inline">\(w^{&#39;}\)</span>与<span class="math inline">\(w^{&#39;&#39;}\)</span>进行concat，得到<span class="math inline">\(w\)</span>，将<span class="math inline">\(w\)</span>输入到BILSTM，得到encoding后的句子的token的表示：<span class="math inline">\(x=\{h_j|h_j\in R^d,j\in 1,2,...,n\}\)</span>，<span class="math inline">\(n\)</span>表示句子的长度。</li>
</ul></li>
<li><p><strong>Adaptive co-attention network：</strong>在得到image与text的representation之后，我们希望将两者进行融合。但是对于sentence中的每一个token，不是所有的image region都有用，反过来也是一样的，对于image中的每一个region，不是sentence中的所有token都与其相关，那么一种很好的方式就是使用co-attention，即：image对text进行attention(word-guided visual attention)与text对text进行attention(image-guided textual attention)与。具体如下：</p>
<ul>
<li><p><strong>word-guided visual attention：</strong>不是所有的image region都与sentence中的token相关，如果将image中所有的信息全部给sentence，那么会引入很多噪音，损害模型性能。所以我们设计了word-guided visual attention，从而让模型自动过滤掉与sentnece无关的image region，只留下最相关的region。公式如下： <span class="math display">\[
z_t=tanh(W_{v_I}v_I\oplus (W_{h_t}h_t+b_{h_t})) \\
\alpha_t=softmax(W_{\alpha_t}z_t+b_{\alpha_t}) \\
\tilde v_t=\sum_{i=1}^{N}\alpha_{t,i}v_i \\
v_i\in v_I,i=1,...,N
\]</span> 注意，<span class="math inline">\(v_I\in E^{N\times d_v}\)</span>，<span class="math inline">\(h_t\in R^d\)</span>，最终的<span class="math inline">\(\tilde v_t\in R^{d&#39;}\)</span>。</p></li>
<li><p><strong>image-guided textual attention：</strong>除了image中的信息外，我们还希望对text本身进行建模，也就是在text中找到与每一个token相关的words。做法是：在word-guided visual attention的基础上，继续使用相似的co-attention机制，来得到新的结果。具体如下： <span class="math display">\[
z_t^{&#39;}=tanh(W_{x}x\oplus (W_{x,\tilde v_t}\tilde v_t+b_{x,\tilde v_t})) \\
\beta_t=softmax(W_{\beta_t}z_t^{&#39;}+b_{\beta_t}) \\
\tilde h_t=\sum_{i=1}^{n}\beta_{t,i}h_i \\
h_i\in x,i=1,...,n
\]</span> 但实际上，这种做法并不是很好，有没有更好的？当然有啊，BERT！今年ACL2020的UMT模型就使用BERT来做，取得了很大的提高。</p></li>
<li><p><strong>Gated multimodal fusion：</strong>得到attention之后的<span class="math inline">\(\tilde v_t\)</span>与<span class="math inline">\(\tilde h_t\)</span>，<span class="math inline">\(t=1,...,n\)</span>，然后我们设计GMF机制，来过滤一些信息。公式如下： <span class="math display">\[
h_{\tilde v_t}=tanh(W_{\tilde v_t}\tilde v^t+b_{\tilde v_t}) \\
h_{\tilde h_t}=tanh(W_{\tilde h_t}\tilde h^t+b_{\tilde h_t}) \\
g_t=sigmoid(W_{g_t}(h_{\tilde v_t}\oplus h_{\tilde h_t})) \\
m_t=g_th_{\tilde v_t}+(1-g_t)h_{\tilde h_t}
\]</span></p></li>
<li><p><strong>Filtration gate：</strong>由于image的信息不是必须的，而text的信息是必须的，所以在最终输入到CRF之前，需要再次调整两者的比重。具体如下： <span class="math display">\[
s_t=sigmoid(W_{s_t,h_t}h_t \oplus (W_{m,s_t}m_t+b_{m_t,s_t})) \\
u_t=s_t(tanh(W_{m_tm_t+b_{m_t}})) \\
\tilde m_t=W_{\tilde m_t}(h_t\oplus u_t)
\]</span> 从公式中可以看出，<span class="math inline">\(s_t\)</span>控制着image信息的比例，当我们不需要image信息的时候，我们就让其越接近于0，如果越需要image的信息，我们就让其越接近于1。</p></li>
</ul></li>
<li><p><strong>CRF tagging：</strong>没什么可讲的，就是标准的CRF层。</p></li>
</ul>
<h3 id="experiment">experiment</h3>
<ol type="1">
<li><p>数据集：twitter数据集(released by this paper)</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/ACN_result.jpg"></p>
<p>可以看到，这篇paper提出的model效果还是可以的，另外·，从ablation实验中可以看到，各个component设计的还是比较合理的，总之是一片很经典的论文，值得多读一读。</p></li>
</ol>
<h2 id="visual-attention-model-for-name-tagging-in-multimodal-social-mediaacl2018">Visual Attention Model for Name Tagging in Multimodal Social Media(ACL2018)</h2>
<h3 id="background-1">background</h3>
<p>这篇paper同样是针对multimodal NER，针对的问题也是一样的：<strong>如何更好的利用image信息来复制NER任务？</strong>在这篇paper中，设计了一种的新的attention机制和gate机制，从而取得了SOTA的结果。</p>
<h3 id="model-1">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/VATT_model.jpg"></p>
<p>GVAtt的整体结构分为部分：<strong>feature representation、Visual attention model、visual modulation module。</strong></p>
<ul>
<li><p><strong>feature representation：</strong>分为两部分：text representation与visual representation。对于前者，是word embedding与character-level word embedding的concat。所谓的character-level word embedding是指将character embedding输入到BILSTM当中(常规操作)。对于后者，采用的是ResNet，保留两部分：<span class="math inline">\(V_g\)</span>与<span class="math inline">\(V_r\)</span>，<span class="math inline">\(V_g\)</span>是指最后的全连接层的输出，维度是：<span class="math inline">\(V_g\in R^{1024}\)</span>，<span class="math inline">\(V_r\)</span>是输入到最后的全连接层之前的输出，其维度是：<span class="math inline">\(V_r\in R^{1024\times 49}\)</span>。这里之所以保留了两个，是因为后面做了实验比较，并不是整张image都与sentence相关，不相关的部分反而会引入噪音。</p></li>
<li><p><strong>visual attention model：</strong>这一部分主要是要解决：对于sentence中的每一个token，如何保留与其最相关的image region，并且去除与其不相关的region？即：提取出与sentence中相关的image region feature，具体公式如下： <span class="math display">\[
Q=LSTM(S) \\
P_t=tanh(W_tQ) \\
P_v=tanh(W_vV_r) \\
A=P_t\oplus P_v \\
E=softmax(W_aA+b_a) \\
v_c=\sum\alpha_iv_i,\alpha\in E,v_i\in V_r
\]</span> 其中，<span class="math inline">\(S\)</span>表示feature representation中的concat后的text embedding，最后得到的<span class="math inline">\(v_c\)</span>最为之后的BISLTM的初始输入。另外再提一句，这里之所以使用的是<span class="math inline">\(V_r\)</span>，是因为之后的实验表明了使用<span class="math inline">\(V_r\)</span>的效果要比<span class="math inline">\(v_g\)</span>更好。</p></li>
<li><p><strong>visual modulation module：</strong>设置这一层的目的是：将第二步提取出的visual信息与text embedding进行结合。首次将第一步得到的text embedding输出到BILSTM当中，得到<span class="math inline">\(h_i\)</span>；然后仿照LSTM的更新方式，设计了gate机制，具体如下： <span class="math display">\[
\beta_v=sigmoid(W_vh_i+U_vv_c+b_v) \\
\beta_w=sigmoid(W_wh_i+U_wv_c+b_w) \\
m=tanh(W_mh_i+U_mv_c+b_m) \\
w_m=\beta_wh_i+\beta_vm
\]</span> 最终计算得到的<span class="math inline">\(w_m\)</span>作为CRF层的输入，CRF就是标准的CRF层，这里不再赘述。</p></li>
</ul>
<h3 id="experiment-1">experiment</h3>
<ol type="1">
<li><p>数据集：twitter、snap captions</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/gvatt_result.jpg"></p>
<p>从结果来看，很不错，虽然paper里面没有个ACN模型进行对比，但是从twitter数据集结果来看，F1值高了近10个百分点！nb！</p></li>
</ol>
<h2 id="multimodal-named-entity-recognition-for-short-social-media-postsnaacl2018">Multimodal Named Entity Recognition for Short Social Media Posts(NAACL2018)</h2>
<h3 id="background-2">background</h3>
<p>这篇paper也是针对multimodal NER，解决的问题也是两个：1.social media上的tweets是非常不规范的、informal并且字数较少，所以一方面因为没有足够的信息会引起一词多义性问题，此外，由于不规范的使用，使得会产生很多未知的token，这个怎么解决？(其实现在来看就很好解决，上BERT啊)；2.怎么让image与text更好进行交互？为了解决这些问题，就有了MNER模型。</p>
<h3 id="model-2">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/MNER_model.jpg" style="zoom:50%;"></p>
<p>整体模型分为三部分：<strong>features、modality attention module、BILSTM+CRF</strong>。</p>
<ul>
<li><p><strong>task definition：</strong>输入的sentence表示为：<span class="math inline">\(x=\{x_t\}_{t=1}^{T}\)</span>，其对应的标签是:<span class="math inline">\(y=\{y_t\}_{t=1}^{T}\)</span>，每一个token由三种modality组成：<span class="math inline">\(x_t=\{x_t^{(w)},x_t^{(c)},x_t^{(v)}\}\)</span>，分别表示word embedding、character embedding、visual embedding。</p></li>
<li><p><strong>features：</strong>共分为三种：word embedding、character embedding、visual embedding。word embedding由GloVe初始化，并在training过程进行更新；character embedding是将character embedding送入BILSTM当中，得出的输出；visual embedding是使用GoogleNet的最后一层的输出。</p></li>
<li><p><strong>modality attention module：</strong>在之前的paper中，大部分都是concat text与image的信息，然后再输入到CRF当中，但是concat的方式会损失信息，造成效果下降。为了解决这个问题，这篇paper分别对三个modality进行编码，再进行加权求和。具体公式如下： <span class="math display">\[
[a_t^{(w)},a_t^{(c)},a_t^{(v)}]=sigmoid(W_m[x_t^{(w)};x_t^{(c)};x_t^{(v)}]+b_m) \\
\alpha_{t}^{(m)}=\frac{exp(a_t^{(m)})}{\sum_{m^{&#39;}\in \{w,c,v\}}exp(a_t^{(m^{&#39;})})} ,\forall m\in \{w,c,v\} \\
\overline x_t=\sum_{m\in \{w,c,v\}}\alpha_t^{(m)}x_t^{(m)}
\]</span> <span class="math inline">\(\overline x_t\)</span>是最终到BILSTM+CRF的输入。</p></li>
<li><p><strong>BILSTM+CRF：</strong>这就是标准的BILSTM+CRF，不再赘述。</p></li>
</ul>
<h3 id="experiment-2">experiment</h3>
<ol type="1">
<li><p>数据集：snap caption</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/MNER_result.jpg"></p>
<p>从结果来看，还行吧，没啥亮点。</p></li>
</ol>
<h2 id="improving-multimodal-named-entity-recognition-via-entity-span-detection-with-uniﬁed-multimodal-transformeracl2020">Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer(ACL2020)</h2>
<h3 id="background-3">background</h3>
<p>这是最新的multimodal NER的paper，主要解决的问题有：1.text与image的交互不够，即便能够得到简单交互后的结果，但是text对image context并不敏感；2.之前的研究并没有考虑image带来的bias。针对于第一个问题，这篇paper提出了MMI module，针对于第二个问题，这篇paper设计了ESD辅助任务来解决。具体的下面小节将详细的介绍。</p>
<h3 id="model-3">model</h3>
<p>先放图～</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/UMT_model.jpg"></p>
<p>整体模型分为三部分：<strong>unimodal input representation、multimodal transformer、unified multimodal transformer。</strong></p>
<ul>
<li><p><strong>unimodal input representation：</strong>主要分为两部分：word representation与visual representation。对于前者，paper中使用了BERT，一方面是能够得到更加强大的representation，另一方面也能够解决一词多义性问题。假设输入的句子为：<span class="math inline">\(S^{&#39;}=(s_0,s_1,...,s_{n+1})\)</span>，其中<span class="math inline">\(s_0、s_{n+1}\)</span>分别表示[CLS]与[SEP] token，通过三种embedding相加，得到的句子表示为：<span class="math inline">\(X=(x_0,x_1,...,x_{n+1})\)</span>，通过BERT之后，为：<span class="math inline">\(C=(c_0,c_1,...,c_{n+1})\)</span>，其中<span class="math inline">\(c_i\in R^d\)</span>；对于后者，采用ResNet来对image进行编码，我们保留ResNet的最后一个卷积层的输出作为该image的representation，为：<span class="math inline">\(U=(u_1,u_2,...,u_49)\)</span>，<span class="math inline">\(u_i\)</span>表示一个region，且其维度是：<span class="math inline">\(R^{2048}\)</span>，然后我们通过一个线性变换，让其维度与word representation的维度一样，为：<span class="math inline">\(V=(v_1,...,v_{49}),v_i\in R^d\)</span>。</p></li>
<li><p><strong>multimodal transformer：</strong>在正式对word与visual进行交互之前，我们首先对word representation再使用一层标准的transformer layer，我个人理解是增加非线性，不加其实也说的通，得到<span class="math inline">\(R=(r_0,r_1,...,r_{n+1})\)</span>。接下来，设计了MMI module，让word与visual进行交互。具体分为两部分：image-aware word representation与word-aware visual representation。</p>
<ul>
<li><p><strong>image-aware word representation：</strong>这一步是image对word进行attention，得到更好的word representation。那么，就是<span class="math inline">\(V\)</span>作为query，<span class="math inline">\(R\)</span>作为key和value，使用m-head cross-modal attention，公式如下： <span class="math display">\[
CA_i(V,R)=softmax(\frac{[W_{q_i}V]^T[W_{k_i}R]}{\sqrt {d/m}})[W_{v_i}R]^T \\
MH-CA(V,R)=W^{&#39;}[CA_1(V,R),CA_2(V,R),...,CA_m(V,R)]^T \\
\tilde P=LN(V+MH-CA(V,R)) \\
P=LN(\tilde P+FFN(\tilde P))
\]</span> 其中，<span class="math inline">\(P=(p_1,...,p_{49})\)</span>。但是这个与我们最终想要得到更好的word representation的目标不一致，所以我们将<span class="math inline">\(R\)</span>作为query，<span class="math inline">\(P\)</span>作为key和value，再输入到一个与上述一样的transformer layer，得到<span class="math inline">\(A=(a_0,a_1,...,a_{n+1})\)</span>。</p></li>
<li><p><strong>word-aware visual representation：</strong>这里是word对visual进行attention，所以将word <span class="math inline">\(R\)</span>作为query，<span class="math inline">\(V\)</span>作为key和value，应用和上述一样的transformer layer，得到<span class="math inline">\(Q=(q_0,q_1,...,q_{n+1})\)</span>。然后我们希望能够控制visual information的输入，所以有设计了visual gate，具体公式如下： <span class="math display">\[
g=sigmoid(W_a^TA+W_q^TQ) \\
B=gQ
\]</span> <span class="math inline">\(B\)</span>就是最终的visual representation。</p></li>
<li><p><strong>CRF：</strong>我们将<span class="math inline">\(A\)</span>与<span class="math inline">\(B\)</span>进行concat，并且输送到标准的CRF kayer当中。</p></li>
</ul></li>
<li><p><strong>unified multimodal transformer：</strong>在上一步的multimodal transformer当中，都是在关注如何对text和image之间的关系进行建模，但是这样很容易导致模型过分去关注image中有的entity，对于在sentence中但是不在image中的实体关注不够，因此为了解决这个问题，设计了ESD辅助任务，用来预测entity的type。假设一个句子的标注为：<span class="math inline">\(z=(z_0,...,z_{n+1})\)</span>，其中<span class="math inline">\(z_i\in \cal Z\)</span>，<span class="math inline">\(\cal Z=\{B,I,O\}\)</span>，我们将其输送到一个transformer layer+CRF layer。然后我们想要将其辅助与最终的预测，所以设计了一个conversion matrix：<span class="math inline">\(W^c\in R^{|\cal Z|\times |\cal Y|}\)</span>，其中<span class="math inline">\(\cal Z=\{B,I,O\}\)</span>，<span class="math inline">\({\cal Y}=\{B-PEC,I-PEC,...\}\)</span>，如果<span class="math inline">\(\cal Z_j\)</span>不是<span class="math inline">\(\cal Y_k\)</span>的相关label，那么<span class="math inline">\(W^c_{j,k}=0\)</span>，否则为<span class="math inline">\(W^c_{j,k}=\frac{1}{|C_j|}\)</span>。</p></li>
</ul>
<h3 id="experiment-3">experiment</h3>
<ol type="1">
<li><p>数据集：twitter2015、twitter2017</p></li>
<li><p>details：训练过程中，resnet参数不更新，bert参数更新，seq_length为128，batch_size为16。</p></li>
<li><p>结果</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/UMT_result.jpg"></p>
<p>从结果来看，提升还是挺大的，个人觉得是得益于BERT的使用以及设计的ESD辅助任务，还是不错的。</p></li>
<li><p>消融实验</p>
<p><img src="/2020/10/21/NLP-Multimodal-NER/UMT_ablation.jpg"></p>
<p>这是paper当中消融实验的结果，可以看到，在UMT模型中，各个component都比较重要，去掉都会有影响，但是相比来说，ESD辅助任务发挥了更大的作用，之后可以搞一波。</p></li>
<li><p>base case分析：在paper的base case分析中，主要的错误在于：当image与text完全不相关的时候，BERT-CRF预测会正确，而UMT预测错误，实际上就相当于image是噪音，在整个数据集中，大概有5%的比例。</p></li>
</ol>
<p>小小的总结一下吧：对与Multimodal NER任务，目前主要的难点是：1.如何设计更好的机制，从而让token在image找到与其最相关的region？2.在1的基础上，怎么对text和image region更好地进行融合？3.image当中其实只有很少的实体，对于sentence中的其他实体，image其实并不能提供有效的信息，怎么去解决这种情况？4.当image与text完全不相关时，怎么解决？</p>
<h2 id="references">references</h2>
<p>《Adaptive Co-Attention Network for Named Entity Recognition in Tweets》</p>
<p>《Visual Attention Model for Name Tagging in Multimodal Social Media》</p>
<p>《Multimodal Named Entity Recognition for Short Social Media Posts》</p>
<p>《Improving Multimodal Named Entity Recognition via Entity Span Detection with Uniﬁed Multimodal Transformer》</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>命名实体识别</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multimodal NER</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|RE-Integrate GNN into Document-level RE task|PART TWO</title>
    <url>/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/</url>
    <content><![CDATA[<p>接着上一篇博客，这篇博客也是讲解图神经网络在Document-level关系抽取上的应用，共讲解4篇论文：《Global-to-Local Neural Networks for Document-Level Relation Extraction》、《HIN: Hierarchical Inference Network for Document-Level Relation Extraction》、《Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling》、《Entity and Evidence Guided Relation Extraction for DocRED》。</p>
<a id="more"></a>
<h2 id="global-to-local-neural-networks-for-document-level-relation-extractionemnlp2020">Global-to-Local Neural Networks for Document-Level Relation Extraction(EMNLP2020)</h2>
<h3 id="background">Background</h3>
<p>这篇文章的核心思想是：通过从粗粒度、细粒度、context information的角度对entity之间的relation进行建模。具体解决的问题是：第一：怎么对document中的复杂语义进行建模？(使用BERT)；第二：怎么学习有效地entity representation？(使用RGCN)；第三：其他的relation也对于当前目标entity之间的relation产生影响，如果有效的利用？(context-aware)。将这三个方面聚合起来，就是GLRE模型。</p>
<blockquote>
<p>目前在doucment-level RE中，存在着三方面的问题：logical reasoning、coreference reasoning、common-sense reasoning。</p>
</blockquote>
<h3 id="model">Model</h3>
<p>先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/GLRE_model.jpg"></p>
<p>GLRE模型分为四部分：<strong>encoding layer、global representation layer、local representation、classifier layer。</strong></p>
<ul>
<li><p><strong>encoding layer：</strong>假设输入的document表示为：<span class="math inline">\(\cal D=\{w_1,w_2,...,w_k\}\)</span>，其中<span class="math inline">\(w_i\)</span>表示的是第<span class="math inline">\(i\)</span>个word，我们使用BERT来对其进行编码，如下： <span class="math display">\[
H=[h_1,h_2,...,h_k]=BERT([w_1,w_2,...,w_k])
\]</span> <span class="math inline">\(H\)</span>是BERT最后一层的输出。</p></li>
<li><p><strong>global representation layer：</strong>GLRE模型同样是基于graph的模型。同样是继承EoG模型，其中有三种node：mention node、entity node与sentence node。对于三种node的表示，我们都是将其本身的avg embedding与各自的type embedding进行conat。具体来说，对于mention node：<span class="math inline">\(n_{m_i}=[avg_{w_j\in m_i}(h_j);t_m]\)</span>；对于entity node：<span class="math inline">\(n_{e_i}=[avg_{m_j\in e_i}(n_{m_j});t_{e}]\)</span>；对于sentence node：<span class="math inline">\(n_{s_i}=[avg_{h_j\in s_i}(h_j);t_s]\)</span>。有五种edge：mention-mention edge(同一个sentence中的所有mentions之间加一条边)、mention-entity edge(entity与其mention之间加一条边)、mention-sentence edge(如果mention node与sentence node加一条边)、entity-sentence edge(如果一个entity至少有一个mention出现在sentence中，那么entity node与sentence之间加一条边)、sentence-sentence edge(所有的sentence node之间加一条边)。<strong>⚠️和EoG模型一样，同样没有entity-entity edge！</strong>构建好graph之后，在GLRE模型中，stack了L层的R-GCN，具体的aggregate的公式如下： <span class="math display">\[
n^{l+1}_{i}=\sigma(\sum_{x\in {\cal X}}\sum_{j\in {\cal N_{x}^{i}}}\frac{1}{|\cal N_{x}^{i}|}W^l_xn^l_j+W^l_0n^l_i)
\]</span> 其中，<span class="math inline">\(\cal N_{x}^{i}\)</span>表示在<span class="math inline">\(x\)</span>这种edge type下，node i的邻接节点的集合，说到底，还是和GCN处理异质图的方式没啥区别。我们将L层之后的每一个node的representation称为entity global representation，记做：<span class="math inline">\(e^{glo}_{i}\)</span>。</p></li>
<li><p><strong>local representation layer：</strong>如果说global rep layer是想在document-level上对不同的entity之间的语义信息进行建模，那么local rep layer是在对同一个entity的不同的mention之间的信息进行建模，从而得到更加好的entity rep，因为在不同的entity pair中，entity的表示肯定是不同的，如果只用global rep layer的结果的话，那么entity的表示就是唯一的了，肯定对最后结果会有影响。怎么做呢？在GLRE中，采用的是multi-head self attention。对于entity pair <span class="math inline">\((e_a,e_b)\)</span>，公式如下： <span class="math display">\[
e^{loc}_a=LN(MHead_a(e^{glo}_a,\{n_{s_i}\}_{s_i\in \cal S_a},\{n_{m_j}\}_{m_j\in \cal M_a})) \\
e^{loc}_b=LN(MHead_b(e^{glo}_b,\{n_{s_i}\}_{s_i\in \cal S_b},\{n_{m_j}\}_{m_j\in \cal M_b}))
\]</span> 其中，<span class="math inline">\(\cal S_a\)</span>表示的是entity a 的mentions所在的sentence的集合，<span class="math inline">\(\cal M_a\)</span>表示的是enity a的mention的集合。</p></li>
<li><p><strong>classifier layer：</strong>得到<span class="math inline">\(e^{glo}\)</span>与<span class="math inline">\(e^{loc}\)</span>之后，我们将两者聚合。对于entity pair <span class="math inline">\((e_a,e_b)\)</span>，具体来说： <span class="math display">\[
\tilde e_a=[e^{glo}_a;e^{loc}_a;\Delta(\delta_{ab})] \\
\tilde e_b=[e^{glo}_b;e^{loc}_b;\Delta(\delta_{ba})]
\]</span> 其中，<span class="math inline">\(\delta_{ab}\)</span>表示的是entity a的第一个mention与entity b的相对距离。最后的表示是：<span class="math inline">\(o_r=[\tilde e_a;\tilde e_b]\)</span>。为了进一步挖掘信息，paper中说的是document的topic information能够暗含某些relation，所以再一次使用self-attention(与transformer不同，名字叫做context-aware),如下： <span class="math display">\[
o_c=\sum_{i=0}^{p}\theta_io_i=\sum_{i=0}^{p}\frac{exp(o_iWo^{\prime}_{i})}{\sum_{j=0}^{p}exp(o_jWo^{\prime}_{j})}
\]</span> 然后将<span class="math inline">\(o_c\)</span>输入到一个FFN层进行分类，即：<span class="math inline">\(y_r=sigmoid(FFN([o_r,;o_c]))\)</span>，最后的loss使用CE。</p></li>
</ul>
<h3 id="experiment">Experiment</h3>
<ol type="1">
<li><p>数据集：CDR、DocRED</p></li>
<li><p>结果</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/GLRE_result.jpg" style="zoom:50%;"></p>
<p>说实话，这个结果一般。但是这篇论文的ablation与error analysis很有意思。GLRE在长距离的entity pair的关系抽取上取得了比较好的结果(&gt;=3)，paper中给它归因于global rep layer的构建，同时local rep layer的构建减少了噪音。</p></li>
<li><p>消融实验结论</p>
<ol type="1">
<li><p>BERT对于最终的结果影响很大，paper中做了几组实验，如下：</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/GLRE_result_pretrained_model.jpg" style="zoom:50%;"></p></li>
<li><p>当local rep layer被移除之后，F1值下降很快，这说明multi-head self attention能够有选择性的aggregatemention rep，过滤掉一些噪音，看来不是所有的mention都有entity rep都有用，结果如下：</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/GLRE_result_ablation.jpg" style="zoom:50%;"></p></li>
</ol></li>
<li><p>Case study</p>
<ol type="1">
<li>Logical reasoning很重要，GLRE模型中，完全靠RGCN，没有另外再搞一套机制来结局logical reasoning；</li>
<li>当一个句子中，很多entity用<code>and</code>这样的词连接的时候，往往模型无法对她们之间的关系进行建模，paper中说在GLRE模型中是靠context-aware来解决的，但是我觉得效果一般吧；</li>
<li>怎么引入先验知识很重要，因为很多common sense不会出现在训练集里面，而很多relation的判断都需要用到先验知识，这个时候怎么做？GLRE也没有解决这个问题；</li>
<li>当一个句子的主语等部分缺失时，怎么处理？在GLRE中，是通过建立global rep layer来解决的，但是效果不是很好；</li>
<li>当代词有歧义的时候，怎么解决？</li>
<li>paper统计了一下，logical reasoning(40.9%)、component missing error(28.8%)、prior knowledge missing error(13.6%)、coreference reasoning error？(12.9%)。</li>
</ol></li>
</ol>
<p>总的来说，GLRE模型一般，但是其case study与消融实验做的还是蛮好的，很有启发性。</p>
<h2 id="hin-hierarchical-inference-network-for-document-level-relation-extractionpakdd2020">HIN: Hierarchical Inference Network for Document-Level Relation Extraction(PAKDD2020)</h2>
<h3 id="background-1">Background</h3>
<p>HIN模型还是蛮好懂的，它并不是从基于graph的方式上来解决document-level RE的，而是通过从entity-level、sentence-level、document-level这样层级的方式来做，有点类似于文本分类模型HAN。</p>
<h3 id="model-1">Model</h3>
<p>先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/HIN_model.jpg"></p>
<p>HIN模型分为四部分：<strong>input layer、entity-level inference module、hierarchical document-level inference module、prediction layer。</strong></p>
<ul>
<li><p><strong>input layer：</strong>原始的输入有三部分：<strong>word embedding、entity type embedding、coreference embedding。</strong>其中，word embedding就是每一个token的word embedding，其维度：<span class="math inline">\(d_w\)</span>；entity type embedding指的是将entity 的type信息转换为一个dense vector，其维度是：<span class="math inline">\(d_t\)</span>；coreference embedding指的是同一个entity的不同的mention被分配同一个entity id，其维度：<span class="math inline">\(d_c\)</span>。这一层的输出是这三者的concat。</p></li>
<li><p><strong>entity-level inference module：</strong>首先使用BILSTM来对document进行编码，即：<span class="math inline">\(h_i=BILSTM(w_i),i\in[1,n]\)</span>，<span class="math inline">\(n\)</span>表示一个document中所有的token数目；每一个mention的rep是其每一个word的avg，每一个entity是其所有的mention rep的avg，即：<span class="math inline">\(e_l=avg_{w_i\in e_l}(h_i),\ E_a=avg_{e_l\in E_a}(e_l)\)</span>，其中，<span class="math inline">\(e_l\)</span>表示某一个mention的rep，<span class="math inline">\(E_a\)</span>表示某一个entity的rep。由于收到multi-head attention的影响：将vector映射到不同的latent space会enrich 模型的信息，所以在HIN模型中，也是同样的做法，将得到的entity rep映射到不同的<span class="math inline">\(K\)</span>个latent space，即： <span class="math display">\[
  E^k_a=W^{(1)}_k(RELU(W^{(0)}_kE_a))
  \]</span> 之后，由于又收到TransE算法的影响，知：<span class="math inline">\(e_h+e\approx e_t\)</span>，所以作者认为<span class="math inline">\(E_b-E_a\)</span>能够在某种程度上反应entity pair <span class="math inline">\((e_a,e_b)\)</span>的relation。所以第k个latent space的结果是： <span class="math display">\[
  I^k_e=[E^k_aR^kE^k_b;E^k_b-E^k_a;E^k_a;E^k_b]
  \]</span> 最终我们将k个latent space的结果进行concat，当然了，我们也加入相对距离进一步丰富信息，如下： <span class="math display">\[
  I_e=G_e([I^1_e,I^2_e,...,e^K_e;M_{ba}-M_{ab}])
  \]</span> 其中，<span class="math inline">\(G_e\)</span>是FFN，<span class="math inline">\(d_{ab}\)</span>是相对距离。</p></li>
<li><p><strong>hierarchical document-level inference module：</strong>这一部分又分为两部分：sentence-level inference与document-level inference。在sentence-level inference中，假设一个document有<span class="math inline">\(L\)</span>个句子，<span class="math inline">\(w_{jt}\)</span>表示第<span class="math inline">\(j\)</span>个sentence的第<span class="math inline">\(t\)</span>个word，我们使用BILSTM对每一个sentence进行编码，如下： <span class="math display">\[
h_{jt}=BILSTM(w_{jt}),j=1,...,L
\]</span> 然后借鉴HAN模型，对<span class="math inline">\(h_{jt}\)</span>使用attention，然后得到sentence vector，如下： <span class="math display">\[
\alpha_{jt}=u^T_wtanh(W_wh_{jt}+b_w) \\
a_{jt}=\frac{exp(\alpha_{jt})}{\sum_{t}exp(\alpha_{jt})} \\
S_j=\sum_{t}a_{jt}h_{jt}
\]</span> 其中，<span class="math inline">\(S_j\)</span>就是第j个sentence的sentence vector。我们进一步将其与<span class="math inline">\(I_e\)</span>进行融合，如下： <span class="math display">\[
I_{sj}=G_s([S_j;I_e;S_j-I_e;S_j  \ o \ I_e]) \\
\]</span> <span class="math inline">\(S_{sj}\)</span>就是最终的第j个sentence的sentence vector。</p>
<p>在document-level inference中，我们还是借鉴HAN模型，使用BISLTM+attention的方式，得到最终document vector，如下： <span class="math display">\[
c_{sj}=BILSTM_D(I_{sj}),j\in[1,L] \\
\alpha_{j}=u^T_stanh(W_sc_{sj}+b_s) \\
a_{jt}=\frac{exp(\alpha_{j})}{\sum_{j}exp(\alpha_{j})} \\
I_d=\sum_{j}a_{j}c_{sj}
\]</span> <span class="math inline">\(I_d\)</span>就是最终的document vector。</p></li>
<li><p><strong>prediction layer：</strong>我们首先对entity-level inference rep与document-level inference rep进行concat，再输入到FFN中进行最终得分类。如下： <span class="math display">\[
P(r|E_a,E_b)=sigmoid(W_r \left[\begin{matrix}I_e \\ I_d \end{matrix}\right]+b_r)
\]</span> 最终的loss采用CE。</p></li>
</ul>
<h3 id="experiment-1">Experiment</h3>
<ol type="1">
<li><p>数据集：DocRED</p></li>
<li><p>结果</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/HIN_result.jpg"></p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/HIN_ablation.jpg"></p>
<p>从结果看，更加一般了，而后在case study中所说明的logical reasoning、coreference reasoning、combine common-sense information问题也没有解决。</p></li>
</ol>
<h2 id="document-level-relation-extraction-with-adaptive-thresholding-and-localized-context-poolingarxiv2020">Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling(arxiv2020)</h2>
<h3 id="background-2">Background</h3>
<p>这篇文章是JD放在arxiv上的文章，从结果来看是目前的top1，值得一读。它也是解决document-level RE。它的核心思想是：目前主流的做法是采用基于graph的方法来做，但是很多基于BERT的工作(譬如corefBERT)也能够得到很好的结果，并且在基于graph的模型的实验部分，也都证明了BERT以及BERT-like预训练模型的巨大提升，以至于让人怀疑是否有必要引入GNN？作者发现如果只用BERT的话，那么对于不同的entity pair，entity的rep都是一样的，这是一个很大的问题，那是否能够不引入graph的方式来解决这个问题呢？这就有了ATLOP模型。</p>
<blockquote>
<p>说实话，这篇paper更多的是解决多标签分类问题，因为即便不是document-level RE，另外随便来个多标签分类，应该也可以得到很好的效果。</p>
</blockquote>
<h3 id="model-2">Model</h3>
<p>ATLOP模型整体是基于BERT而不是graph来做的。所以我分两部分阐述：主体部分+改良部分。</p>
<h4 id="backbone">backbone</h4>
<ul>
<li><p><strong>task definition：</strong>给定document <span class="math inline">\(d\)</span>以及一系列entity <span class="math inline">\(\{e_i\}_{i=1}^{n}\)</span>，目标是得到entity pair <span class="math inline">\((e_s,e_o)\)</span>的relation <span class="math inline">\(r\)</span>，<span class="math inline">\(r\in {\cal R}\cup \{NA\}\)</span>。</p></li>
<li><p><strong>encoder：</strong>给定document <span class="math inline">\(d=[x_t]_{t=1}^l\)</span>，对于每一个可能的mention的开头与结尾，加入<code>*</code>来进行标注，然后将sentence输入到BERT当中，得到contextual的rep。即：<span class="math inline">\([h_1,...,h_l]=BERT([x_1,...,x_l])\)</span>。mention embedding由mention首部的<code>*</code>rep来表示，entity rep是对其所有的mention的rep进行logexpsum得到的，即：<span class="math inline">\(h_{e_i}=log\sum_{j=1}^{N_{e_i}}exp(h_{m_j^i})\)</span>，其中<span class="math inline">\(m_j^i\)</span>表示entity <span class="math inline">\(e_i\)</span>的第<span class="math inline">\(j\)</span>个mention。</p></li>
<li><p><strong>binary classification：</strong>对于entity pair <span class="math inline">\((h_{e_s},h_{e_o})\)</span>，我们对其分别进行编码，并使用bilinear得到最终的结果，如下： <span class="math display">\[
z_s=tanh(W_sh_{e_s}) \\
z_o=tanh(W_oh_{e_o}) \\
P(r|e_s,e_o)=sigmoid(z^T_sW_rz_o+b_r)
\]</span> 在ATLOP里，为了减少<span class="math inline">\(W_r\)</span>的参数量，使用了group bilinear的技术，其实就是multi-head attention的思想，就是将<span class="math inline">\(h_{e_s}与h_{e_o}\)</span>的维度分为k份，分别进行bilinear操作，最后对结果进行concat，再使用sigmoid得到结果。</p></li>
</ul>
<h4 id="improvement">improvement</h4>
<p><strong>adaptive thresholding</strong></p>
<p>backbone中描述的就是不基于graph的基本操作，但是存在一个问题：我们使用sigmoid函数的时候，需要确定一个阈值(一般是0.5)，当结果大于这个阈值的时候，我们给它positive label，当它小于这个阈值的时候，我们给它negative label。但是作者发现，对于DRE这样的多实体多标签问题，不同的entity pair的不同的realtion判断，应该是有不同的阈值才能够得到更好的结果。一般我们会手动去试不同的阈值在dev上的效果，选取F1值最好的阈值。但是人工选择仍然无法达到最优解(近似最优解)，是否可以让模型自己来选择阈值呢？在ATLOP模型中，设计了adaptive thresholding机制来解决这个问题。</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/ATLOP_adaptive_thresholding.jpg" style="zoom:50%;"></p>
<p>具体来说，对于给定的entity pair <span class="math inline">\(T=(e_s,e_o)\)</span>，我们定义它的positive label set(<span class="math inline">\(\cal P_T\)</span>)与negative label set(<span class="math inline">\(\cal P_N\)</span>)：前者是<span class="math inline">\(T\)</span>有的relation，后者是<span class="math inline">\(T\)</span>没有的realtion。我们追求的是：<strong>positive label的logits比negative label的要高，并且两者差距越大越好。</strong>因此，在ATLOP模型中，引入了<code>TH</code>这个class(<code>TH</code>class的概率计算仍然和计算普通的relation是一样的)，在测试的时候，对于给定的entity pair，我们返回的relation class是比<code>TH</code>class的logits要高的class。具体的公式如下： <span class="math display">\[
\cal L_1=\sum_{r\in P_T}log(\frac{exp(logits_r)}{sum_{r^{\prime}\in \cal P_T\cup\{TH\}}exp(logits_r^{\prime})}) \\
\cal L_2=log(\frac{exp(logits_{TH})}{sum_{r^{\prime}\in \cal P_T\cup\{TH\}}exp(logits_r^{\prime})}) \\
\cal L=L_1+L_2
\]</span> 这里有个疑问，为什么不计算negative label的loss？🧐</p>
<p><strong>localized context pooling</strong></p>
<p>对于不同entity pair中的entity，其实最终用于分类的rep是一样的，而实际上，对于不同的entity pair，它所表达的信息却有可能完全不一样，并且在document中与之相关的信息的位置也是不一样的。为了解决这个问题，在ATLOP模型中，提出了LOP技术。</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/ATLOP_LOP.jpg" style="zoom:50%;"></p>
<p>具体来说，LOP技术，就是使用BERT中最后一个transformer layer中的multi-head self attention，将其融入到最终每一个entity的rep中，从而在不同的entity pair中，每一个entity的rep能够聚合其最相关的信息，个人认为这是这篇paper中最出彩的地方。具体公式如下： <span class="math display">\[
A^{(s,o)}=A^E_s·A^E_o \\
q^{(s,o)}=\sum_{i=1}^{H}A^{(s,o)}_i \\
a^{(s,o)}=q^{(s,o)} \ or \ 1^Tq^{(s,o)} \\
c^{(s,o)}=H^Ta^{(s,o)}
\]</span> 其中<span class="math inline">\(A^E_s、A^E_o\)</span>是entity-level attention，然后我们将<span class="math inline">\(c^{(s,o)}\)</span>与<span class="math inline">\(h_{e_s}、h_{e_o}\)</span>相结合，如下： <span class="math display">\[
z_s^{(s,o)}=tanh(W_sh_{e_s}+W_{c_1}c^{(s,o)}) \\
z_o^{(s,o)}=tanh(W_oh_{e_o}+W_{c_2}c^{(s,o)}) \\
\]</span> 接下来操作就是一样的了，直接送入sigmoid就行。</p>
<h3 id="experiment-2">Experiment</h3>
<ol type="1">
<li><p>数据集：CDR、GDA、DocRED</p></li>
<li><p>结果</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/ATLOP_result_docred.jpg"></p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/ATLOP_result_CDR_GDA.jpg" style="zoom:50%;"></p>
<p>从结果上看，非常可以。</p></li>
<li><p>消融实验</p>
<p>作者对各个component中进行了消融实验(dev)，如下</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task-PART-TWO/ATLOP_ablation_1.jpg" style="zoom: 50%;"></p>
<p>说实话，这个adaptive thresholding loss的效果真是出乎我的意料，我感觉掉到41.74%有点太不正常了，即便不用BERT，用BILSTM也能达到50%，不是很懂这个结果。🧐</p></li>
</ol>
<h2 id="entity-and-evidence-guided-relation-extraction-for-docredarxiv2020">Entity and Evidence Guided Relation Extraction for DocRED(arxiv2020)</h2>
<h3 id="background-3">Background</h3>
<h3 id="model-3">Model</h3>
<h3 id="experiment-3">Experiment</h3>
<h2 id="references">References</h2>
<p>《Global-to-Local Neural Networks for Document-Level Relation Extraction》</p>
<p>《HIN: Hierarchical Inference Network for Document-Level Relation Extraction》</p>
<p>《Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling》</p>
<p>《Entity and Evidence Guided Relation Extraction for DocRED》</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>关系抽取</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Document-level RE</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|RE-Integrate GNN into Document-level RE task|PART ONE</title>
    <url>/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/</url>
    <content><![CDATA[<p>这篇博客主要是讲解图神经网络在Document-level关系抽取上的应用，共讲解4篇论文：《Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network》、《Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs》、《Reasoning with Latent Structure Reﬁnement for Document-Level Relation Extraction》、《Double Graph Based Reasoning for Document-level Relation Extraction》。</p>
<a id="more"></a>
<h2 id="inter-sentence-relation-extraction-with-document-level-graph-convolutional-neural-networkacl2019">Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network(ACL2019)</h2>
<h3 id="background">Background</h3>
<p>文档级关系抽取的难点在于：我们往往需要抽取的entity pair是span across multi-sentences，所以我们必须对entity的local、non-local、semantic、dependency等信息进行提取和建模。这篇paper提出的GCNN，正是以word为node，words之间的local与non-local dependency作为edge，来构建document-level graph，从而解决文档级关系抽取问题。</p>
<h3 id="model">Model</h3>
<p>先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/GCNN_model.jpg"></p>
<p>GCNN模型分为4部分：<strong>input layer、graph construction、GCNN layer、MIL-based relation classification。</strong></p>
<ul>
<li><p><strong>task definition：</strong>给定一个document <span class="math inline">\(t\)</span>: <span class="math inline">\([w_1,w_2,...,w_n]\)</span>，<span class="math inline">\(e_1,e_2\)</span>是<span class="math inline">\(t\)</span>中的两个entity，我们的目标是：提取出<span class="math inline">\(e_1\)</span>与<span class="math inline">\(e_2\)</span>之间的relation。</p></li>
<li><p><strong>input layer：</strong>对于document中的每一个word <span class="math inline">\(i\)</span>，我们将它本身以及它与我们target entity pair的的相对位置分别编码为三个向量：<span class="math inline">\(w_i,d^1_i,d^2_i\)</span>，如果一个entity有多个mention的话，那么就选择与当前word最接近的mention来计算相对位置。最后，每一个word的representation为：<span class="math inline">\(x_i=[w_i;d^1_i;d^2_i]\)</span>。</p></li>
<li><p><strong>graph construction：</strong>我们需要根据document来构建graph。在GCNN中，node set就是每一个word，edge set有5种：</p>
<ul>
<li><strong>Syntactic dependency edge：</strong>句法依赖，也就是使用每一个sentence中的word之间的句法关系建立edge；</li>
<li><strong>Coreference edge：</strong>指代，对于表示同一个含义的phrase，进行连接；</li>
<li><strong>Adjacent sentence edge：</strong>将sentence的根结点鱼上下文sentence的根结点进行连接；</li>
<li><strong>Adjacent word edge：</strong>对于同一个sentence，我们去连接当前word的前后节点；</li>
<li><strong>self node edge：</strong>word与本身进行连接；</li>
</ul></li>
<li><p><strong>GCNN layer：</strong>在构建好doucment graph的基础上，使用GCNN来计算得到每一个node的representation。这里使用的GCNN与普通的GCN不同，这里在aggregate node representation的时候，只使用了其邻域的信息，并且对于不同类型的edge，分别使用GCN(毕竟GCN只能用于同质图)，最终的结果是所有类型的graph的结果的加和。我认为这里是GCNN模型最出彩的地方了，虽然现在来看并没有什么很新颖的，不过这种GCN的使用方法并不是很好，参见how power GNN这篇论文。公式如下： <span class="math display">\[
x^{k+1}_i=f(\sum_{u\in v(i)}(W^k_{l(i,u)}x^k_u+b^k_{l(i,u)}))
\]</span> 其中，<span class="math inline">\(f()\)</span>表示某种encoder(e.g. relu)。</p></li>
<li><p><strong>MIL-based relation classification：</strong>这里使用MIL。因为在一篇document中，每一个entity会有多个mention，我们希望能够去聚合target entity所有的mention，并通过bi-affine pairwise scoring来进行最终的关系分类。具体公式如下： <span class="math display">\[
x_i^{head}=W^{(1)}_{head}(RELU(W^{(0)}_{head}x_i^K)) \\
x_i^{tail}=W^{(1)}_{tail}(RELU(W^{(0)}_{tail}x_i^K)) \\
scores(e^{head},e^{tail})=log\sum_{i\in E^{head},j\in E^{tail}}exp((x^{head}_iR)x^{tail}_j)
\]</span></p></li>
</ul>
<h3 id="experiment">Experiment</h3>
<ol type="1">
<li><p>数据集：CDR、CHR(这种两个biochemistry领域的document-level 关系抽取数据集)</p></li>
<li><p>结果</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/GCNN_result.jpg" style="zoom:50%;"></p>
<p>结果就不分析了，没什么好分析的，总的来说，GCNN中规中矩吧，没有太出色的地方，缺点在于：构建了异质图，但是却没有考虑到不同类型的edge的作用；对于mention的处理不够好；logical reasoning几乎没有另外处理，纯靠GCNN；edge的构建需要其他的工具，不一定准确；对于word representation的表示不够。</p></li>
</ol>
<h2 id="connecting-the-dots-document-level-neural-relation-extraction-with-edge-oriented-graphsemnlp2019">Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs(EMNLP2019)</h2>
<h3 id="background-1">Background</h3>
<p>这篇paper也是解决document-level relation extraction问题，提出的模型基于以下几个发现：1. 之前很多的graph based model都是基于node的，然而作者发现entity 之间的relation，可以通过节点之间路径来形成唯一的edge representation，从而更好地得到表达；2.每一个target entity的mentions对于entity之间的relation是非常重要的。这篇paper提出的EoG模型很好地解决了这两个问题。</p>
<h3 id="model-1">Model</h3>
<p>先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/EoG_model.jpg"></p>
<p>EoG模型分为四部分：<strong>sentence encdoing layer、graph construction、inference layer、classification layer。</strong></p>
<ul>
<li><p><strong>task definition：</strong>给定标注好的document(entity与mentions)，目标是抽取出所有entity pair的relation。</p></li>
<li><p><strong>sentence encoding layer：</strong>doucment中的每一个sentence的word都被编码为一个vector，实际上，这样得到的是4维的张量[batch_size,nums_seqs,seq_length,embedding_dim]，然后将其输送到BILSTM中进行编码，得到contxtual representation。</p></li>
<li><p><strong>graph costruction：</strong>graph construction分为两部分：node construction与edge construction。</p>
<ul>
<li><p><strong>node construction：</strong>在EoG模型中，有三种node：mention node(<span class="math inline">\(n_m\)</span>)、entity node(<span class="math inline">\(n_e\)</span>)、sentence node(<span class="math inline">\(n_s\)</span>)。mention node是所有entity的mentions的集合。每一个mention node的representation是此mention的所有word embedding的平均；entity node是所有entity的集合，每一个entity node的representation是该entity所有的mentions的平均；sentence node是所有sentence的集合，每一个sentence node是该sentence中所有word embedding的平均。除此之外，我们为了区别不同类型的node，还给每一个node的representation上concat对应类型的node embedding。所以最终的表示: <span class="math display">\[
n_m=[avg_{w_i\in m}(w_i);t_m] \\
n_e=[avg_{m_i\in e}(m_i);t_e] \\
n_s=[avg_{w_i\in s}(w_i);t_s] \\
\]</span></p></li>
<li><p><strong>edge construction：</strong>有五种edge：mention-mention(MM)、mention-entity(ME)、mention-sentence(MS)、entity-sentence(ES)、sentence-sentence(SS)。MM edge，我们是连接两个在同一个sentence的两个mention，并且其表示我们是concat这两个mention本身的representation+context+两个mention的距离。具体公式： <span class="math display">\[
x_{MM}=[n_{m_i};n_{m_j};c_{m_i,m_j};d_{m_i,m_j}] \\
\]</span> <span class="math inline">\(x_{MM}\)</span>表示的是对于mention pair <span class="math inline">\((m_i,m_j)\)</span>的MM edge，当i=1，j=2时，<span class="math inline">\(x_{MM}=[n_{m_1};n_{m_2};c_{m_1,m_2};d_{m_1,m_2}]\)</span>,其中<span class="math inline">\(c_{m_1,m_2}\)</span>的计算如下： <span class="math display">\[
\alpha_{k,i}=n^T_{m_k}w_i \\
a_{k,i}=\frac{exp(\alpha_{k,i})}{\sum_{j\in[1,n],j\not\in m_k}exp(\alpha_{k,j})} \\
a_i=(a_{1,i}+ a_{2,i})/2 \\
c_{m_1,m_2}=H^Ta \\
k=\{1,2\}
\]</span> 其中，<span class="math inline">\(a_i\)</span>表示sentence的第<span class="math inline">\(i\)</span>个word对此mention pair的重要性程度，也就是attention weight。</p>
<p>ME edge，我们连接所有的mention与其对应的entity，<span class="math inline">\(x_{ME}=[n_m;n_e]\)</span>；</p>
<p>MS edge，将mention与此mention所在的sentence node进行连接，<span class="math inline">\(x_{MS}=[n_m,n_s]\)</span> ;</p>
<p>ES edge，如果一个sentence中至少存在一个entity的mention，那么我们将setence node与entity node进行连接，<span class="math inline">\(x_{ES}=[n_e;n_s]\)</span>;</p>
<p>SS edge：将所有的sentence node进行连接，以获得non-local information，<span class="math inline">\(x_{SS}=[n_{s_i};n_{s_j};d_{s_i,s_j}]\)</span>，其中<span class="math inline">\(d_{s_i,s_j}\)</span>表示两个sentence的距离vector。</p>
<p>当然了，我们最终的目的是<strong>提取出entity pair的relation</strong>，所以我们对所有的edge representation都做一个线性变换，从而让其唯独一致。即：<span class="math inline">\(e^{(1)}_z=W_zx_z,z\in[MM,ME,MS,ES,SS]\)</span>。</p></li>
</ul></li>
<li><p><strong>inference layer：</strong>由于我们没有直接的EEedge，所以我们需要得到entity之间的唯一路径的表示，来产生EE edge的representation。这里使用了two-step inference mechanism来实现这一点。</p>
<ul>
<li><p>the first step：利用中间节点<span class="math inline">\(k\)</span>在两个节点<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>之间产生一条路径，如下： <span class="math display">\[
f(e^{(l)}_{ik},e^{(l)}_{kj})=\sigma(e^{(l)}_{ik}\odot e^{(l)}_{kj})
\]</span></p></li>
<li><p>The second step：将原始的edge(如果有的话)与所有新产生的edge进行聚合，如下： <span class="math display">\[
e^{(2l)}_{ij}=\beta e^{(2l)}_{ij}+(1-\beta)\sum_{k\not=i,j}f(e^{(l)}_{ik},e^{(l)}_{kj})
\]</span> 重复上述两步N次，我们就可以得到比较充分混合的EE edge。实际上，这一步就是为了解决logical reasoning。</p></li>
</ul></li>
<li><p><strong>classification layer：</strong>这里使用softmax进行分类，因为实验所使用的两个数据集其实都是每一个entity pair都只有一个relation。具体公式： <span class="math display">\[
y=softmax(W_ce_{EE}+b_c)
\]</span></p></li>
</ul>
<h3 id="experiment-1">Experiment</h3>
<ol type="1">
<li><p>数据集：CDR、GDA</p></li>
<li><p>实验结果：</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/EoG_result.jpg"></p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/EoG.jpg" style="zoom:50%;"></p>
<p>结果没什么好看的，主要来看一下ablation。</p></li>
<li><p>ablation</p>
<p>EoG模型做了一些消融实验。首先是对EoG(full)、EoG(NoInf)、EoG(Sent)在不同长度的entity pair进行实验：</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/EoG_result2.jpg" style="zoom:50%;"></p>
<p>可以看到，当entity pair之间相差4句以上时，结果明显要好，这说明原始的EoG忽略了一些重要节点之间的交互信息，那么能不能让模型自动选择哪些节点要交互，哪些节点不要交互呢？(在LSR就是这么做的)除此之外，作者还对不同的compnent进行了消融实验。如下：</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/EoG_ablation.jpg" style="zoom:50%;"></p>
<p>从结果可以看到，去掉SS对结果影响巨大，这说明对于document-level RE，提取inter-sentence之间的交互信息是非常重要的，另外，MM似乎对结果影响最小，但是我认为MM对于entity pair的relation identification是非常重要的，只是EoG里面构造的方式不对，在之后的GAIN模型里面，可以看到MM对结果提升巨大，当然构建方式不一样。</p></li>
<li><p>当然了，最后还做了一些bad case的分析，有三种：使用and相连接的entity，model无法找到她们之间的relation；缺少coreference；3.不完全的实体链接。个人觉得EoG模型是一个非常好的开端，提供了很多思路，值得细细品读。</p></li>
</ol>
<h2 id="reasoning-with-latent-structure-reﬁnement-for-document-level-relation-extractionacl2020">Reasoning with Latent Structure Reﬁnement for Document-Level Relation Extraction(ACL2020)</h2>
<h3 id="background-2">Background</h3>
<p>这篇我个人感觉是EoG模型的改良，针对的问题就是：在EoG模型的消融实验中，发现full version在长度大于4时的效果要比原始的EoG模型要好，那么在full的情况下，是否可以让模型自动选择哪些边重要，那些不重要呢？这就是LSR模型。</p>
<blockquote>
<p>个人觉得这篇相对较难，主要是其使用矩阵树原理，对数学要求较高，如果觉得看不懂，只看个大概也是OK的，这篇个人觉得没有follow的价值。</p>
</blockquote>
<h3 id="model-2">Model</h3>
<p>LSR模型分为三部分：<strong>node constructor、dynamic reasoner、classifer。</strong></p>
<ul>
<li><p><strong>node constructor：</strong>这一部分分为两小部分：context encoding与node extraction，主要就是对document中的所有word进行编码，并得到graph所有类型的node的representation。先放图·</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/LSR_node_constructor.jpg"></p>
<ul>
<li><p>Context encoding：对于给定的document <span class="math inline">\(d\)</span>，我们将其输入到一个encoder中(BILSTM/BERT etc)，得到contextual representation。</p></li>
<li><p>node extraction：在LSR中，有三种node：mention node、entity node以及meta dependancy path node。mention node表示的是一个sentence中entity的所有的mention，其表示是该mention中的所有word的representation的平均；entity node指的是entity node，其表示是所有mention node的representation的平均；MDP表示一个句子中所有mention的最短依赖路径集，在MDP元依赖路径中，mention和单词的表示分别被提取为提及节点和MDP节点。(说实话，不是很懂MDP。🧐)</p>
<blockquote>
<p>⚠️LSR与EoG模型不同的地方之一在于：mention node与entity node一样的，但是LSR没有sentence node，并且使用了MDP node来代替，本质上差不多吧，不过相比于sentence node，MDP node能够过滤掉无关信息(paper原话🧐)，但是说实话，在context encoding中，已经引入了无关信息吧，而且在之后的实验中，确实也证明MDP没啥用。</p>
</blockquote></li>
</ul></li>
<li><p><strong>dynamic reasoner：</strong>这一部分就是inference，因为在EoG里面，已经证明了没有inference对最终结果影响还是蛮大的。主要分为两部分：<strong>structure induction与multi-hop reasioning</strong>。先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/LSR_dynamic_reasoner.jpg" style="zoom:50%;"></p>
<ul>
<li><p>structure induction：这一部分主要是用来学习文档图的结构，从而得到文档图的邻接矩阵，从而以便使用GCN来进行aggregate。公式如下： <span class="math display">\[
s_{ij}=(tanh(W_pu_i)^TW_b(tanh(W_cu_j)) \\
s_i^r=W_ru_i \\
P_{ij}=\begin{cases} 0,&amp;if \ i=j, \\ exp(s_{ij}),&amp;otherwise \end{cases} \\
L_{ij}=\begin{cases}\sum_{i^{&#39;}=1}^{n}P_{i^{&#39;}j},&amp;if \ i=j, \\ -P_{ij},&amp;otherwise \end{cases} \\
\tilde L_{ij}=\begin{cases} exp(s_i^r), &amp;if \ i=1,\\ 0, &amp;if \ i&gt;1 \end{cases} \\
A_{ij}=(1-\delta_{1,j})P_{ij}[\tilde L^{-1}]_{ij}-(1-\delta_{i,1})P_{ij}[\tilde L^{-1}]_{ij}
\]</span> 其中，<span class="math inline">\(s_{ij}\)</span>表示的是node i与node j之间的关联度，<span class="math inline">\(s_i^r\)</span>表示的是node i被当作structure root node的非规范化概率，<span class="math inline">\(P_{ij}\)</span>表示的是node i与node j之间的edge的权重，<span class="math inline">\(L\)</span>是laplace矩阵，<span class="math inline">\(\tilde L\)</span>是变体，为了进一步计算用的，<span class="math inline">\(A\)</span>表示的是文档图的邻接矩阵。</p>
<blockquote>
<p>这里使用了structure attention network以及矩阵树原理的思想，这里不具体展开，想要深究的可以百度或者Google。</p>
</blockquote></li>
<li><p>multi-hot reasoning：在得到邻接矩阵之后，LSR便使用GCN来对graph进行aggregate。公式如下： <span class="math display">\[
u^l_i=\sigma(\sum_{j=1}^{n}A_{ij}W^lu_{j}^{l-1}+b^l)
\]</span> 当然了，我们可以重复多次dynamic reasoner模块，从而得到更加丰富的node representation。</p></li>
</ul></li>
<li><p><strong>classifier：</strong>这一步就是直接对entity pair进行关系分类，使用sigmoid函数。如下： <span class="math display">\[
P(r|e_i,e_j)=\sigma(e_i^TW_ee_j+b_e)
\]</span></p></li>
</ul>
<h3 id="experiment-2">Experiment</h3>
<ol type="1">
<li><p>数据集：CDR、GDA、DocRED</p></li>
<li><p>结果</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/LSR_result.jpg"></p>
<p>从在DocRED上的结果来看，可以说是非常好。我们在来看一下其在CDR与GDA数据集上的结果。</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/LSR_CDR_GDA.jpg"></p>
<p>在没有MDP节点的情况下，取得了SOTA，说明MDP的作用一般。总的来说，LSR模型确实很新颖，但是follow的价值一般。</p></li>
</ol>
<h2 id="double-graph-based-reasoning-for-document-level-relation-extractionemnlp2020">Double Graph Based Reasoning for Document-level Relation Extraction(EMNLP2020)</h2>
<h3 id="background-3">Background</h3>
<p>这篇paper也是继承了EoG模型，主要解决三个问题：1.一个relation的subject与object可能位于不同的sentence，不能仅仅利用一个句子来得到relation；2.同一个entity可能会出现在不同的sentence当中，因此需要利用cross-sentence context information，从而更好的表示entity；3.很多relation需要logical reasoning(main issue)。为此提出了GAIN模型。个人认为这篇paper的模型比较优雅，挺好的。</p>
<h3 id="model-3">Model</h3>
<p>先放图～</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/GAIN_model.jpg"></p>
<p>GAIN模型分为四部分：<strong>encoding module、mention-level graph aggregation module、entity-level graph aggregation module、classification module。</strong></p>
<ul>
<li><p><strong>task definition：</strong>给定一篇含有<span class="math inline">\(N\)</span>个sentence的documnt：<span class="math inline">\(\cal D=\{s_i\}_{i=1}^{N}，s_i=\{w_j\}_{j=1}^{M}\)</span>，以及<span class="math inline">\(P\)</span>个entity：<span class="math inline">\(\cal E=\{e_i\}_{i=1}^{P},e_i=\{m_j\}_{j=1}^{Q}\)</span>，其中<span class="math inline">\(e_i\)</span>表示第<span class="math inline">\(i\)</span>个entity，<span class="math inline">\(m_j\)</span>表示第<span class="math inline">\(i\)</span>个entity的第<span class="math inline">\(j\)</span>个mention。我们的目标是entity之间的relation。</p></li>
<li><p><strong>encoding module：</strong>这一部分主要是将document中的word经过编码，得到contextual representation。给定有n个word的document：<span class="math inline">\(\cal D=\{w_i\}_{i=1}^{n}\)</span>,然后将word embedding与type embedding以及coreference embedding进行concat，得到final word embedding，即： <span class="math display">\[
x_i=[E_w(w_i);E_t(t_i);E_c(c_i)]
\]</span> 其中，<span class="math inline">\(t_i\)</span>与<span class="math inline">\(c_i\)</span>是<span class="math inline">\(w_i\)</span>对应的named entity type id与entity id，对于那些不属于任何entity的word，我们引入<code>None</code>type。得到final word embedding之后，我们将其输入到一个encoder当中(BISLTM/BERT etc)，得到这一层的输出： <span class="math display">\[
[g_1,...,g_n]=Encoder([x_1,...,x_n])
\]</span></p></li>
<li><p><strong>mention-level graph aggregation module：</strong>这个graph的构建主要是对mention之间的关系进行建模。在mention-level graph当中，<strong>node set：mention node与document node。</strong>mention node就是表示每一个mention，document node是一个虚拟节点，主要是为了对document information进行建模，同时也是视其为一个中继节点，让不同mention之间的交互变得更加容易。<strong>edge set：intra-entity edge、inter-entity edge、document edge。</strong>intra-entity edge是让同一个entity的mention彼此连接；inter-entity edge是让一个句子内的不同entity的mention之间进行连接；document edge是让所有的mention与document node进行连接。</p>
<p>构建好graph之后，GAIN模型使用与GCNN中同样的GCN，如下： <span class="math display">\[
h^{(l+1)}_{u}=\sigma(\sum_{k\in{\cal K}}\sum_{v\in \cal N_k(u)}W^{(l)}_kh^{(l)}_v+b^{(l)}_k)
\]</span> 但是这种虽然很接近WL-test，但是会存在范数不收敛的情况，这个真的不需要考虑一下吗？🧐在stack N层GCN之后，对于每一个node，我们去concat所有layer的representation，来作为每一个node的final representation，即：<span class="math inline">\(m_u=[h^{(0)}_u;h^{(1)}_u;...,h^{(N)}_u]\)</span>。</p></li>
<li><p><strong>entity-level graph inference module：</strong>这一步就是进行inference，得到entity-entity的表示，用于最终的分类，所以path reasoning mechanism很重要。在entity-level graph中，我们将同一个entity的所有mention的表示的平均作为此entity的表示，即：<span class="math inline">\(e_i=\frac{1}{N}\sum_n m_n\)</span>。entity node之间的边表示：<span class="math inline">\(e_{ij}=\sigma(W_q[e_i;e_j]+b_q),\sigma \text{是激活函数}\)</span>。对于entity pair之间不同的路径，其表示是：<span class="math inline">\(p^i_{h,t}=[e_{ho};e_{ot};e_{to};e_{oh}]\)</span>，当然了，entity pair之间的path会有多个，我们使用attention机制，如下： <span class="math display">\[
s_i=\sigma ([e_h;e_t]W_lp^i_{h,t}) \\
\alpha_i=\frac{exp(s_i)}{\sum_jexp(s_j)} \\
p_{h,t}=\sum_i\alpha_ip^i_{h,t}
\]</span> 其中，<span class="math inline">\(\sigma\)</span>是某一个激活函数。这里只使用了2-hop，如果使用multi-hop，效果会不会更好？🧐</p></li>
<li><p><strong>classification module：</strong>这里就是最终的分类。但是这里借用了ESIM中表示，对于enitty pair <span class="math inline">\((e_h,e_t)\)</span>，其综合的inferential path information是： <span class="math display">\[
I_{h,t}=[e_h;e_t;|e_h-e_t|;e_h\odot e_t;m_{doc};p_{h,t}]
\]</span> 其中，<span class="math inline">\(m_{doc}\)</span>就是document node，然后我们将其看作一个多标签分类问题，使用sigmoid函数，来完成分类，如下： <span class="math display">\[
P(r|e_h,e_t)=sigmoid(W_b\sigma(W_aI_{h,t}+b_a)+b_b)
\]</span> 最终的loss使用CE。</p></li>
</ul>
<h3 id="experiment-3">Experiment</h3>
<ol type="1">
<li><p>数据集：DocRED</p></li>
<li><p>实验结果</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/GAIN_result.jpg"></p>
<p>从结果上看，GAIN的效果是令人惊艳的。我们再来看看ablation，如下：</p>
<p><img src="/2020/10/24/NLP-RE-Integrate-GNN-into-Document-level-RE-task/GAIN_result_2.jpg"></p>
<p>从消融实验来看，可以发现各个模块对于结果的话影响都是非常巨大的，很优雅，没有多余的设计。</p>
<p>目前来看，主要的处理：encoding？怎么构建图，才能够利用好mention、entity、sentence、document的信息（edge-oriented or node-oriented）？heterogeneous graph的处理？logical reasoning的处理？长期建模很重要，怎么更好地处理？</p></li>
</ol>
<h2 id="references">References</h2>
<p>《Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network》</p>
<p>code：见EoG的code，差不多</p>
<p>《Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs》</p>
<p>code：https://github.com/fenchri/edge-oriented-graph</p>
<p>《Reasoning with Latent Structure Reﬁnement for Document-Level Relation Extraction》</p>
<p>code：https://github.com/nanguoshun/LSR</p>
<p>《Double Graph Based Reasoning for Document-level Relation Extraction》</p>
<p>code：https://github.com/DreamInvoker/GAIN</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>关系抽取</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Document-level RE</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|谈谈预训练模型中的Adapter结构</title>
    <url>/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<p>最近应该会产出大量的关于预训练模型的解读的内容🤩，主要是目前预训练模型确实在几乎各个任务上的表现都超越了传统的模型。将预训练模型应用于各个领域，这也是一个大的趋势。这篇文章主要是通过AdapterBERT与K-Adapter两篇paper，来谈谈预训练模型中的Adapter结构。</p>
<a id="more"></a>
<h2 id="adapter-bert">Adapter-BERT</h2>
<p>Adapter-Bert来源于Google的《Parameter-Efﬁcient Transfer Learning for NLP》论文，主要目的在不降低模型效果的情况下，减小finetune时候的参数。感觉对于无GPU党来说，是一个非常大的福音。</p>
<h3 id="背景">背景</h3>
<p>首先讲讲为什么要有Adapter-Bert。在预训练模型中，可以分为两种：feature-based以及fine-tuning。feature-based指的是通过预训练提取出词向量，然后输入到下游任务当中，在训练过程中，词向量可以随之更新，也可以不更新，侧重的是offline的词向量的生成，典型代表是ELM；fine-tuning指的是首先预训练模型，然后接入下游任务，将预训练阶段的权重作为finetuning的初始化权重，之后将下游任务与预训练部分一起训练，侧重的是co-training，典型代表是BERT。一般来说，fine-tuning的效果要比feature-based要好一些，当然计算代价也要更高一些。但是，总体而言，finetuning的代价实在是太大了，因为我们每一次都要更新所有的参数，这对很多应用尤其是低资源以及multi-mask的应用场景非常不友好。所以，我们想要尽可能地减少fine-tuning的所需要更新的参数数目，同时还要保证尽可能地逼近fine-tuning时的结果，于是便有了Adapter-BERT。</p>
<h3 id="adapter-bert模型架构">Adapter-BERT模型架构</h3>
<p>Apdater-Bert的想法是将task-specific layer放在预训练模型中间，也就是加入Adapter结构，然后冻结住预训练模型参数，最后我们fientuning的时候，只更新Apdater、layerNorm以及与具体任务相关的layer的参数。具体结构图如下：</p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/Adapter-bert.jpg"></p>
<ul>
<li><strong>左图</strong>是Adapter-BERT中的transformer layer，我们可以看到每一个transformer layer增加了两个Adapter layer，分别加在LayerNorm之前，当然了，在进行LayerNorm之前，我们需要进行讲Apdater layer的输出进行残差连接。</li>
<li><strong>右图</strong>是Adapter layer的具体结构示意。这个其实让我想到了ALBERT中的低秩因式分解。假设输入input的维度是<span class="math inline">\(d\)</span>，我们首先通过一个FFN让其维度变为<span class="math inline">\(m\)</span>，且<span class="math inline">\(m&lt;&lt;d\)</span>；之后再通过一个FFN得到输出结果ouput，其其维度变为<span class="math inline">\(d\)</span>。最后，我们进行残差连接，即input+output作为Adapter layer的输出结果。另外，分析一下Apdater layer的参数，对于一个transformer layer来说，增加的参数是2*(2dm+d+m+2d+2d)，其中2d表示是LN的参数量，增加的参数量占总的参数量的3%。</li>
</ul>
<blockquote>
<p>思考一下，这里为什么要用残差连接？主要是因为当初始化的时候，权重都很小，残差连接可以保证模型输出与预训练模型相同。</p>
</blockquote>
<h3 id="实验结果">实验结果</h3>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/结果.jpg"></p>
<p>从结果来看，还是不错的，基本上能接近fine-tuning的结果。接下来看一下Ablation。</p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/Ablation.jpg"></p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/LN.jpg"></p>
<p>从结果中，我们可以得到如下结论：</p>
<ul>
<li>对于fine-tune来说，减小训练的层数会大幅降低准确率；而对于Adapter-based来说，几乎没有什么影响；</li>
<li>Fine-tune layerNorm没有什么用。</li>
</ul>
<p>作者另外还做了很多实验，但是发现基本上没有什么影响。</p>
<h3 id="代码部分">代码部分</h3>
<p>看完论文部分，我们来看一下代码实现部分。整体代码是在Google官方发布的BERT代码上改的，并且只修改了极小的一部分，首先是transformer部分，在每一个子层加上了Adapter layer。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                      adapter_fn=None)</span>:</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This is almost an exact implementation of the original Transformer encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  See the original paper:</span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Also see:</span></span><br><span class="line"><span class="string">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*seq_length,num_attention_heads*attention_head_size]</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`.</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          <span class="comment"># [batch_size*seq_length,hidden_size]</span></span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          <span class="keyword">if</span> adapter_fn:</span><br><span class="line">            <span class="comment"># adapter结构</span></span><br><span class="line">            attention_output = adapter_fn(attention_output)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        <span class="keyword">if</span> adapter_fn:</span><br><span class="line">          <span class="comment"># adapter 结构</span></span><br><span class="line">          layer_output = adapter_fn(layer_output)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># [batch_size,seq_length,hidden_size]</span></span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>
<p>具体的Apdater layer代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward_adapter</span><span class="params">(input_tensor, hidden_size=<span class="number">64</span>, init_scale=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">  <span class="string">"""A feedforward adapter layer with a bottleneck.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Implements a bottleneck layer with a user-specified nonlinearity and an</span></span><br><span class="line"><span class="string">  identity residual connection. All variables created are added to the</span></span><br><span class="line"><span class="string">  "adapters" collection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: input Tensor of shape [batch size, hidden dimension]</span></span><br><span class="line"><span class="string">    hidden_size: dimension of the bottleneck layer.</span></span><br><span class="line"><span class="string">    init_scale: Scale of the initialization distribution used for weights.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    Tensor of the same shape as x.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size*seq_length,hidden_size]</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"adapters"</span>):</span><br><span class="line">    <span class="comment"># in_size是self-attention layer的hidden_size，与这里的hidden_size不是一个概念！</span></span><br><span class="line">    in_size = input_tensor.get_shape().as_list()[<span class="number">1</span>]</span><br><span class="line">    w1 = tf.get_variable(</span><br><span class="line">        <span class="string">"weights1"</span>, [in_size, hidden_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=init_scale),</span><br><span class="line">        collections=[<span class="string">"adapters"</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    b1 = tf.get_variable(</span><br><span class="line">        <span class="string">"biases1"</span>, [<span class="number">1</span>, hidden_size],</span><br><span class="line">        initializer=tf.zeros_initializer(),</span><br><span class="line">        collections=[<span class="string">"adapters"</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,hidden_size]</span></span><br><span class="line">    net = tf.tensordot(input_tensor, w1, [[<span class="number">1</span>], [<span class="number">0</span>]]) + b1</span><br><span class="line"></span><br><span class="line">    net = gelu(net)</span><br><span class="line"></span><br><span class="line">    w2 = tf.get_variable(</span><br><span class="line">        <span class="string">"weights2"</span>, [hidden_size, in_size],</span><br><span class="line">        initializer=tf.truncated_normal_initializer(stddev=init_scale),</span><br><span class="line">        collections=[<span class="string">"adapters"</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    b2 = tf.get_variable(</span><br><span class="line">        <span class="string">"biases2"</span>, [<span class="number">1</span>, in_size],</span><br><span class="line">        initializer=tf.zeros_initializer(),</span><br><span class="line">        collections=[<span class="string">"adapters"</span>, tf.GraphKeys.GLOBAL_VARIABLES])</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,in_size]</span></span><br><span class="line">    net = tf.tensordot(net, w2, [[<span class="number">1</span>], [<span class="number">0</span>]]) + b2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 残差连接</span></span><br><span class="line">  <span class="keyword">return</span> net + input_tensor</span><br></pre></td></tr></table></figure>
<p>这是模型部分，除此之外，修改的部分是：我们在fine-tune的时候，要冻结原来的预训练模型的参数，只更新新加的部分。这部分代码在optimization.py文件中，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_optimizer</span><span class="params">(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu)</span>:</span></span><br><span class="line">  <span class="string">"""Creates an optimizer training op."""</span></span><br><span class="line">  global_step = tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Implements linear decay of the learning rate.</span></span><br><span class="line">  learning_rate = tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate=<span class="number">0.0</span>,</span><br><span class="line">      power=<span class="number">1.0</span>,</span><br><span class="line">      cycle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the</span></span><br><span class="line">  <span class="comment"># learning rate will be `global_step/num_warmup_steps * init_lr`.</span></span><br><span class="line">  <span class="keyword">if</span> num_warmup_steps:</span><br><span class="line">    global_steps_int = tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    global_steps_float = tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line"></span><br><span class="line">    warmup_percent_done = global_steps_float / warmup_steps_float</span><br><span class="line">    warmup_learning_rate = init_lr * warmup_percent_done</span><br><span class="line"></span><br><span class="line">    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    learning_rate = (</span><br><span class="line">        (<span class="number">1.0</span> - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># It is recommended that you use this optimizer for fine tuning, since this</span></span><br><span class="line">  <span class="comment"># is how the model was trained (note that the Adam m/v variables are NOT</span></span><br><span class="line">  <span class="comment"># loaded from init_checkpoint.)</span></span><br><span class="line">  optimizer = AdamWeightDecayOptimizer(</span><br><span class="line">      learning_rate=learning_rate,</span><br><span class="line">      weight_decay_rate=<span class="number">0.01</span>,</span><br><span class="line">      adapter_weight_decay_rate=<span class="number">0.01</span>,</span><br><span class="line">      beta_1=<span class="number">0.9</span>,</span><br><span class="line">      beta_2=<span class="number">0.999</span>,</span><br><span class="line">      epsilon=<span class="number">1e-6</span>,</span><br><span class="line">      exclude_from_weight_decay=[<span class="string">"LayerNorm"</span>, <span class="string">"layer_norm"</span>, <span class="string">"bias"</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_tpu:</span><br><span class="line">    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment">#--------------------只更新后来新增的参数-------------------------------#</span></span><br><span class="line"></span><br><span class="line">  tvars = []</span><br><span class="line">  <span class="keyword">for</span> collection <span class="keyword">in</span> [<span class="string">"adapters"</span>, <span class="string">"layer_norm"</span>, <span class="string">"head"</span>]:</span><br><span class="line">    tvars += tf.get_collection(collection)</span><br><span class="line">  grads = tf.gradients(loss, tvars)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is how the model was pre-trained.</span></span><br><span class="line">  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">  train_op = optimizer.apply_gradients(</span><br><span class="line">      zip(grads, tvars), global_step=global_step)</span><br><span class="line">  </span><br><span class="line"><span class="comment">#--------------------只更新后来新增的参数-------------------------------#</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normally the global step update is done inside of `apply_gradients`.</span></span><br><span class="line">  <span class="comment"># However, `AdamWeightDecayOptimizer` doesn't do this. But if you use</span></span><br><span class="line">  <span class="comment"># a different optimizer, you should probably take this line out.</span></span><br><span class="line">  new_global_step = global_step + <span class="number">1</span></span><br><span class="line">  train_op = tf.group(train_op, [global_step.assign(new_global_step)])</span><br><span class="line">  <span class="keyword">return</span> train_op</span><br></pre></td></tr></table></figure>
<p>到此，Adapter-BERT的论文预代码部分都讲完了☕️～</p>
<h2 id="k-adapter">K-Adapter</h2>
<p>K-Adapter模型来源于MSRA的《K-A DAPTER: Infusing Knowledge into Pre-Trained Models with Adapters》论文。它所要解决的问题是：在预训练模型中嵌入知识是非常重要的，因为当前的预训练模型无法学习到非常的知识信息；此外，在训练multi-task时，会出现灾难性遗忘问题，所谓的灾难性遗忘问题指的是：同一个模型，在学习完一个task之后，如果再学习新的任务，那么在原先的task上学习到的权重会被完全破坏掉，如果我们想要解决灾难性遗忘的问题，我们就需要用到持续学习。在这个背景下，K-Adapter预训练模型被提出来了。</p>
<h3 id="k-adapter模型架构">K-Adapter模型架构</h3>
<p>首先放一下K-Adapter预训练模型的架构图吧～</p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/K-Adapter%20total.jpg"></p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/adapter.jpg"></p>
<p>第一张图中的a部分表示的是传统的multi-task learning，b部分表示的是使用了K-Adapter结构之后的整体模型；第二张图表示的是Adapter layer。下面着重讲讲Adapter layer的构成以及模型是如何运作的。由于这篇paper没有放出源代码(都0202年，居然还有paper不放源代码🤷‍♂️)，所以整体内容的理解只能从论文来获得。</p>
<ul>
<li><p>整篇paper是以RoBERTa预训练模型作为基准，来进行设计的；</p></li>
<li><p>从整体来看，在使用了K-Adapter结构的模型中，我们是将Adapter结构独立出来，与pre-train model平行操作；此外，也不是每一个transformer layer都配有Adapter，在这篇paper里面， 是在RoBERTa Large 的第 0, 11, 23 层之后增加有 Adapter 层；</p></li>
<li><p>对于每一种知识，我们都去设计一个对应的Adapter，即所有的Apdater都是knowledge-specific的，这样一来，每一种knowledge产生的representation都不会互相干扰，从而解决了多任务学习中的灾难性遗忘的问题；</p></li>
<li><p>当我们只嵌入一种知识的时候，我们输出到task-specific layer的是knowledge-specific adapter最后一层的输出(最后一层的输出是pre-train model的输出与adapter的输出的concatenation)；当我们嵌入多个知识的时候，我们输出到task-specific layer的是所有knowledge-specific adapters最后一层的输出的concatenation；</p></li>
<li><p>对于单个 adapter layer，它的输入是：pre-train model中当前transformer层的输出结果与上一个adapter layer输出结果的concatenation；然后输入到一个投影层，即线性变换，然后再经过若干个transformer layer(论文是2个)，然后使用残差连接，将最初的输入与经过多个transformer layer输出结果进行concatenation，作为这个adapter layer的输出结果；</p></li>
<li><p>这篇paper使用了两种Adapter：factual Adapter与linguistic Adapter。factual Adapter 训练一个关系分类(relation classification)任务。通过判断三元组中 entity 是否存在相应关系来学习关系的知识。数据集是过滤 entity 出现小于 50 次的 T-RE-rc. 因为 Entity 长度不一，利用 Pooling 来对齐. 该任务训练 5epochs, Batch size 为 128；linguistic Adapter 则是完成预测依存关系中父节点 index 这个任务。数据集是利用 Stanford Parser 标注的 Book Corpus。因为是 token-level 的任务，最后过一个线性层输出到相应的分类。该任务训练 10epochs, Batch size 为 256；</p></li>
<li><p>在预训练的时候，pre-trained model是frozen的，只训练Adapter结构；但是在fine-tune的时候，还是和BERT一样，pre-trained model、所有的Adapter结构以及task-specific layer一起进行参数的更新；</p></li>
<li><p>与Adapter-BERT相比，K-Adapter更加侧重于对于多knowlwdge的处理以及灾难性遗忘的问题，而Adapter-BERT更加侧重于fine-tune阶段参数的减少。</p></li>
</ul>
<h3 id="实验结果-1">实验结果</h3>
<p>首先它对比了多个知识嵌入的预训练模型，我觉得还挺好的，如果之后在知识嵌入做文章的话，这里可以看作是一个小结～</p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/some%20embedding%20models.jpg"></p>
<p>然后我们来看一下实验结果～</p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/K-adapter-result.jpg"></p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/K-adapter-result2.jpg"></p>
<p><img src="/2020/07/02/NLP-%E8%B0%88%E8%B0%88BERT%E4%B8%AD%E7%9A%84Adapter%E7%BB%93%E6%9E%84/K-adapter-result3.jpg"></p>
<p>上面三张图分别是在：细粒度实体类型预测任务、常识QA与开放域QA任务、关系分类任务上的结果，总的来说，采用Adapter结构的预训练模型的效果确实有所提升～</p>
<p>以上就是预训练Adapter结构的内容啦，之后可能会再关注一下知识嵌入的预训练模型吧，这是一个大的方向，感觉目前还是有搞头的，over☕️～</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>AdapterBERT</tag>
        <tag>K-Adapter</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|深入探究Transformer模型</title>
    <url>/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>目前为止，已经学了很多东西，但是没有输出，总感觉似乎少了点什么。这片博客将回顾经典的Transformer模型。Transformer模型是Google在2017年所提出的模型。该模型抛弃了传统的RNN与CNN，全部采用Attention机制，结果证明其在当时取得了SOTA的效果，得到了广泛的应用。Transformer也是后来大火的BERT中的核心组成部分，所以Transformer模型的提出是非常具有开创性的工作。本文将首先介绍Transformer提出的背景，紧接着详细讲解其内部架构，最后对Transformer做一个小小的总结。</p>
<a id="more"></a>
<h2 id="transformer模型提出的原因">Transformer模型提出的原因</h2>
<p>在基于RNN的seq2seq+attention模型中，由于attention机制的加入，能够很好地解决信息损失与句子长距离依赖问题，但是它存在与普通RNN同样的问题。其t时刻的hidden state依赖于t-1时刻的hidden state与t时刻的输入，只有t-1时刻的hidden state计算完成后，才能够去计算t时刻的hidden state。这样固定的顺序计算在训练时会非常缓慢，无法并行计算，计算效率低下；此外，当训练集非常大的时候，网络的计算量也会变得非常庞大。</p>
<p>后来，基于CNN的seq2seq+attention模型被提出，它具有基于RNN的seq2seq+attention模型的捕捉长距离依赖的能力，并且可以并行化实现，但是缺点是：输入序列与输出序列越长，计算量越大。</p>
<p>所以，Transformer模型的提出主要是想在不损害性能的情况下，减少计算量并提高并行效率。</p>
<h2 id="transformer模型分析">Transformer模型分析</h2>
<h3 id="transformer模型概览">Transformer模型概览</h3>
<p>首先，没错，Transformer模型还是encoder-decoder架构，输入序列<span class="math inline">\((x_1,x_2,x_3,...,x_{T_X})\)</span>经过encoder，输出<span class="math inline">\((h_1,h_2,h_3,...,h_{T_X})\)</span>作为decoder的输入，最后经过decoder得到最终的输出序列<span class="math inline">\((y_1,y_2,y_3,..,y_{T_Y})\)</span>。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/0.0..png"></p>
<p>那么再具体一点，在Transformer中，Encoder部分由多个encoder堆叠组成，Decoder部分也是由多个decoder堆叠而成。需要注意的是：<strong>Encoder部分的输出会与Decoder部分的每一个decoder相结合，这是不同的地方。</strong></p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/0.模型架构概览.png"></p>
<p>宏观上的结构我们知道了，最后来看每一个encoder与decoder的内部结构。</p>
<p><strong>encoder层：</strong>每一个encoder层由两个子层构成。</p>
<p><strong>第一个子层(multi-head attention layer)：</strong>其输入是前一个encoder层的输出，输入首先进入multi-head attention机制，再接一个残差连接与层归一化；</p>
<p><strong>第二个子层(FFN layer)：</strong>其输入是第一个子层的输出，将第一个子层的输出输入到一个全连接的前馈神经网络，再接一个残差连接与层归一化，最后作为该encoder层的输出。</p>
<p><strong>decoder层：</strong>每一个decoder层由三个子层构成。</p>
<p><strong>第一个子层(masked multi-head attention layer)：</strong>其输入是前一个decoder层的输出，输入进入Masked multi-head attention机制，再接一个残差连接与层归一化；</p>
<p><strong>第二个子层(encoder-decoder attention layer)：</strong>其输入有两个：<strong>Encoder的输出与第一个子层的输出</strong>，经过multi-head attention机制，再接一个残差连接与层归一化；</p>
<p><strong>第三个子层(FFN layer)：</strong>其输入是第二个子层的输出，将第二个子层的输出输入到一个全连接的前馈神经网络，再接一个残差连接与层归一化，之后需要再接一个线性转换层，最后通过softmax，得到该decoder层输出的单词。</p>
<p><strong>这里需要明确的是：在decoder部分，并不是最后一次性把所有的序列全部decode出来，而是像RNN一样，一次只decode一个单词，因为下一个decoder层还需要用到前一个的输出作为输入。</strong></p>
<p>下面将一一讲解每一个部分。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/1.encoder-decoder内部架构.jpg"></p>
<h3 id="核心scaled-dot-product-attention与multi-head-attention">核心—Scaled Dot-Product Attention与Multi-Head Attention</h3>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/2.scaled%20-dot-product%20.jpg"></p>
<blockquote>
<p>在这里，首先要理解，传统的attention机制的本质其实可以理解为：输入序列中的元素可以想象为&lt;key,value&gt;对，那么给定target中某个要预测的元素Q，通过计算Q与每个key的相关性，得到每一个key对应value的权重系数（attention weights），再对value进行加和平均，就是最终的attention值（context vector）。在NLP中，key和value常常看做一个值（hidden state）。参看：<a href="https://blog.csdn.net/malefactor/article/details/78767781" target="_blank" rel="noopener">attention机制本质</a></p>
</blockquote>
<p>直接放公式： <span class="math display">\[
Attention(Q,K,V)=softmax(\frac {QK^{T}}{\sqrt{d_k}})V,
\]</span></p>
<p><span class="math display">\[
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O,其中 head_i=Attention(QW_i^Q,KW_i^K,VW_i^V).
\]</span></p>
<ol type="1">
<li>首先理解Scaled Dot-Product Attention中的Q、K、V是怎么得来的。比如给出输入序列的word embedding<span class="math inline">\(x=\{x_1,x_2\}\)</span>，通过线性变换，得到Q、K、V。公式表达如下：</li>
</ol>
<p><span class="math display">\[
Q=xW^Q,
K=xW^K,
V=xW^V.
\]</span></p>
<p>其中参数<span class="math inline">\(W^Q,W^K,W^V\)</span>通过网络的训练得出。以下图为例：</p>
<p>输入：两个单词，Thinking, Machines. 通过嵌入变换会<span class="math inline">\(x_1,x_2\)</span>两个向量[1 x 4]。分别与 <span class="math inline">\(W^Q,W^K,W^V\)</span> 三个矩阵[4x3]做点乘得到，${q_1,q_2},{k_1,k_2},{v_2,v_2} $6个向量[1x3]。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/3.qkv的计算.png"></p>
<ol start="2" type="1">
<li>接着，计算self-attention的分数值。该分数表示当我们在encode一个词的时候，对句子其他词的关注程度。我们通过Q与K做点乘得到分数值。以下图为例：</li>
</ol>
<p>我们需要计算其他词相对于thinking这个词的分数。首先是thinking自己: <span class="math inline">\(q_1·k_1\)</span>；machines相对于thinking的分数：<span class="math inline">\(q_1·k_2\)</span>。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/4.QKscore.png"></p>
<ol start="3" type="1">
<li>对score进行缩放，即规范化，并对score使用softmax，得到分布。以下图为例：</li>
</ol>
<p>score之所以需要进行缩放是因为：要使梯度更加稳定。因为当<span class="math inline">\(d_k\)</span>非常大的时候，点乘结果会非常大，如果不做缩放，那么经过sofmax归一化之后，二者差距会非常大，从而使得在反向传播的时候，梯度会非常小，难以训练。譬如：如果不进行缩放，经过softmax的结果是：0.99999...，0.0000000...，这显然不利于之后的计算。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/5.softmax.png"></p>
<ol start="4" type="1">
<li>将归一化的score与v相乘，得到attention值，并对加权向量值求和， 这将在此位置（对于第一个单词thinking）产生self-attention层的输出。以下图为例：</li>
</ol>
<p>最终的输出：<span class="math inline">\(z_1=0.88v_1+0.12v_2\)</span>.</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/6.sum.png"></p>
<p>上述是计算的一个单词的attention值，在实际操作中，往往以矩阵形式操作。如下：</p>
<p>输入是一个[2x4]的矩阵（单词嵌入），每个运算是[4x3]的矩阵，求得Q,K,V。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/7.矩阵计算.png"></p>
<p>Q对K转制做点乘，除以dk的平方根。做一个softmax得到合为1的比例，对V做点乘得到输出Z。那么这个Z就是self-attention的输出，Z的第一行就是thinking的self-attention值，第二行就是machines的self-attention值。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/8.矩阵计算.png"></p>
<p>Scaled Dot-Product Attention大致就讲完了，那么对于Multi-Head Attention，其实就很简单了，就是将上述过程重复H次，然后再concat，最后输出。主要是为了能够并行运算，加快计算效率，看图👇。</p>
<p>在Multi-Head attention中，我们为每个head维护单独的Q / K / V权重矩阵，从而导致不同的Q / K / V矩阵。 在transformer原文中，使用了8个head，所以最后会得到8个矩阵。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/9.multihead.png"></p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/10..png"></p>
<p>得到8个矩阵之后，再concat变成一个矩阵，与<span class="math inline">\(W^O\)</span>做点乘后，输入到FFN中。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/11.png"></p>
<p>Multi-Head Attention部分就讲完了。然而还有一些细节需要关注。我们再回顾一下self-attention的计算公式： <span class="math display">\[
Attention(Q,K,V)=softmax(\frac {QK^{T}}{\sqrt{d_k}})V,
\]</span> Q、K、V是由输入与W相乘得到的，输入的维度为：(m,512)，m是一个句子的单词数，512是word embedding的维度，那么Q、K、V的维度是：(m,64)，那么<span class="math inline">\(QK^T\)</span>的维度是：(m,m)。也就是一个attention map，比如说输入是一句话 "i have a dream" 总共4个单词， 这里就会形成一张4x4的注意力机制的图，每一个格子就对应一个权重，如下👇。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/12.jpg" style="zoom:50%;"></p>
<p><strong>需要注意的是，在encoder中，叫做self-attention，在decoder中，叫做masked self-attention</strong>。所谓的mask，意思就是不给模型看到未来的信息。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/13.jpg" style="zoom:50%;"></p>
<p>就比如说，i作为第一个单词，只能有和i自己的attention。have作为第二个单词，有和i, have 两个attention。 a 作为第三个单词，有和i,have,a 前面三个单词的attention。到了最后一个单词dream的时候，才有对整个句子4个单词的attention。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/14.jpg" style="zoom:50%;"></p>
<p>这是做完softmax之后的结果。</p>
<p>对与self-attention来说，就会出现一个问题，如果输入的句子特别长，那就为形成一个 NxN的attention map，这就会导致内存爆炸...所以要么减少batch size多gpu训练，要么剪断输入的长度，还有一个方法是用conv对K,V做卷积减少长度。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/15.jpg" style="zoom:50%;"></p>
<p>对K,V做卷积和stride（stride的话(n,1)是对seq_len单边进行跳跃），会减少seq_len的长度而不会减少hid_dim的长度。所以最后的结果Z还是和原先一样（因为Q没有改变）。mask的话比较麻烦了，作者用的是local attention。</p>
<p>Encoder和Decoder部分的区别如下： 1、 Encoder部分的self-attention计算的是两两单词之间attention。然后decoder部分计算的就是当前的单词和它前面单词的attention了。 2、 Decoder部分多了一层，Encoder-Decoder的attention，就是将decoder的self attention部分和Encoder部分output进行attention。然后再去Feed Forward。</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>在transformer中，由于舍弃了CNN与RNN，那么就单词的位置信息就没有被考虑到，这是不合理的。因此，为了解决这个问题，transformer中在encoder与decoder的输入中，添加了positional encoding。维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下： <span class="math display">\[
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}),
\]</span></p>
<p><span class="math display">\[
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}}).
\]</span></p>
<p>其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在<strong>偶数位置，使用正弦编码，在奇数位置，使用余弦编码</strong>。</p>
<p>最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/16.png"></p>
<h3 id="layer-normalization">Layer Normalization</h3>
<p>在transformer中，每一个子层（self-attetion，ffnn）之后都会接一个残缺模块，并且有一个Layer normalization。</p>
<p><img src="/2020/02/17/NLP%EF%BD%9C%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6Transformer%E6%A8%A1%E5%9E%8B/17.png"></p>
<p>残差连接很简单，在这里，主要讲解一下Layer normalization。其实所有的normalization都是为了解决covariate shfit问题。就是如果在训练集上已经建立好了x到y的映射，那么如果test set的分布与training set的分布不一样，那么模型效果不会很好。比较著名的就是Batch normalization。通过将输入数据转化为固定均值与方差的数据，从而能够加速训练。而Layer Normalization与BN最大的不同在于，LN是固定住每一层的均值和方差，从而降低covariate shift带来的影响。参看链接：<a href="https://blog.csdn.net/zhangjunhit/article/details/53169308" target="_blank" rel="noopener">Layer Normalization</a></p>
<h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3>
<p>在multi-head attention后，会接一个FFN层，计算公式如下： <span class="math display">\[
FFN(x)=max(0,xW_1+b_1)W_2+b_2.
\]</span></p>
<h3 id="mask">MASK</h3>
<p>mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。</p>
<p><strong>Padding Mask</strong></p>
<p>什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p>
<p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</p>
<p>而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。</p>
<p><strong>Sequence mask</strong></p>
<p>文章前面也提到，sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。</p>
<p>那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p>
<p>对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。 其他情况，attn_mask 一律等于 padding mask。</p>
<h2 id="重要的细节">重要的细节</h2>
<ol type="1">
<li><strong>Mask是怎么工作的？是否只有在decoder的时候，才会用到mask？如果不是，请详细描述encoder与decoder的mask的工作状况。</strong></li>
</ol>
<p>答：其实如果看源码的话，这个问题非常简单明了（padding mask 与sentence mask）。这里先不写，放一个连接：<a href="https://blog.csdn.net/u012526436/article/details/86295971" target="_blank" rel="noopener">Mask</a></p>
<ol start="2" type="1">
<li><strong>最后一层的encoder输出了什么？是输出了Z还是输出了K与V？如果是输出了K与V，那么与前面的encoder层的计算是否有区别？</strong></li>
</ol>
<p>答：答案是输出K、V到每个decoder层，而且K与V是同一个矩阵，还记得我说的attention本质吗？具体过程，还是看源码，先不写了，码字太难受了。</p>
<ol start="3" type="1">
<li><strong>在训练过程中，是teacher forcing还是free run？</strong></li>
</ol>
<p>答：论文说的是free run，但是实际操作还是会有teacher forcing。一般会设置一个teacher_forcing_prob，不会一直都是teacher forcing，这样效果会好些。</p>
<ol start="4" type="1">
<li><strong>什么是BPE？在transformer中起到了什么作用？</strong></li>
</ol>
<p>答：之后再写吧，这是一种编码方式，特简单，但是很多地方都会用到，之后应该会专门写一篇文章介绍～</p>
<ol start="5" type="1">
<li><strong>为什么要使用multi-head self-attention？</strong></li>
</ol>
<p>答：其实这个原文里也没有讲的特别清楚，个人理解是：multi-head就像是cnn中的多个filter，不同的head可以关注句子中不同位置的信息（位置信息、句法信息、罕见字信息等等），可以更加综合地利用各方面的信息，提取出更加丰富的特征。</p>
<ol start="6" type="1">
<li><strong>transformer的优点和缺点？</strong></li>
</ol>
<p>答：优点：相比rnn来说，能够并行计算；相比cnn来说，不需要叠加非常多的层来扩大感受野。</p>
<p>​ 缺点：因为抛弃了rnn，所以失去了句子之间的位置信息，虽然加了positional encoding，但是仍然无法做到像rnn那样，完全地考虑单词之间的位置关系。此外，transformer的空间复杂度非常大，如果句子长度为<span class="math inline">\(L\)</span>，那么每一个head都需要存储<span class="math inline">\(L^2\)</span>的score，当<span class="math inline">\(L\)</span>非常大的时候，那么就会出现OOM的情况，那么，这种情况下，就需要讲句子非常多个句子，但是一旦分割句子，那么句子之间的前后关系就没有了，那么模型结果也会受影响。</p>
<ol start="7" type="1">
<li><strong>为什么要使用label-smoothing？</strong></li>
</ol>
<p>答：<a href="https://zhuanlan.zhihu.com/p/60821628" target="_blank" rel="noopener">transformer细节</a></p>
<p>​</p>
<p>我的这片博文只是详细地剖析transformer的原理，但是有很多细节的地方（譬如上述两个问题），原论文其中讲的并不清楚🤷‍♂️。anyway，需要看源码，然后自己亲自调试才能真正地看懂，所以，RTFSC，源码链接：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">pytorch-transformer</a>、<a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">tensorflow-transformer</a></p>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>《Attention Is All You Need》</li>
<li>https://jalammar.github.io/illustrated-transformer/</li>
<li>https://zhuanlan.zhihu.com/p/39034683</li>
<li>https://zhuanlan.zhihu.com/p/60821628</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>深度学习</tag>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>self-attention</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|感知机模型原理详解与实现</title>
    <url>/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BD%9C%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>感知机模型(perceptron)可以说是机器学习中最为简单的模型之一，也是之后的神经网络的原型与基础。本篇博客将对感知机模型的原理进行详细的讲解，并采用纯python实现以及调用scikit-learn库实现，这两种方式对感知机模型进行实现。</p>
<a id="more"></a>
<h2 id="感知机模型概述">感知机模型概述</h2>
<p>感知机模型是二分类的模型，属于监督学习的范畴。当输入实例的特征向量到<strong>感知机模型</strong>中后，模型能够输出实例的类别(1或者-1)。所以，感知机的核心在于找到能够完全正确分离正类与负类的分离超平面。那么，为了能够求得此分离超平面，我们需要定义<strong>学习策略</strong>。在感知机中，定义了误分类驱动的损失函数，并采用<strong>随机梯度下降算法</strong>，对损失函数进行求解。下面，将从<strong>模型、策略、算法</strong>三方面，依此对感知机进行详细地讲解。</p>
<h2 id="感知机模型">感知机模型</h2>
<p>感知机模型如下： <span class="math display">\[
y=f(x)=sign(wx+b),
\]</span></p>
<p><span class="math display">\[
sign(z)=
\begin{cases}
+1,&amp; \text{x&gt;=0}\\
-1,&amp; \text{x&lt;0}
\end{cases}
\]</span></p>
<p>其中，<span class="math inline">\(x\)</span>是模型输入，<span class="math inline">\(y\)</span>是模型输出，并且<span class="math inline">\(x\in R^n，y\in \{1,-1\}\)</span>；<span class="math inline">\(w，b\)</span>是模型参数，并且<span class="math inline">\(w\in R^n，b\in R\)</span>。那么该模型是什么意思呢？可以做如下解释：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BD%9C%E6%84%9F%E7%9F%A5%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.分离超平面.jpg"></p>
<p>感知机模型实际上是在找一个分离超平面：<span class="math inline">\(wx+b=0\)</span>，它将特征空间分成两部分。当实例类别是+1的时候，它在超平面的上方，即使得<span class="math inline">\(wx+b&gt;0\)</span>；当实例类别是-1的时候，它在超平面的下方，即使得<span class="math inline">\(wx+b&lt;0\)</span>。那么，知道了模型之后，我们发现感知机模型中有两个参数：<span class="math inline">\(w,b\)</span>。那么，我们就需要找到一种学习策略来求解这两个参数，从而得到真正的感知机模型。</p>
<h2 id="感知机模型的学习策略">感知机模型的学习策略</h2>
<p>首先，我们要知道，感知机的训练数据集是线性可分的，要不然的话，学习出来的感知机模型不会收敛。那什么是线性可分的数据集呢？如下：</p>
<blockquote>
<p>给定数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i\in R^n，y\in R，i=1...N\)</span>。如果存在某个超平面<span class="math inline">\(S\)</span>，能够将<span class="math inline">\(X\)</span>中的所有正例与负例能够完全区分开来，那么就说数据集<span class="math inline">\(X\)</span>是线性可分的。</p>
</blockquote>
<p>那么，现在给定线性可分训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_N,y_N)\}\)</span>，我们要从中学习出感知机模型。我们需要考虑的是学习策略，即损失函数，也就是怎么样才能得到参数<span class="math inline">\(w,b\)</span>。</p>
<p>对于损失函数，一个很自然的想法，就是以误分类的实例的数目为模型的损失函数。但是，这样做的一个缺陷在于，这样的损失函数不是<span class="math inline">\(w,b\)</span>的可导函数，不易优化。所以，在感知机中，采取以误分类实例到分离超平面的总距离为损失函数。对于误分类点<span class="math inline">\((x_i,y_i)\)</span>，当<span class="math inline">\(y_i=1\)</span>的时候，<span class="math inline">\(wx_i+b&lt;0\)</span>；当<span class="math inline">\(y_i=-1\)</span>的时候，<span class="math inline">\(wx_i+b&gt;0\)</span>。那么我们将其用一个公式表达，如下： <span class="math display">\[
-y_i(wx_i+b)&gt;0
\]</span> 那么，假设误分类集合为<span class="math inline">\(M\)</span>，我们根据点到平面的距离公式，所有误分类点到分离超平面的总距离如下： <span class="math display">\[
D=-\sum_{x_i\in M}\frac {y_i(wx_i+b)}{\left \|w\right \|}
\]</span> 我们不考虑<span class="math inline">\(\frac {1}{\left\|w\right\|}\)</span>，便得到了感知机模型的损失函数，如下： <span class="math display">\[
L(w,b)=-\sum_{x_i\in M}{y_i(wx_i+b)}
\]</span> 所以，我们最小化该损失函数，就可以得到最优的参数<span class="math inline">\(w,b\)</span>。此外，该损失函数是非负的，如果没有误分类点，那么损失函数为0，并且，对于一个特点的误分类点的时候，误分类点离分离超平面越近，损失函数越小。当分离超平面越过该误分类点的时候，损失函数为0。</p>
<h2 id="感知机模型的学习算法">感知机模型的学习算法</h2>
<p>当我们定义了感知机模型的学习策略(损失函数)后，那么求解参数就变成一个最优化问题了。在这里，学习算法有其原始形式与对偶形式，下面我们将一一介绍。</p>
<h3 id="感知机模型学习算法的原始形式">感知机模型学习算法的原始形式</h3>
<p>在感知机模型中，采取的学习算法是随机梯度下降算法(SGD)。那为什么不采取batch梯度下降呢？原因在于，只有误分类点才能被计算入损失函数中，所以不能使用所有的样本来进行优化。对于单个误分类点，我们使用损失函数对<span class="math inline">\(w，b\)</span>求偏导，结果如下： <span class="math display">\[
\frac {\partial L}{\partial w}=-y_ix_i,
\]</span></p>
<p><span class="math display">\[
\frac {\partial L}{\partial b}=-y_i.
\]</span></p>
<p><strong>原始形式</strong></p>
<p>输入：给定训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i\in R^n，y\in R，i=1...N\)</span>。学习率<span class="math inline">\(\eta\in (0,1]\)</span>。</p>
<p>输出：参数<span class="math inline">\(w,b\)</span>。</p>
<ol type="1">
<li>初始化参数<span class="math inline">\(w,b\)</span>，给定初值为<span class="math inline">\(w_0,b_0\)</span>。</li>
<li>在训练数据集中选取实例<span class="math inline">\((x_i,y_i)\)</span>。</li>
<li>如果<span class="math inline">\(y_i(wx_i+b)&lt;0\)</span>，那么</li>
</ol>
<p><span class="math display">\[
w:=w-\eta(-y_ix_i)  ==&gt;  w:=w+\eta y_ix_i
\]</span></p>
<p><span class="math display">\[
b:=b-\eta (-y_i)==&gt;b:=b+\eta y_i
\]</span></p>
<ol start="4" type="1">
<li>转到第2步，循环，直到损失函数为0，也就是没有误分类点为止。</li>
</ol>
<p><strong>注意：感知机学习算法会由于选取不同的参数初值以及选取误分类点的顺序，而导致最后的模型不同，即感知机模型不唯一。</strong></p>
<h3 id="感知机模型学习算法的对偶形式">感知机模型学习算法的对偶形式</h3>
<p>对偶形式采取的是另外一种想法，我们将参数<span class="math inline">\(w,b\)</span>表示为<span class="math inline">\(x_i,y_i\)</span>的线性组合，通过求解参数<span class="math inline">\(w,b\)</span>的系数，从而求得参数<span class="math inline">\(w,b\)</span>。具体做法是：令参数<span class="math inline">\(w,b\)</span>的初值为0，这样的话，最终的<span class="math inline">\(w,b\)</span>可以用如下的公式来表示： <span class="math display">\[
w=\sum_{i=1}^{N}\alpha_iy_ix_i，b=\sum_{i=1}^{N}\alpha_iy_i.
\]</span> 其中，<span class="math inline">\(\alpha_i=n_i\eta\)</span>，<span class="math inline">\(\eta\)</span>表示学习率，<span class="math inline">\(n_i\)</span>表示由于第i个实例被误分类，从而更新的次数。所以，我们只要求得<span class="math inline">\(\alpha=\{\alpha_1,\alpha_2,...,\alpha_N\}\)</span>，就可以得到参数<span class="math inline">\(w,b\)</span>。</p>
<p><strong>对偶形式</strong></p>
<p>输入：给定训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i\in R^n，y\in R，i=1...N\)</span>。学习率<span class="math inline">\(\eta\in (0,1]\)</span>。</p>
<p>输出：参数<span class="math inline">\(w,b\)</span>。感知机模型<span class="math inline">\(f(x)=sign(\sum_{j=1}^{N}\alpha_jy_jx_jx+b)\)</span>，其中，<span class="math inline">\(\alpha=\{\alpha_1,\alpha_2,...,\alpha_N\}\)</span>。</p>
<ol type="1">
<li>初始化参数<span class="math inline">\(w,b\)</span>，给定初值为<span class="math inline">\(w_0=0,b_0=0\)</span>。</li>
<li>在训练数据集中选取实例<span class="math inline">\((x_i,y_i)\)</span>。</li>
<li>如果<span class="math inline">\(y_i(\sum_{j=1}^{N}\alpha_jy_jx_jx_i+b)&lt;0\)</span>，那么</li>
</ol>
<p><span class="math display">\[
\alpha_i:=\alpha_i+\eta,
\]</span></p>
<p><span class="math display">\[
b:=b+\eta y_i.
\]</span></p>
<ol start="4" type="1">
<li>转到第2步，直到损失函数为0，没有误分类点为止。</li>
</ol>
<p><strong>注意：我们如果仔细看训练过程的话，我们会发现在第3步，我们需要频繁计算训练实例的内积。那么，我们可以实现把所有实例的内积都算出来，放入一个矩阵(该矩阵叫Gram矩阵)只能够存储。当需要的时候，直接查矩阵就可以了，这样的话，就可以大大加快计算速度。这就是为什么对偶形式比原始形式更优的原因。</strong></p>
<p>但是在实践中，我们并不总是选择对偶形式。当实例特征数量特别多的时候，我们选择对偶形式；当样本数量特别多的时候，我们选择原始形式。</p>
<p>到此，感知机的理论部分就讲完了，是不是非常简单😁？我们可以回顾一下，在感知机中，一个最重要的前提条件就是：训练数据集是线性可分的。那如果训练数据集不是线性可分的，该怎么办？这就需要大名鼎鼎的支持向量机(SVM)来进行解决了，后面就专门写一篇文章来介绍支持向量机～</p>
<h2 id="感知机模型的实现">感知机模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现感知机模型：纯python实现以及调用scikit-learn库来实现。我的github里面可以下在到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="纯python实现">纯python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2019-12-15</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist数据集</span></span><br><span class="line"><span class="string">模型：感知机模型，对其原始形式与对偶形式均进行了实现</span></span><br><span class="line"><span class="string">实现方式：python+numpy</span></span><br><span class="line"><span class="string">-----------------------------------</span></span><br><span class="line"><span class="string">使用原始形式</span></span><br><span class="line"><span class="string">数据量：1000（可以全部使用，主要是对偶形式做对比）</span></span><br><span class="line"><span class="string">准确率：0.7691</span></span><br><span class="line"><span class="string">时间：19.176676034927368</span></span><br><span class="line"><span class="string">-----------------------------------</span></span><br><span class="line"><span class="string">使用对偶形式</span></span><br><span class="line"><span class="string">如果使用对偶形式的话，数据集最好不要全部使用，会非常慢！</span></span><br><span class="line"><span class="string">(主要就是在计算gram矩阵的时候，60000✖️60000的矩阵挺大的了)</span></span><br><span class="line"><span class="string">数据量：1000</span></span><br><span class="line"><span class="string">准确率：0.7796.</span></span><br><span class="line"><span class="string">时间：40.42902708053589</span></span><br><span class="line"><span class="string">------------------------------------</span></span><br><span class="line"><span class="string">但是如果把训练样本数目改为10的话，会发现，原始:54.4%,14.43s;对偶：65.05%，14.715s!</span></span><br><span class="line"><span class="string">所以由此可以看出，当样本数远小于特征数的时候。选择对偶会更好，其余情况，选原始形式。</span></span><br><span class="line"><span class="string">通过实验就看出来了～</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    从fileName数据文件中加载Mnist数据集</span></span><br><span class="line"><span class="string">    :param fileName: 数据集的路径</span></span><br><span class="line"><span class="string">    :return: 返回数据的特征向量与标签类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 存放数据的特征向量</span></span><br><span class="line">    data_list = []</span><br><span class="line">    <span class="comment"># 存放数据的标签类别</span></span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取文件，将特征向量与标签分别存入data_list与label_list</span></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            data_list.append([int(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line">            <span class="keyword">if</span> int(curline[<span class="number">0</span>]) &gt;= <span class="number">5</span>:</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将list类型的特征向量，变换成矩阵，维度为（60000，784）</span></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    <span class="comment"># 将list类型的标签类别，变换成矩阵，维度为（1,60000）</span></span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义Peceptron类，里面包括了其学习算法的原始形式与对偶形式的实现函数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_matrix, label_matrix, iteration=<span class="number">30</span>, learning_rate=<span class="number">0.0001</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        :param data_matrix: 数据的特征向量</span></span><br><span class="line"><span class="string">        :param label_matrix: 数据的标签类别</span></span><br><span class="line"><span class="string">        :param iteration: 迭代次数,默认为100</span></span><br><span class="line"><span class="string">        :param learning_rate: 学习率，默认为0.001</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.data_matrix = data_matrix</span><br><span class="line">        self.label_matrix = label_matrix</span><br><span class="line">        self.iteration = iteration</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">original_method</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        感知机学习算法的原始形式的实现</span></span><br><span class="line"><span class="string">        :return: 返回参数w，b</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        data_matrix = self.data_matrix</span><br><span class="line">        label_matrix = self.label_matrix</span><br><span class="line">        <span class="comment"># input_num表示训练集数目，feature_num表示特征数目</span></span><br><span class="line">        input_num, feature_num = np.shape(data_matrix)</span><br><span class="line">        print(data_matrix.shape)</span><br><span class="line">        w = np.random.randn(<span class="number">1</span>, feature_num)</span><br><span class="line">        b = np.random.randn()</span><br><span class="line">        <span class="comment"># 迭代iteration次</span></span><br><span class="line">        <span class="keyword">for</span> iter <span class="keyword">in</span> range(self.iteration):</span><br><span class="line">            <span class="comment"># 在每一个样本上都进行判断</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(input_num):</span><br><span class="line">                x_i = data_matrix[i]</span><br><span class="line">                y_i = label_matrix[i]</span><br><span class="line">                result = y_i * (np.matmul(w, x_i.T) + b)</span><br><span class="line">                <span class="keyword">if</span> result &lt;= <span class="number">0</span>:</span><br><span class="line">                    w = w + self.learning_rate * y_i * x_i</span><br><span class="line">                    b = b + self.learning_rate * y_i</span><br><span class="line">            print(<span class="string">f"this is <span class="subst">&#123;iter&#125;</span> round ,the total round is <span class="subst">&#123;self.iteration&#125;</span>."</span>)</span><br><span class="line">        <span class="keyword">assert</span> (w.shape == (<span class="number">1</span>, feature_num))</span><br><span class="line">        <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dual_method</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        感知机学习算法的对偶形式的实现</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        data_matrix = self.data_matrix</span><br><span class="line">        label_matrix = self.label_matrix.T</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input_num表示训练集数目，feature_num表示特征数目</span></span><br><span class="line">        input_num, feature_num = np.shape(data_matrix)</span><br><span class="line">        <span class="comment"># 系数a，初始化为全0的(1,input_num)的矩阵</span></span><br><span class="line">        a = np.zeros(input_num)</span><br><span class="line">        <span class="comment"># 系数b，初始化为0</span></span><br><span class="line">        b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算出gram矩阵</span></span><br><span class="line">        print(a.shape)</span><br><span class="line">        gram = np.matmul(data_matrix, data_matrix.T)</span><br><span class="line">        <span class="keyword">assert</span> (gram.shape == (input_num, input_num))</span><br><span class="line">        print(gram.shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 迭代iteration次</span></span><br><span class="line">        <span class="keyword">for</span> iter <span class="keyword">in</span> range(self.iteration):</span><br><span class="line">            <span class="comment"># 在每一个样本上都进行判断</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(input_num):</span><br><span class="line">                result = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(input_num):</span><br><span class="line">                    result += a[j] * label_matrix[j] * gram[j][i]</span><br><span class="line">                result += b</span><br><span class="line">                result *= label_matrix[i]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 判断当前样本会不会被误分类</span></span><br><span class="line">                <span class="keyword">if</span> (result &lt;= <span class="number">0</span>):</span><br><span class="line">                    a[i] = a[i] + self.learning_rate</span><br><span class="line">                    b = b + self.learning_rate * label_matrix[i]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            print(<span class="string">f"this is <span class="subst">&#123;iter&#125;</span> round,the total round is <span class="subst">&#123;self.iteration&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">        w = np.matmul(np.multiply(a,self.label_matrix),self.data_matrix)</span><br><span class="line">        print(w.shape)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_model</span><span class="params">(test_data_matrix, test_label_matrix, w, b)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    再测试数据集上测试</span></span><br><span class="line"><span class="string">    :param test_data_matrix: 测试集的特征向量</span></span><br><span class="line"><span class="string">    :param test_label_matrix: 测试集的标签类别</span></span><br><span class="line"><span class="string">    :param w: 计算得到的参数w</span></span><br><span class="line"><span class="string">    :param b: 计算得到的参数b</span></span><br><span class="line"><span class="string">    :return: 返回准确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    test_input_num, _ = np.shape(test_data_matrix)</span><br><span class="line">    error_num = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计在测试集上数据被误分类的数目</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(test_input_num):</span><br><span class="line">        result = (test_label_matrix[i]) * (np.matmul(w, test_data_matrix[i].T) + b)</span><br><span class="line">        <span class="keyword">if</span> (result &lt;= <span class="number">0</span>):</span><br><span class="line">            error_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    accuracy = (test_input_num - error_num) / test_input_num</span><br><span class="line">    <span class="comment"># 返回模型在测试集上的准确率</span></span><br><span class="line">    <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 获取当前时间,作为程序开始运行时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    train_data_list, train_label_list = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data_list, test_label_list = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line">    perceptron = Perceptron(train_data_list[:<span class="number">1000</span>], train_label_list[:<span class="number">1000</span>], iteration=<span class="number">30</span>, learning_rate=<span class="number">0.0001</span>)</span><br><span class="line">    w, b = perceptron.dual_method()</span><br><span class="line"></span><br><span class="line">    accuracy = test_model(test_data_list, test_label_list, w, b)</span><br><span class="line">    <span class="comment"># 获取当前时间，作为程序结束运行时间</span></span><br><span class="line">    end = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印模型在测试集上的准确率</span></span><br><span class="line">    print(<span class="string">f"accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line">    <span class="comment"># 打印程序运行总时间</span></span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="使用scikit-learn实现">使用scikit-learn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2019-12-15</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist数据集</span></span><br><span class="line"><span class="string">模型：感知机模型，对其原始形式与对偶形式均进行了实现</span></span><br><span class="line"><span class="string">实现方式：使用scikit-learn库</span></span><br><span class="line"><span class="string">结果：</span></span><br><span class="line"><span class="string">在测试集上的准确率：0.7849</span></span><br><span class="line"><span class="string">时间：25,72s</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    从fileName数据文件中加载Mnist数据集</span></span><br><span class="line"><span class="string">    :param fileName: 数据集的路径</span></span><br><span class="line"><span class="string">    :return: 返回数据的特征向量与标签类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 存放数据的特征向量</span></span><br><span class="line">    data_list = []</span><br><span class="line">    <span class="comment"># 存放数据的标签类别</span></span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取文件，将特征向量与标签分别存入data_list与label_list</span></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            data_list.append([int(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line">            <span class="keyword">if</span> int(curline[<span class="number">0</span>]) &gt;= <span class="number">5</span>:</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(np.mat(data_list))</span><br><span class="line">    label_matrix = np.array(np.mat(label_list))</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义感知机</span></span><br><span class="line">    <span class="comment"># n_iter_no_change表示迭代次数，eta0表示学习率，shuffle表示是否打乱数据集</span></span><br><span class="line">    clf = Perceptron(n_iter_no_change=<span class="number">30</span>, eta0=<span class="number">0.0001</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 使用训练数据进行训练</span></span><br><span class="line">    train_data_matrix, train_label_matrix = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data_matrix, test_label_matrix = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line"></span><br><span class="line">    print(train_data_matrix.shape)</span><br><span class="line">    print(test_data_matrix.shape)</span><br><span class="line"></span><br><span class="line">    train_label_matrix = np.squeeze(train_label_matrix)</span><br><span class="line">    test_label_matrix = np.squeeze(test_label_matrix)</span><br><span class="line"></span><br><span class="line">    print(train_label_matrix.shape)</span><br><span class="line">    print(test_label_matrix.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    clf.fit(train_data_matrix, train_label_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用测试集进行验证，得到模型在测试集上的准确率</span></span><br><span class="line">    accuracy = clf.score(test_data_matrix, test_label_matrix)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">f"accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>perceptron</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|预训练模型专题—ELMo/GPT/BERT/XLNet/ERNIE/ALBERT模型原理详解</title>
    <url>/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>在如今的NLP领域中，预训练模型占据着愈发重要的地位。而BERT的出世，更是几乎横扫了NLP中所有的任务记录。本篇博客将详解几大预训练模型：ELMo、GPT、BERT、XLNet、ERNIE、ALBERT。(下图是可爱的bert😋注意，这真不是芝麻街啊喂！)</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/0.bert.jpg"></p>
<a id="more"></a>
<blockquote>
<p>本篇文章致力于讲解清楚各个预训练语言模型的原理，但是细节部分不会展开讨论，譬如transformer架构的原理、self-attention的原理、highway network的原理等等，这些基础的东西请自行查找资料进行补充～</p>
</blockquote>
<h2 id="elmo模型介绍">ELMo模型介绍</h2>
<p>ELMo模型，全称：Embedding from Language Models。这个模型所要解决的问题是：<strong>1.词向量要更好地表达语法与语义等信息；2.单词在不同的上下文会有不同的意思，而传统的word2vec等word embedding模型，每一个单词的词向量都是固定的，也就是与上下文无关，这对于处理单词的多义性非常地不友好。</strong>所以，ELMo模型就是利用深度的双向LSTM模型来解决这些问题。</p>
<h3 id="elmo模型的大体结结构">ELMo模型的大体结结构</h3>
<p>ELMo模型的大体结构是：多层的biLSTM语言模型。结果如下：</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/0.png" style="zoom: 67%;"></p>
<ul>
<li>首先是输入<code>Input Sentence</code>。它的shape是<code>(B,W,C)</code>。其中，<code>B</code>表示batch_size，<code>W</code>表示一个句子的token的数目， <code>C</code>表示一个token的最大字符数，在论文里是固定的数目：50。在这里，需要明确的是：<strong>ELMo模型是以字符为单位进行编码的。</strong></li>
<li>接着，进入 char encoder layer。在ELMo模型，会对每一个字符进行编码，也就是给每个字符进行embedding。然后，经过一些一维卷积操作，以及highway操作，最后的输出维度为：<code>(B,W,D)</code>。</li>
<li>将char encoder layer的输出作为biLSTM layer的输入，假设 biLSTM layer有L层，最终输出的维度为：<code>(L+1,B,W,2*D)</code>。在这里，之所以+1，是因为把 char encoder layer的输出也算进去了。</li>
<li>最后，将<code>(L+1,B,W,2*D)</code>的输出作为 scalar Mixer的输入，其实就是将这L+1个输出<strong>进行加权平均并且放缩</strong>，最终的输出的维度是：<code>(B,W,2*D)</code>。这就是最终生成的ELMo词向量。</li>
</ul>
<p>下面来具体介绍一下每一层的结构。</p>
<h3 id="char-encoder-layer">Char Encoder Layer</h3>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/1.png" style="zoom:80%;"></p>
<ul>
<li>char encoder layer的输入维度是<code>(B,W,C)</code>，然后需要被reshape为<code>(B*W,C)</code>。之后针对每一个token的每一个字符进行编码。字符表总共有262个，其中，0-255是unicode编码，其余的是：<code>&lt;bos&gt;,&lt;eos&gt;,&lt;bow&gt;,&lt;eow&gt;,&lt;pow&gt;,&lt;pos&gt;</code>。其中每一个字符的embeddng的维度是d。所以，整个词表的维度是：<code>262*d</code>。经过char embedding之后，输出为：<code>(B*W,C,d)</code>。</li>
<li>紧接着，我们需要使用一维卷积。这里需要注意的是：<strong>并不是在纵向上叠加很多卷积层，而是在横向上使用若干个一维卷积层</strong>。在ELMo中，使用了m个一维卷积。以第一个为例：卷积核为k1，个数为d1，那么一维卷积后得到的维度是：<code>(B*W,C1,d1)</code>；接着，再使用maxpooling(之所以要使用maxpooling，是因为经过卷积之后，各个向量维度可能不一致，无法合并，所以才需要进行pooling操作)，从中选择出最大的单词，其维度变为：<code>(B*W,d1)</code>，最后再经过激活函数，就完成了一个操作。</li>
<li>在完成m个卷积操作之后，我们将得到的m个向量在d维进行concat，并且需要reshape，即得到的维度为：<code>(B,W,(d1+d2+d3+...+dm))</code>。</li>
<li>接着，我们在经过若干个highway层。所谓的highway层的作用与残差连接是一样的，能够让网络训练的效率随着深度的增加不降低。具体的公式请参看highway的原始论文。</li>
<li>最后，需要进行一个线性变换，将维度从<code>(B*W,(d1+d2+d3+...+dm))</code>变为<code>(B,W,D)</code>。这里的<code>D</code>是最后我们得到的ELMo词向量维度的一半。</li>
</ul>
<h3 id="bilstm-layers">biLSTM layers</h3>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/2.png" style="zoom:80%;"></p>
<ul>
<li>biLSTM layers是接在 char encoder layer之后的，它的输入维度是：<code>(B,W,D)</code>。biLSTM相信大家都很熟悉了，在这里，假设LSTM的 hidden state的维度是h，那么LSTM的输出是<code>(B,W,h)</code>。当然，我们需要进行一个线性变换，将维度从<code>(B,W,h)</code>变成<code>(B,W,D)</code>。由于是双向的，所以，我们concat之后，得到的一层的输出是：<code>(B,W,2D)</code>。</li>
<li><strong>注意，在biLSTM层，每一层的LSTM的输出作为下一层LSTM的输入。假设有L层biLSTM，那么，我们最后得到的输出向量个数为L+1个，因为最初的char encoder layer的输出也要算进去。</strong>所以biLSTM layers最终的输出维度是：<code>(L+1,B,W,2D)</code>。</li>
</ul>
<p><strong>这里需要着重讲一下ELMo中biLSTM的使用，这也是其与BERT最大的不同。</strong></p>
<p>在ELMo中，其实是训练了两个正反向的语言模型。前向的语言模型的概率如下： <span class="math display">\[
P(t_1,t_2,...,t_N)=\prod_{k=1}^{N}P(t_k|t_1,t_2,..,t_{k-1})
\]</span> 后向的语言模型的概率如下： <span class="math display">\[
P(t_1,t_2,...,t_N)=\prod_{k=1}^{N}P(t_k|t_{k+1},t_{k+2},..,t_{N})
\]</span> ELMo模型的目标函数是： <span class="math display">\[
maximize \ \sum_{k=1}^{N}(logP(t_k|t_1,t_2,...,t_{k-1},\theta_x,\overrightarrow{\theta_{LSTM}},\theta_s)+logP(t_k|t_{k+1},t_{k+2},...,t_{N},\theta_x,\overleftarrow{\theta_{LSTM}},\theta_s))
\]</span> 其中<span class="math inline">\(\theta_x\)</span>表示词嵌入的参数；<span class="math inline">\(\theta_s\)</span>表示softmax之前的参数，即输出层的参数；<span class="math inline">\(\overrightarrow{\theta_{LSTM}},,\overleftarrow{\theta_{LSTM}}\)</span>表示biLSTM层的参数。所以，我们可以看出，<strong>ELMo模型是分别训练两个LM，并不是真正的双向语言模型，这就是ELMo模型与BERT最大的区别。</strong></p>
<h3 id="scalar-mixer">Scalar Mixer</h3>
<p>在biLSTM layers之后，我们需要输入到scalar mixer，即做一个变换，得到最终的ELMo词向量。我们需要注意的是，在每一个位置k，每一层都会生成一个关于词向量的表示<span class="math inline">\(\overrightarrow {h_{k,j}^{LM}}\)</span>。其中，j表示第j层。一个L层的biLSTM，会生成2L+1个词汇表征。 <span class="math display">\[
R_k=\{h_{k,j}^{LM}|j=0,...,L\},\\
h_{k,j}^{LM}=[\overrightarrow {h_{k,j}^{LM}},\overleftarrow {h_{k,j}^{LM}}]
\]</span> 第k个位置的最终的词向量如下公式： <span class="math display">\[
ELMo_k=\gamma\sum_{j=0}^{L}s_jh_{k,j}^{LM}
\]</span> 其中，<span class="math inline">\(s_j\)</span>表示经过softmax之后的概率值，可以理解为各个层的输出的权重；常数参数<span class="math inline">\(\gamma\)</span>是缩放参数，用来对整个ELMo词向量进行缩放。这两个参数是需要学习的超参数。此外，<strong>每一层的输出的分布会有比较大的差别，有时候使用layer normalization。</strong>经过 scalar mixer之后，我们得到的最终的ELMo词向量的的维度是：<code>(B,W,2*D)</code>。</p>
<p>之后如果介入NLP下游任务，那么可以固定ELMo模型，不让其参与训练，也可以让ELMo模型参与训练，对词向量模型进行fine-tune。</p>
<p><strong>总结，ELMo的优势在于：</strong></p>
<ul>
<li>得到的词向量更好，词向量与其上下文内容有关，能够解决单词的多义性问题；</li>
<li>使用了深层网络的所有的输出，比只是用顶层的LSTM layer更好；具体来说，低层的LSTM layer能够表示单词的语法信息；高层的LSTM layer能够表示单词的语义信息；</li>
<li>基于字符进行embedding，解决了OOV问题。<strong>但是注意，我们最后得到的是词向量！</strong></li>
</ul>
<h2 id="gpt模型介绍">GPT模型介绍</h2>
<p>GPT模型，全称：Generative Pre-Training。它的整个过程分为两个阶段：<strong>1.无监督的预训练；2.有监督的微调。</strong>下面依次介绍这两个阶段。</p>
<h3 id="pre-training">pre-training</h3>
<p>GPT模型，采用的是Transformer的 decoder部分作为language model，但是稍微有点不同：其去掉了encoder-decoder attention layer。它的目标函数是： <span class="math display">\[
maximize \ L_1= \sum_{i}logP(u_i|u_{i-k},u_{i-k+1},...,u_{i-1};\theta)
\]</span> 这就是一般的LM的目标函数。其中，<span class="math inline">\(k\)</span>表示的是context window。(使用了markov假设？) <span class="math inline">\(\theta\)</span>表示网络的参数；我们使用SGD来进行训练。</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/3.jpg"></p>
<p>上图就是GPT的模型。左边就是使用transformer的去掉了encoder-decoder attention layer的decoder。注意，<strong>既然使用了decoder部分，那么网络在预测当前词的时候，就不能看到未来的信息，并且也无法并行化操作。</strong>如果使用公式来表示的话，则有： <span class="math display">\[
h_0=UW_e+W_p \\
h_l=transformer_block(h_{l-1}),l\in[1,L]       \\
P(u)=softmax(h_LW)
\]</span></p>
<h3 id="fine-tune">fine-tune</h3>
<p>以分类任务为例，我们在使用LM得到最后一步的输出之后，我们再接上一个线性层。譬如现在有一个句子，表示为：<span class="math inline">\((x_1,x_2,...,x_m)\)</span>，其标签为<span class="math inline">\(y\)</span>。我们使用LM得到最后一个输出是<span class="math inline">\(h_l^m\)</span>，在将其喂入一个线性层，从而得到结果，表示如下： <span class="math display">\[
P(y|x_1,x_2,...,x_m)=softmax(h_l^mW_y)
\]</span></p>
<p>目标函数如下： <span class="math display">\[
maximize \ L_2=\sum_{x,y}logP(y|x_1,x_2,...,x_m)
\]</span> 那么，我们将LM作为一个辅助的目标函数，这样做的好处有两个：1. 提高模型的泛化能力；2. 加速收敛。如下： <span class="math display">\[
maximize \ L_3=L_2+\lambda L_1
\]</span> 模型的一些修正细节：</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/4.jpg"></p>
<p><strong>总结，GPT模型的优势如下：</strong></p>
<ul>
<li>使用了transformer而不是biLSTM，这样大大增强了模型的捕捉依赖关系的能力；</li>
<li>GPT是把模型接入到下游任务进行训练，可以加上原来的LM作为辅助目标函数。不侧重生成offline的词向量。</li>
</ul>
<h2 id="bert模型介绍">BERT模型介绍</h2>
<p>终于写到BERT啦，哈哈哈。BERT可以说是带火了预训练模型，BERT系现在大有统一NLP之势啊。废话不多说，直接进入正题！🤩</p>
<p>BERT模型，全称叫做：Bidirectional Encoder Representations from Transformers。从名字看就很直观，BERT是双向transformer的encoder部分。它所要解决的问题是：不管ELMo还是GPT，语言模型都是单向的，而BERT的作者发现，单向语言模型限制了fine-tuning的表达能力。所以，<strong>BERT所采取的方式是：使用双向的Transformer的encoder部分进行预训练，并采用MLM与NSP两种任务对BERT进行pre-training，之后在对接下游任务的时候，fine-tuning。</strong></p>
<h3 id="输入是什么">输入是什么？</h3>
<p>BERT的输入，可以是单个句子，也可以是句子对。<strong>但是在预训练的时候，实际上都是输入的句子对。</strong>在BERT中，使用的是BPE vocabulary，size为30000，最长的输入长度为512。之所以是512，是为了减少padding的计算浪费。这个在源码中可以看到。</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/5.jpg"></p>
<p>这是BERT的输入，有三部分：<strong>token embedding</strong>，就是每一个token的embedding，这个可以使用 word2vec的结果，也可以随机初始化；<strong>segment embedding</strong>，就是就来区别两个句子；<strong>position embedding</strong>，就是用来表示每一个token的位置信息，和transformer一样。<strong>这里需要注意的是：句子的第一个token一定是CLS，这是为了给后面做分类准备的，实际上是NSP任务来训练CLS token的。</strong></p>
<p>BERT中给的例子如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">This text is included to make sure Unicode is handled properly: 力加勝北区ᴵᴺᵀᵃছজটডণত</span><br><span class="line">Text should be one-sentence-per-line, with empty lines between documents.</span><br><span class="line">This sample text is public domain and was randomly selected from Project Guttenberg.</span><br><span class="line"></span><br><span class="line">The rain had only ceased with the gray streaks of morning at Blazing Star, and the settlement awoke to a moral sense of cleanliness, and the finding of forgotten knives, tin cups, and smaller camp utensils, where the heavy showers had washed away the debris and dust heaps before the cabin doors.</span><br><span class="line">Indeed, it was recorded in Blazing Star that a fortunate early riser had once picked up on the highway a solid chunk of gold quartz which the rain had freed from its incumbering soil, and washed into immediate and glittering popularity.</span><br><span class="line">Possibly this may have been the reason why early risers in that locality, during the rainy season, adopted a thoughtful habit of body, and seldom lifted their eyes to the rifted or india-ink washed skies above them.</span><br><span class="line">&quot;Cass&quot; Beard had risen early that morning, but not with a view to discovery.</span><br><span class="line">A leak in his cabin roof,--quite consistent with his careless, improvident habits,--had roused him at 4 A. M., with a flooded &quot;bunk&quot; and wet blankets.</span><br><span class="line">The chips from his wood pile refused to kindle a fire to dry his bed-clothes, and he had recourse to a more provident neighbor&#39;s to supply the deficiency.</span><br><span class="line">This was nearly opposite.</span><br><span class="line">Mr. Cassius crossed the highway, and stopped suddenly.</span><br><span class="line">Something glittered in the nearest red pool before him.</span><br><span class="line">Gold, surely!</span><br><span class="line">But, wonderful to relate, not an irregular, shapeless fragment of crude ore, fresh from Nature&#39;s crucible, but a bit of jeweler&#39;s handicraft in the form of a plain gold ring.</span><br><span class="line">Looking at it more attentively, he saw that it bore the inscription, &quot;May to Cass.&quot;</span><br><span class="line">Like most of his fellow gold-seekers, Cass was superstitious.</span><br><span class="line"></span><br><span class="line">The fountain of classic wisdom, Hypatia herself.</span><br><span class="line">As the ancient sage--the name is unimportant to a monk--pumped water nightly that he might study by day, so I, the guardian of cloaks and parasols, at the sacred doors of her lecture-room, imbibe celestial knowledge.</span><br><span class="line">From my youth I felt in me a soul above the matter-entangled herd.</span><br><span class="line">She revealed to me the glorious fact, that I am a spark of Divinity itself.</span><br><span class="line">A fallen star, I am, sir!&#39; continued he, pensively, stroking his lean stomach--&#39;a fallen star!--fallen, if the dignity of philosophy will allow of the simile, among the hogs of the lower world--indeed, even into the hog-bucket itself. Well, after all, I will show you the way to the Archbishop&#39;s.</span><br><span class="line">There is a philosophic pleasure in opening one&#39;s treasures to the modest young.</span><br><span class="line">Perhaps you will assist me by carrying this basket of fruit?&#39; And the little man jumped up, put his basket on Philammon&#39;s head, and trotted off up a neighbouring street.</span><br><span class="line">Philammon followed, half contemptuous, half wondering at what this philosophy might be, which could feed the self-conceit of anything so abject as his ragged little apish guide;</span><br><span class="line">but the novel roar and whirl of the street, the perpetual stream of busy faces, the line of curricles, palanquins, laden asses, camels, elephants, which met and passed him, and squeezed him up steps and into doorways, as they threaded their way through the great Moon-gate into the ample street beyond, drove everything from his mind but wondering curiosity, and a vague, helpless dread of that great living wilderness, more terrible than any dead wilderness of sand which he had left behind.</span><br><span class="line">Already he longed for the repose, the silence of the Laura--for faces which knew him and smiled upon him; but it was too late to turn back now.</span><br><span class="line">His guide held on for more than a mile up the great main street, crossed in the centre of the city, at right angles, by one equally magnificent, at each end of which, miles away, appeared, dim and distant over the heads of the living stream of passengers, the yellow sand-hills of the desert;</span><br><span class="line">while at the end of the vista in front of them gleamed the blue harbour, through a network of countless masts.</span><br><span class="line">At last they reached the quay at the opposite end of the street;</span><br><span class="line">and there burst on Philammon&#39;s astonished eyes a vast semicircle of blue sea, ringed with palaces and towers.</span><br><span class="line">He stopped involuntarily; and his little guide stopped also, and looked askance at the young monk, to watch the effect which that grand panorama should produce on him.</span><br></pre></td></tr></table></figure>
<p>这是BERT给的例子的输入。BERT对与输入的文本的要求是：<strong>1. 一个句子一行；2. 不同文章之间空一行，防止跨文章读取。</strong>那么，处理原始文本之后，得到的输出有：</p>
<ul>
<li>tokens：包括了普通的token，以及CLS、SEP、MASK。</li>
<li>segment_id：0表示是第一句话，1表示第二句话</li>
<li>is_random_next：是否为下一句，NSP人物的label</li>
<li>masked_lm_positions：被MASK的token的位置</li>
<li>masked_lm_labels：被MASK的token的原本的值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tokens: [CLS] for more than a [MASK] up [MASK] great main street , [MASK] [MASK] [MASK] centre of the city , at right angles , [MASK] one equally magnificent , at each end ##ミ which , miles away , appeared , dim and distant over the heads of the living stream of passengers , the yellow [MASK] - hills of [MASK] [MASK] ; while at the end of the vista in front [MASK] them gleamed the blue harbour , through a network [SEP] possibly this may have been the reason why early rise ##rs in [MASK] locality , during the rainy season , adopted [MASK] [MASK] [MASK] of body , and seldom lifted [MASK] eyes [MASK] the rift [MASK] or india - ink washed skies above them . [SEP]</span><br><span class="line">segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span><br><span class="line">is_random_next: True</span><br><span class="line">masked_lm_positions: 5 7 12 13 14 24 32 55 59 60 71 94 103 104 105 109 112 114 117</span><br><span class="line">masked_lm_labels: mile the crossed in the by of sand the desert of that a thoughtful habit and their to ##ed</span><br></pre></td></tr></table></figure>
<h3 id="pre-training-1">pre-training</h3>
<p><strong>Masked Language Model</strong></p>
<p>标准的语言模型，是单向的，只能根据之前的所有词来预测当前词。在BERT中，使用的是双向的transformer的encoder，这样不就让单词能够看到未来的内容了吗？BERT的处理方式就是使用MLM。MLM的核心思想是：<strong>随机mask掉句子的一部分内容，然后使用上下文来预测被mask的词。</strong>在论文中，具体的是mask掉句子的15%的token。但是这样做存在一个问题：在fine-tune中，模型可能会从来没见过mask掉的词，这样就会导致与fine-tune的输入分布不一样，为了减小这个问题。论文中提出，在每次选定一个单词进行mask的时候：</p>
<ul>
<li>有80%的概率，使用MASK替换；</li>
<li>有10%的概率，随机选定一个token进行替换；(加入噪声，使得模型更加具有鲁棒性)</li>
<li>有10%的概率，保持原样。(为了能够靠近fine-tune的输入分布)</li>
</ul>
<p>所以，我们可以看到，模型每次只预测15%的token，所以收敛的速度会非常慢，这也是BERT的缺陷之一，也是后来XLNet提出的原因之一。</p>
<p><strong>Next Sentence Prediction</strong></p>
<p>之所以设计这个任务，主要是为了解决需要确定句子对之间的关系的任务而设计的，譬如QA、文本蕴含、NLI等等。因为作者认为LM里面学不到这样的知识。这个任务要做的是：预测后面一个句子是不是前面一个句子的下一句。具体的做法是：<strong>以50%的概率选择一个句子是前面一个句子的下一句，label是：isNext；以50%的概率选择不是前面一个句子的下一句的句子，label是：NotNext。</strong>(然而，实验证明，NSP任务并没有什么用，所以，后来ALBERT改进了NSP任务，产生了很好的效果。)</p>
<h3 id="fine-tuning">fine-tuning</h3>
<p>fine-tuning与GPT没什么大的不痛，都是根据下游任务更改很少的顶层参数。对于分类任务，只加了一层分类层。</p>
<p><strong>总结，BERT模型的优势：</strong></p>
<ul>
<li>使用了transformer的encoder部分，特征抽取能力比lstm要强；</li>
<li>使用了MLM，是真正的双向语言模型，能够使用上下文的信息来预测被MASK掉的单词。</li>
</ul>
<h2 id="xlnet模型介绍">XLNet模型介绍</h2>
<p>XLNet模型是对BERT模型收敛太慢的改进，当然，XLNet模型并不只是这些。在了解XLNet模型之前，需要去了解transformer-xl这个模型，因为，XLNet模型就是基于transformer-xl而得来的。这里，我就不具体讲transformer-xl，简单提一下transformer-xl相较于transformer的改进的地方吧，有兴趣的筒子们可以自己去看transformer-xl的原始论文～（<strong>注意，目前基本没有延续XLNet的研究，所以这块大家如果不感兴趣的话，可以不去细看；当然也可以仔细研究，说不定在这上面就能搞出新点子也说不定哈哈～</strong>）</p>
<ul>
<li>transformer-xl是基于vanilla transformer的得来的。vanilla transformer的训练方式是：将训练文本进行分段，在每一段中分别去训练LM。在预测的时候，我们使用limited context来预测下一个单词，没预测完一个单词，就讲window向右移动一个字符。这样做的缺点在于：<strong>1.单词的上下文的长度受限，在原始论文中，只能处理512长度的输入文本；2. 推理速度非常慢，每次预测一个单词，都要重新计算；3.段与段之间没有上下文依赖性，影响模型性能。</strong>transformer-xl在此基础上引入了两个两个机制：<strong>循环机制与相对位置编码。</strong></li>
<li><strong>循环机制。</strong>循环机制的引入是为了解决段与段之间没有上下文依赖性。它能够让当前段在进行LM建模的时候，能够利用之前段的信息，从而实现长期依赖性。在预测的时候，就可以前进一整段，并且可以利用之前段的信息来预测当前的输出。</li>
<li><strong>相对位置编码。</strong>在普通的transformer中，使用固定的position embedding来表示token之间的顺序关系，但是如果分段处理的话，那么，不同分段的同一位置使用的是同一个position embedding，这显然是不对的。所以，在transformer-xl中，提出新的位置编码方式：<strong>相对位置编码。</strong>也就是说，会根据单词的相对位置进行编码，而不是像transformer的那样，按照绝对位置进行编码。最后，我们得到的结论是：<strong>transformer-xl的长期依赖性比LSTM高出80%(LSTM平均上下文长度200个左右)，比transformer高出450%！（transformer平均上下文长度低于50）是不是非常牛逼！</strong></li>
</ul>
<p>以上就是transformer-xl的大致内容，还是那句话，想要了解全部内容，请读transformer-xl的原始论文，其实只要看懂了transformer的话，transformer-xl决定小菜一碟🤩～接下来正式讲XLNet模型的内容～</p>
<p><strong>语言模型目标：排列语言建模</strong></p>
<p>使用这个方法，不仅可以保留AR模型的优点，同时也允许模型捕获双向信息。从所有的排列中采样一种，然后根据这个排列来分解联合概率成条件概率的乘积，然后加起来。也就是说，只要分解的顺序不一样，我们计算的顺序也就不一样。</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/20.jpg" style="zoom:67%;"></p>
<p><strong>模型架构：基于目标感知表征的双流自注意力</strong></p>
<p>这个还没具体细看，大致意思是：在teansformer里面，使用了token embedding+position embedding。但是如果我要预测第3个位置的单词的内容，那么，我不能提前获取到第3个位置的内容向量(token embedding)，只能只其位置向量(position embedding)，但是这在输入的时候，两个向量就已经混在一起了。所以在XLNet中，就计算了两套注意力，一个只包含它的位置信息，一个要包含它的位置信息与内容信息，以便使用。具体的，等我那天有时间了在具体写吧。。。</p>
<h2 id="ernie模型介绍">ERNIE模型介绍</h2>
<p>ERNIE是THU与华为诺亚一起提出来的针对于中文的预训练模型。(这里不得不佩服一波隔壁，NLP做的是真不错，小北要跟上啊！🧐)（发现百度也提出了ERNIE，重名了，而且似乎是百度先提出来的，好吧，那就再看一篇吧😭，似乎百度这篇读的比较顺畅！）</p>
<p>ERNIE模型提出的原因：<strong>BERT等模型只是针对字或者word粒度来学习，没有充分利用文本中的词法结构、语法结构以及语义信息去建模。</strong>譬如：<code>我要买苹果手机</code>这句话，我们MASK一个词，变成：<code>我要买[MASK]果手机</code>，如果使用BERT模型，我们当然可以预测MASK掉的字是<code>苹</code>。但是BERT模型没有学到<code>苹果手机</code>是一个名词这一信息，那么，如果在未知的文本中，出现了<code>香蕉手机</code>这样的词语，那么，BERT等模型是无法对其进行很好的向量表示的。所以ERNIE模型就诞生了。ERNIE模型的核心思想是：<strong>将外部知识整合到预训练模型中，让模型能够学习到词法、语法与语义信息。</strong></p>
<p>在ERNIE1.0中，除了BERT中的基础的masking策略，还提出了基于phrase的masking策略与基于entity的masking策略。在ERNIE中，将多个字组成的phrase与entity当一个单元，统一被mask。这样的话，模型就能够潜在的学会知识的依赖与语义依赖。</p>
<p>在ERNIE2.0中，提出一个很重要的概念：连续学习。也就是，模型顺序训练多个任务，以便在下个任务中记住前一个任务得到的结果，从而可以不断的积累新知识。具体的可以参看论文，这是大致的思想。</p>
<h2 id="albert模型介绍">ALBERT模型介绍</h2>
<p>ALBERT模型是对BERT模型的改进。怎么说呢，感觉ALBERT是一个调参调出来的模型🤪。它的目标是：<strong>在不损害模型性能的前提下，尽量减少BERT模型的参数。</strong>在ALBERT中，首先分析了BERT中模型的参数来源。在BERT中，参数主要分为两部分：<strong>token embedding的参数与attention、FFN层的参数</strong>。（以下所用到的图片来源于ALBERT第一作者lanzhenzhong博士的讲解。）</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/6.jpg"></p>
<h3 id="method1factorized-embedding-parametrization">Method1:factorized embedding parametrization</h3>
<p>token embedding占参数的20%，attention于FFN层的参数占80%。那么，我们就从这两方面入手。对于token embedding方面，我们可以先将其映射到一个地位i空间，再映射为高维空间。那么为什么可以这么做呢？有两点：**1. token embedding一般都是与上下文无关的，而attention层的输出一半都是与上下文有关系的，也就是说，输出层应该包含了更多的信息，所以，输出层的维度应该比token embedding的维度要高才比较合理；2. 如果让E与H相等的话，那么，整个词表的维度是V*H，那么这个词表是相当大的，梯度更新的时候，也会比较稀疏。**</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/7方法一.jpg"></p>
<p>当然，这样做会损失信息，但是结果表明，损失的性能并不多，而减少的参数却高达80%以上。</p>
<h3 id="method2-cross-layer-parameter-sharing">Method2: cross-layer parameter sharing</h3>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/10.jpg"></p>
<p>跨层的参数恭喜那个有两种方案：attention模块的参数共享以及FFN层的参数共享。在实验中，作者发现共享FFN的参数是导致参数下降的最主要的原因。</p>
<h3 id="sop--sentence-order-prediction">SOP--Sentence Order Prediction</h3>
<p>在BERT中，除了MLM之外，还使用了另一种任务——NSP来学习句子之间的关系，但是实现证明：NSP任务对于结果的影响并不大，原因在于：<strong>NSP任务实际上是预测两句话是采样于同一个文档中的两个连续的句子（正样本），还是采样于两个不同的文档中的句子（负样本）。</strong>所以在ALBERT中，对NSP任务进行了改造，叫做SOP任务。它的思想是：<strong>正样本的构建与BERT一样，负样本的构建则是将证样本的句子顺序调换一下。</strong>这样的话，就能够专心学习到句子之间的关系，而不会去学习topic。</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/12.jpg"></p>
<p>结果证明，SOP任务比NSP任务表现更加良好。</p>
<p><img src="/2020/03/02/NLP-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%93%E9%A2%98%E2%80%94ELMo-GPT-BERT-XLNet-ALBERT%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/13.jpg"></p>
<h3 id="去掉dropout">去掉Dropout</h3>
<p>dropout在防止过拟合上有显著效果，但是实际上MLM很难过拟合，去掉dropout，由于可以腾出很多临时变量占用的内存而使得内存上有所提升。</p>
<p><strong>总结，ALBERT模型的优势如下：</strong></p>
<ul>
<li>ALBERT模型通过两种技术，降低了参数的数量；</li>
<li>通过重新设计NSP任务，得到了SOP任务，最终改善了模型性能；</li>
<li>减少参数，同时也就提高了模型的capacity，即可以通过将网络变深变宽，提升模型。</li>
</ul>
<h2 id="预训练模型总结">预训练模型总结</h2>
<ul>
<li><p>整个预训练模型可分为两类：feature based approach与fine-tuning approach。前者的代表性模型就是ELMo，后者的代表性模型就是GPT与BERT。</p></li>
<li><p>整个语言模型有两种：AR与AE。AR语言模型指的是使用上下文单词来预测下一个单词的模型。上下文单词被限制了方向：前向或者后向。代表性模型有：ELMo、GPT、GPT2、XLNet。AE模型就是从损坏的数据中重建原始数据。也就是，我们使用MASK标记来替换原单词，然后使用上下文来预测MASK掉的单词。代表性模型有：BERT、ALBERT。</p></li>
<li><p>另外，说说我的看法吧，feature-based的预训练模型基本上大势已去，从word2vec、fasttext、ELMo等等；现在，pre-train+fine-tune两段式的预训练模型越来越受到欢迎，从GPT、BERT、XLNet、ALBERT、T5等等。未来NLP真有可能像cv那样，一个模型统一天下。我们只需要去改进模型所使用的特征抽取器就可以了。从RNN、CNN到transformer，特征抽取的能力越来越强，未来，我们只要不断改进transformer或者提出比transformer更好的模型，就能够大大提升模型在各项NLP任务上的表现。对于我们NLPer来说，是一件好事吧，因为不需要再去学很多千奇百怪的trick，只要去提升基础模型的能力，就可以得到很大的提高。</p></li>
</ul>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li>《Deep contextualized word representations》(ELMo)</li>
<li>https://blog.csdn.net/Magical_Bubble/article/details/89160032 (ELMo)</li>
<li>《Improving Language Understanding by Generative Pre-Training》(GPT)</li>
<li>https://blog.csdn.net/Magical_Bubble/article/details/89497002 (GPT)</li>
<li>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</li>
<li>https://blog.csdn.net/Magical_Bubble/article/details/89514057. (BERT)</li>
<li>《XLNet: Generalized Autoregressive Pretraining for Language Understanding》</li>
<li>https://blog.csdn.net/weixin_37947156/article/details/93035607 (XLNet)</li>
<li>https://blog.csdn.net/u012526436/article/details/93196139 (XLNet)</li>
<li>《ALBERT: A LITE BERT FOR SELF--SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS》</li>
<li>《ERNIE: Enhanced Representation through Knowledge Integration》</li>
<li>https://blog.csdn.net/PaddlePaddle/article/details/102713947 (ERNIE)</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>python</tag>
        <tag>self-attention</tag>
        <tag>BERT</tag>
        <tag>ELMo</tag>
        <tag>GPT</tag>
        <tag>ALBERT</tag>
        <tag>XLNET</tag>
        <tag>ERNIE</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|支持向量机模型原理详解与实现</title>
    <url>/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>支持向量机(SVM)是一种二类分类模型，属于监督学习，能够用来解决数据线性不可分的问题，是对感知机模型的补充和扩展。本篇博客将对SVM的三种情形进行详细地讲解，并采用纯python实现以及调用scikit-learn库实现，这两种方式对感知机模型进行实现。</p>
<a id="more"></a>
<h2 id="支持向量机模型概述">支持向量机模型概述</h2>
<p>支持向量机模型本身是一种二类分类模型，当然目前已经有很多方法，从而使得SVM实现多分类。SVM可以属于硬分类模型，它打破了感知机训练数据集线性可分的假设，从而能够处理训练数据集线性不可分的情况。SVM有三种情形：<strong>训练数据集线性可分、训练数据集近似线性可分、训练数据集线性不可分</strong>。当训练数据集线性可分时，我们使用硬间隔最大化策略，训练一个线性分类器，称为<strong>线性可分SVM</strong>；当训练数据集近似线性可分时（严格来说就是线性不可分），我们使用软间隔最大化策略，训练一个线性分类器，称为<strong>线性SVM</strong>；当训练数据集线性不可分时，我们使用核技巧+软间隔最大化策略，训练一个非线性分类器， 称为<strong>非线性SVM或kernel SVM</strong>。接下来，我们将逐步讲解这三种情况，依然从<strong>模型、策略、算法</strong>的角度，来进行讲解。</p>
<h2 id="线性可分svm">线性可分SVM</h2>
<p>当训练数据集时线性可分时，训练出来的分类器。我们称为线性可分SVM。</p>
<h3 id="线性可分svm模型">线性可分SVM模型</h3>
<p>线性可分SVM模型如下： <span class="math display">\[
y=f(x)=sign(wx+b),
\]</span></p>
<p><span class="math display">\[
sign(z)=
\begin{cases}
+1,&amp; \text{x&gt;=0}\\
-1,&amp; \text{x&lt;0}
\end{cases}.
\]</span></p>
<p>其中，<span class="math inline">\(x\)</span>是模型输入，<span class="math inline">\(y\)</span>是模型输出，并且<span class="math inline">\(x\in R^n，y\in \{1,-1\}\)</span>；<span class="math inline">\(w，b\)</span>是模型参数，并且<span class="math inline">\(w\in R^n，b\in R\)</span>。看到这个模型，是不是非常熟悉？对，就是之前讲过的感知机模型(perceptron)！实际上，我们就是要找一个分离超平面，将数据集中的正例与负例完全分开。但是在感知机中，分离超平面不唯一。那么，到底哪个超平面才是最优的呢？在SVM中，就是要找到最优的超平面。</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg"></p>
<p>那么，一般来说，对于被模型分类的点，我们关注两个方面：模型分类的确信程度，模型分类是否正确。对于模型分类的确定程度，我们可以使用点到平面的距离来表示；对于模型是否分类正确，我们使用模型的预测值与该实例本身的类别标号是否一致来表示。所以，我们可以使用<span class="math inline">\(y_i(wx_i+b)\)</span>来表示模型的正确性与确信程度，这就是函数间隔。但是，只有函数间隔是不够的，因为当<span class="math inline">\(w,b\)</span>增大若干倍的时候，超平面没变，但是函数间隔却变大了，所以我们需要对其加一些约束，这就是几何间隔。几何间隔越大，那么点距离超平面就越远(距离超平面越近的点，那么越难被正确分类)，那么说明这样的超平面的泛化能力应该越好。所以，<strong>线性可分SVM的策略就是找到能够正确分类并且几何间隔最大的分离超平面。</strong></p>
<h3 id="线性可分svm模型的学习策略">线性可分SVM模型的学习策略</h3>
<p><strong>原始形式</strong></p>
<p>线性可分SVM模型的学习策略叫做最大间隔法。就是找到间隔最大的分离超平面。那么就转换为下面这个最优化问题：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1..jpg" style="zoom: 50%;"></p>
<p>其中，<span class="math inline">\(\gamma\)</span> 表示几何间隔，此外，带有一个约束条件：所有的点到超平面的几何间隔都必须大于最小的几何间隔，这是显而易见的。又由于几何间隔与函数间隔的转换关系，那么，我们可以将上述最优化问题，变换一下形式，如下:</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.jpg" style="zoom:50%;"></p>
<p>我们仔细分析一下这个最优化问题，我们会发现函数间隔<span class="math inline">\(\hat \gamma\)</span> 不影响目标函数与约束条件。因为当函数间隔<span class="math inline">\(\hat \gamma\)</span> 增大<span class="math inline">\(\lambda\)</span>倍后，及<span class="math inline">\(w,b\)</span>均增大<span class="math inline">\(\lambda\)</span>倍后，将其带入上述目标函数与约束条件，会发现整个问题并不会发生变化，因为<span class="math inline">\(\lambda\)</span>都被约掉了。所以，我们不妨就设<span class="math inline">\(\lambda=1\)</span>。此外，在机器学习中，我们更加习惯求解最小化问题，那么，化简后就是最终的最优化问题。公式如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg" style="zoom:50%;"></p>
<p><strong>总结一下</strong>：</p>
<p>输入：给定训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i\in R^n，y\in R，i=1...N\)</span>。</p>
<p>输出：参数<span class="math inline">\(w，b\)</span>，从而得到SVM模型与分离超平面。</p>
<ul>
<li>构造最优化问题：</li>
</ul>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg" style="zoom:50%;"></p>
<p>通过求解凸二次规划问题的最优化算法，求得<span class="math inline">\(w^{\ast},b^{\ast}\)</span>。</p>
<ul>
<li>将<span class="math inline">\(w^{\ast},b^{\ast}\)</span> 带入公式中，得到SVM模型与分离超平面。如下：</li>
</ul>
<p><span class="math display">\[
f(x)=sign(w^{\ast}x+b^{\ast})，w^{\ast}x+b^{\ast}=0.
\]</span></p>
<p><strong>对偶形式</strong></p>
<p>对于凸二次规划问题，我们可以通过求解其对偶问题，从而来求原始问题的解。我们使用拉格朗日乘子法，构造拉格朗日函数，如下:</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/4.0.jpg" style="zoom:50%;"></p>
<p>那么，我们要求的公式就是：<span class="math inline">\(max_{\alpha}min_{w,b}L(w,b,\alpha)\)</span>。所以就分两步：先求令<span class="math inline">\(L(w,b,\alpha)\)</span>最小的<span class="math inline">\(w,b\)</span>(此时的<span class="math inline">\(w,b\)</span>的值时带有未知变量<span class="math inline">\(\alpha\)</span>的，还不是最终的值)，再求令令<span class="math inline">\(L(w,b,\alpha)\)</span>最大的<span class="math inline">\(\alpha\)</span>的值，最后根据<span class="math inline">\(\alpha\)</span>的值，得出<span class="math inline">\(w,b\)</span>最终的值。</p>
<ul>
<li>先求<span class="math inline">\(min_{w,b}L(w,b,\alpha)\)</span>。我们对<span class="math inline">\(w,b\)</span>求偏导，即令<span class="math inline">\(\frac {\partial L}{\partial w}=0，\frac {\partial L}{\partial b}=0\)</span>，可以得到下述式子：</li>
</ul>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/4.0.1.jpg" style="zoom: 50%;"></p>
<p>将上述式子带入到最初的拉格朗日函数中，并化简得到如下公式：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/4.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/5.jpg"></p>
<ul>
<li>接下来，我们求<span class="math inline">\(max_{\alpha}min_{w,b}L(w,b,\alpha)\)</span>。</li>
</ul>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/6.jpg" style="zoom:50%;"></p>
<p>由于其满足KKT条件，所谓KKT条件如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/7.jpeg"></p>
<p>我们会根据KKT条件，尤其是互补松弛条件，可以得出b的值。参数<span class="math inline">\(w,b\)</span>最终的值如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/8.jpg" style="zoom:50%;"></p>
<p><strong>总结一下：</strong></p>
<p>输入：给定训练数据集<span class="math inline">\(X=\{(x_1,y_1),(x_2,y_2),(x_3,y_3),...,(x_N,y_N)\}\)</span>，其中<span class="math inline">\(x_i\in R^n，y\in R，i=1...N\)</span>。</p>
<p>输出：参数<span class="math inline">\(w，b\)</span>，从而得到SVM模型与分离超平面。</p>
<ul>
<li>构造最优化问题，并求解最优化问题：</li>
</ul>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/9.jpg" style="zoom:50%;"></p>
<p>求得最优解<span class="math inline">\(\alpha^{\ast}=\{\alpha^{\ast}_{1},\alpha^{\ast}_{2},...,\alpha^{\ast}_{N}\}\)</span>。</p>
<ul>
<li>根据<span class="math inline">\(\alpha^{\ast}\)</span>，计算得到最优参数<span class="math inline">\(w^{\ast},b^{\ast}\)</span>，公式如下：</li>
</ul>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/8.jpg" style="zoom:50%;"></p>
<ul>
<li>将参数<span class="math inline">\(w^{\ast},b^{\ast}\)</span>带入SVM模型与超平面公式中，就得到了结果。</li>
</ul>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/10.jpg" style="zoom:50%;"></p>
<h3 id="学习算法">学习算法</h3>
<p>即便在对偶形式的SVM中，最终还是要求<span class="math inline">\(\alpha^{\ast}\)</span>的值，这个时候就需要采取求解凸二次规划问题的算法求解，在SVM中，我们采取的方法叫做：<strong>序列最小最优化算法(SMO)</strong>。后面会介绍。</p>
<h2 id="线性svm">线性SVM</h2>
<p>当线性可分中数据集混入一些噪音(outlier)，也就是分类不正确的点，那么这个时候，整个数据集就变得线性不可分了；但是由于噪音的数量较小，所以我们可以通过改变一下线性可分SVM的学习策略，从而训练处线性SVM。</p>
<p><strong>注意：由于线性可分SVM与线性SVM的模型与对偶最优化问题的化简求解是一样的，所以在这里，我只会将讲解其与线性可分SVM不通的地方，也就是学习策略：软间隔最大化。</strong></p>
<p>所谓的噪音<span class="math inline">\((x_i,y_i)\)</span>，也就是它的几何间隔不满足约束，也就是<span class="math inline">\(y_i(wx_i+b)&lt;1\)</span>。那么，我们就可以加入一个松弛变量<span class="math inline">\(\varepsilon_i\)</span>，使得其约束条件大于或等于1。此外，我们需要在目标函数中加入惩罚项<span class="math inline">\(C\)</span>，惩罚被误分类的点，从而使得实例更难被误分类。具体公式如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/11.jpg" style="zoom:50%;"></p>
<p>这是一个凸二次规划问题，同样地，我们可以通过求其对偶最优化问题，来求解参数$w,b，从而得到SVM模型与分离超平面，步骤与线性可分SVM步骤一样(这个推导步骤，最好自己手推一遍～)。最后结果如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/12.jpg" style="zoom:50%;"></p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/13.jpg" style="zoom:50%;"></p>
<h2 id="非线性svm-kernel-svm">非线性SVM / kernel SVM</h2>
<p>当线性可分数据集中，混入了大量的噪音，导致整个数据集完全不可通过线性模型来进行分类的话，这个时候就需要非线性SVM了。当然，在这种SVM中，一个非常重要的概念叫做 <strong>kernel trick</strong>。下面就简单地介绍一下什么是kernel trick。(关于kernel trick详细的数学证明与推导，请参看<strong>《统计学习方法》或者PRML</strong>，在这里，我只讲解kernel trick的原理。)</p>
<h3 id="kernel-trick">kernel trick</h3>
<p>由于在原来的输入空间中，我们得到的分离超平面最终会是一个曲面，那么这样的非线性模型是不好训练的。那么一种想法是：将原来的输入空间通过非线性变换，将其映射到一个高维空间，将非线性问题转换为线性问题，从而在新的特征空间中学习线性分类模型。(这是有定理保证的，高维比低维更易线性可分。)这其实就是kernel trick的思想。</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/14.jpg" style="zoom:50%;"></p>
<p>这个公式中，<span class="math inline">\(\phi\)</span>是非线性映射，<span class="math inline">\(x,z\)</span>均属于原来的输入空间，正定核函数<span class="math inline">\(K(x,z)\)</span>是变换后的结果。通过这个变换，我们就可以将原来输入空间中的内积<span class="math inline">\(x_ix\)</span>，变换到新的特征空间中的内积<span class="math inline">\(\phi(x_i)\phi(x)\)</span>，从而在新的特征空间中学习线性分类模型即可。</p>
<p>但是，这样带来一个问题：非线性映射<span class="math inline">\(\phi\)</span>是非常难求的，求它们的内积更加难求。那么，在实际操作中，我们通常的做法是直接去<span class="math inline">\(K(x,z)\)</span>，而不通过找<span class="math inline">\(\phi\)</span>来计算结果，这样大大减少了计算量。那么如果判断一个函数是正定核函数(kernel function)呢？在这里有一个定理，叫做<strong>Mercer定理</strong>。它其实是从核函数的定义出发的(这里可能和《统计学习方法》表述不一样，但是意思是一样的)。如下：</p>
<p><strong>设<span class="math inline">\(x,z\in\cal X\in R^n\)</span>，<span class="math inline">\(\cal X\)</span>表示原来的输入空间，对于任意的<span class="math inline">\(x,z\)</span>，存在函数<span class="math inline">\(K:\cal X *\cal X -&gt;R^p\)</span>，并且这个函数<span class="math inline">\(K\)</span>满足两个性质：对称性与正定性，那么就说函数<span class="math inline">\(K\)</span>是正定核函数。</strong>所谓对称性，就是<span class="math inline">\(K(x,z)=K(z,x)\)</span>。所谓正定性，就是<span class="math inline">\(K(x,z)\)</span>所对应的Gram矩阵是半正定的，半正定的意思是该矩阵的特征值大于等于0。对称性显而易见，那么判断一个<strong>对称函数是不是的正定核函数的充要条件就是其构成的Gram矩阵是半正定的</strong>。具体证明过程，可以参看《统计学习方法》。我们常用的正定核函数是高斯核函数，如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/15.jpg" style="zoom:50%;"></p>
<p>那么，根据高斯核函数，我们可以得到SVM模型，也就是分类决策函数如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/16.jpg" style="zoom:50%;"></p>
<h2 id="序列最小最优化算法smo">序列最小最优化算法(SMO)</h2>
<p>这是SVM中最后一个部分，主要是用来求解凸二次规划问题，也就是求<span class="math inline">\(\alpha\)</span>的值，只有求出了<span class="math inline">\(\alpha\)</span>的值，<span class="math inline">\(w,b\)</span>的值才算是真正的求出来。我们将核函数带入到最优化问题当中，得到如下式子：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/17.0.jpg" style="zoom:50%;"></p>
<p>其实有很多最优化算法来求解这个凸二次规划问题，在SVM中，我们使用SMO算法来求解，因为当数据量很大的时候，该算法也能很好很快地求解。该算法的思想是：由于KKT条件是最优化问题最优解的充要条件，如果所有的变量<span class="math inline">\(\alpha\)</span>都满足KKT条件的话，那么最优解就求出来了。如果不满足的话，那么我们就不断地去调整<span class="math inline">\(\alpha\)</span>的值，直到所有的<span class="math inline">\(\alpha\)</span>都满足KKT1条件。那么怎么调整呢？方法是：<strong>我们构建多个只有两个变量的二次规划问题，最终找到所有最优的<span class="math inline">\(\alpha\)</span>的值</strong>。具体过程如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/17.jpeg" style="zoom: 50%;"></p>
<p>通过上述计算，我们就可以得到未修建的<span class="math inline">\(\alpha_2\)</span>与<span class="math inline">\(\alpha_1\)</span>的值。但是在得到这个值的时候，我们并没有考虑约束条件，所以我们要将约束条件加上，得到最终的<span class="math inline">\(\alpha_2\)</span>的值，计算过程如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/18.jpeg" style="zoom:50%;"></p>
<p>那得到<span class="math inline">\(\alpha_2\)</span>的后，其实<span class="math inline">\(\alpha_1\)</span>的就很容易求出来了，如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/19.jpeg" style="zoom: 33%;"></p>
<p>之后，我们不断去重复这样的过程，便可将所有的<span class="math inline">\(\alpha\)</span>的值都求出来。但是，问题来了，<strong>我们每次怎么去选择这两个变量呢？</strong>也就是说，我们怎么知道选择哪两个变量，才能使得最优化问题能够朝着最小的方向走呢？</p>
<p>在SMO中，<strong>第一个变量，我们可以选择违反KKT条件最明显的变量作为<span class="math inline">\(\alpha_1\)</span>。</strong>KKT条件如下：</p>
<p><img src="/2020/02/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/20.jpg" style="zoom:50%;"></p>
<p>在实际操作中，可以遍历所有的样本点，看哪个违反最严重，从而将该点最为第一个变量。</p>
<p><strong>第二个变量，找使<span class="math inline">\(|E_1-E_2|\)</span>的变量</strong>。<span class="math inline">\(E_i\)</span>的计算公式，前面已经提到了。</p>
<p>以上就是全部的SVM的理论部分了～</p>
<h2 id="svm的实现">SVM的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现SVM模型：纯python实现以及调用scikit-learn库来实现。我的github里面可以下在到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2019-12-20</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist(实际只使用了前1000个，当然可以全部使用，如果算力够的话。)</span></span><br><span class="line"><span class="string">准确率：0.98</span></span><br><span class="line"><span class="string">时间：0.98</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName:路径名</span></span><br><span class="line"><span class="string">    :return: 返回特征向量与标签类别</span></span><br><span class="line"><span class="string">    在这里需要注意的是：最后使用要np.array，以保证维度。</span></span><br><span class="line"><span class="string">    当然使用np.mat等也没有问题，但是相关操作容易搞混。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = []</span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">if</span> (int(curline[<span class="number">0</span>]) ==<span class="number">0</span>):</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">-1</span>)</span><br><span class="line">            data_list.append([int(num)/<span class="number">255</span> <span class="keyword">for</span> num <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_matrix, label_matrix, sigma, C, toler, iteration)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        构造函数</span></span><br><span class="line"><span class="string">        :param data_matrix: 训练数据集矩阵</span></span><br><span class="line"><span class="string">        :param label_matrix: 训练数据标签矩阵</span></span><br><span class="line"><span class="string">        :param sigma: 高斯核函数的sigma</span></span><br><span class="line"><span class="string">        :param C: 惩罚参数</span></span><br><span class="line"><span class="string">        :param toler: 松弛变量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        self.train_data = data_matrix</span><br><span class="line">        self.train_label = label_matrix.T</span><br><span class="line">        self.input_num,self.feature_num = np.shape(self.train_data)</span><br><span class="line"></span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.C = C</span><br><span class="line">        self.toler = toler</span><br><span class="line">        self.K = self.kernel_for_train()</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.alpha = np.zeros(self.input_num)</span><br><span class="line">        self.E = np.zeros(self.input_num)</span><br><span class="line">        self.iteration = iteration</span><br><span class="line">        self.support_vector = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kernel_for_train</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算核函数</span></span><br><span class="line"><span class="string">        使用的是高斯核 详见“7.3.3 常用核函数” 式7.90</span></span><br><span class="line"><span class="string">        :return: 高斯核矩阵</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        k=np.zeros((self.input_num,self.input_num))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_num):</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'construct the kernel:'</span>, i, self.input_num)</span><br><span class="line">            X = self.train_data[i, :]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i, self.input_num):</span><br><span class="line">                Z = self.train_data[j, :]</span><br><span class="line">                result =np.matmul((X - Z) , (X - Z).T)</span><br><span class="line">                result = np.exp(<span class="number">-1</span> * result / (<span class="number">2</span> * self.sigma**<span class="number">2</span>))</span><br><span class="line">                k[i][j] = result</span><br><span class="line">                k[j][i] = result</span><br><span class="line">        <span class="comment">#返回高斯核矩阵</span></span><br><span class="line">        <span class="keyword">return</span> k</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_satisfied_kkt_condition</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        判断alpha_i是否满足KKT条件</span></span><br><span class="line"><span class="string">        :param i: 索引第i个alpha</span></span><br><span class="line"><span class="string">        :return: 返回true/false</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        g_xi = self.cal_g_xi(i)</span><br><span class="line">        y_i = self.train_label[i]</span><br><span class="line">        <span class="keyword">if</span> ((abs(self.alpha[i]) &lt; self.toler) <span class="keyword">and</span> (y_i * g_xi &gt;= <span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> (abs(self.alpha[i] - self.C) &lt; self.toler <span class="keyword">and</span> (y_i * g_xi &lt;= <span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> (self.alpha[i] &gt; -self.toler) <span class="keyword">and</span> (self.alpha[i] &lt; (self.C + self.toler)) \</span><br><span class="line">                <span class="keyword">and</span> (abs(y_i * g_xi - <span class="number">1</span>) &lt; self.toler):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_g_xi</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算g_xi，从而辅助判断变量是不是满足KKT条件</span></span><br><span class="line"><span class="string">        :return: 返回g_xi的值,是标量</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        g_xi = <span class="number">0</span></span><br><span class="line">        index = [i <span class="keyword">for</span> i,alpha <span class="keyword">in</span> enumerate(self.alpha) <span class="keyword">if</span> alpha!=<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> index:</span><br><span class="line">            g_xi += self.alpha[j] * self.train_label[j] * self.K[j][i]</span><br><span class="line">        g_xi += self.b</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> g_xi</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_ei</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算ei的值，标量</span></span><br><span class="line"><span class="string">        :param i: 表示第i个样本，i=1，2</span></span><br><span class="line"><span class="string">        :return: 返回Ei</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> self.cal_g_xi(i) - self.train_label[i]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_alpha_j</span><span class="params">(self, e1, i)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        在SMO中，选择第2个变量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param e1: 第一个变量的e1</span></span><br><span class="line"><span class="string">        :param i: 第一个变量alpha1的下标</span></span><br><span class="line"><span class="string">        :return: 返回E2，第二个遍历变量的下标</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        e2 = <span class="number">0</span></span><br><span class="line">        max_e1_e2 = <span class="number">-1</span></span><br><span class="line">        max_index = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">        none_0_e = [i <span class="keyword">for</span> i, ei <span class="keyword">in</span> enumerate(self.E) <span class="keyword">if</span> ei != <span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> none_0_e:</span><br><span class="line">            <span class="comment"># 计算E2</span></span><br><span class="line">            e2_tmp = self.cal_ei(j)</span><br><span class="line">            <span class="comment"># 如果|E1-E2|大于目前最大值</span></span><br><span class="line">            <span class="keyword">if</span> abs(e1 - e2_tmp) &gt; max_e1_e2:</span><br><span class="line">                <span class="comment"># 更新最大值</span></span><br><span class="line">                max_e1_e2 = abs(e1 - e2_tmp)</span><br><span class="line">                <span class="comment"># 更新最大值E2</span></span><br><span class="line">                e2 = e2_tmp</span><br><span class="line">                <span class="comment"># 更新最大值E2的索引j</span></span><br><span class="line">                max_index = j</span><br><span class="line">        <span class="comment"># 如果列表中没有非0元素了（对应程序最开始运行时的情况）</span></span><br><span class="line">        <span class="keyword">if</span> max_index == <span class="number">-1</span>:</span><br><span class="line">            max_index = i</span><br><span class="line">            <span class="keyword">while</span> max_index == i:</span><br><span class="line">                <span class="comment"># 获得随机数，如果随机数与第一个变量的下标i一致则重新随机</span></span><br><span class="line">                max_index = int(np.random.uniform(<span class="number">0</span>, self.input_num))</span><br><span class="line">            <span class="comment"># 获得E2</span></span><br><span class="line">            e2 = self.cal_ei(max_index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> e2, max_index</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        cur_iter_step = <span class="number">0</span></span><br><span class="line">        params_changed = <span class="number">1</span></span><br><span class="line">        <span class="comment">#结束循环条件：1.达到循环次数；2.当参数不再发生变化的时候，说明收敛了</span></span><br><span class="line">        <span class="keyword">while</span> ((cur_iter_step &lt; self.iteration) <span class="keyword">and</span> (params_changed &gt; <span class="number">0</span>)):</span><br><span class="line">            print(<span class="string">f"the current iteration is <span class="subst">&#123;cur_iter_step&#125;</span>,the total iteration is <span class="subst">&#123;self.iteration&#125;</span>."</span>)</span><br><span class="line">            cur_iter_step += <span class="number">1</span></span><br><span class="line">            params_changed = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_num):</span><br><span class="line">                <span class="keyword">if</span> (self.is_satisfied_kkt_condition(i) == <span class="literal">False</span>):</span><br><span class="line">                    e1 = self.cal_ei(i)</span><br><span class="line">                    e2, j = self.cal_alpha_j(e1, i)</span><br><span class="line"></span><br><span class="line">                    y1 = self.train_label[i]</span><br><span class="line">                    y2 = self.train_label[j]</span><br><span class="line"></span><br><span class="line">                    alpha_old_1 = self.alpha[i]</span><br><span class="line">                    alpha_old_2 = self.alpha[j]</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (y1 != y2):</span><br><span class="line">                        L = max(<span class="number">0</span>, alpha_old_2 - alpha_old_1)</span><br><span class="line">                        H = min(self.C, self.C - alpha_old_1 + alpha_old_2)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        L = max(<span class="number">0</span>, alpha_old_1 + alpha_old_2 - self.C)</span><br><span class="line">                        H = min(self.C, alpha_old_1 + alpha_old_2)</span><br><span class="line">                    <span class="keyword">if</span> (L == H):</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                    k11 = self.K[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">                    k22 = self.K[<span class="number">2</span>][<span class="number">2</span>]</span><br><span class="line">                    k12 = self.K[<span class="number">1</span>][<span class="number">2</span>]</span><br><span class="line">                    k21 = self.K[<span class="number">2</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                    alpha_new_2 = alpha_old_2 + y2 * (e1 - e2) / (k11 + k22 - <span class="number">2</span> * k12)</span><br><span class="line">                    <span class="keyword">if</span> (alpha_new_2 &lt; L):</span><br><span class="line">                        alpha_new_2 = L</span><br><span class="line">                    <span class="keyword">elif</span> (alpha_new_2 &gt; H):</span><br><span class="line">                        alpha_new_2 = H</span><br><span class="line"></span><br><span class="line">                    alpha_new_1 = alpha_old_1 + y1 * y2 * (alpha_old_2 - alpha_new_2)</span><br><span class="line"></span><br><span class="line">                    b1_new = <span class="number">-1</span> * e1 - y1 * k11 * (alpha_new_1 - alpha_old_1) \</span><br><span class="line">                             - y2 * k21 * (alpha_new_2 - alpha_old_2) + self.b</span><br><span class="line">                    b2_new = <span class="number">-1</span> * e2 - y1 * k12 * (alpha_new_1 - alpha_old_1) \</span><br><span class="line">                             - y2 * k22 * (alpha_new_2 - alpha_old_2) + self.b</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> (alpha_new_1 &gt; <span class="number">0</span>) <span class="keyword">and</span> (alpha_new_1 &lt; self.C):</span><br><span class="line">                        b_new = b1_new</span><br><span class="line">                    <span class="keyword">elif</span> (alpha_new_2 &gt; <span class="number">0</span>) <span class="keyword">and</span> (alpha_new_2 &lt; self.C):</span><br><span class="line">                        b_new = b2_new</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        b_new = (b1_new + b2_new) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">                    self.alpha[i] = alpha_new_1</span><br><span class="line">                    self.alpha[j] = alpha_new_2</span><br><span class="line">                    self.b = b_new</span><br><span class="line"></span><br><span class="line">                    self.E[i] = self.cal_ei(i)</span><br><span class="line">                    self.E[j] = self.cal_ei(j)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> abs(alpha_new_2 - alpha_old_2) &gt;= <span class="number">0.00001</span>:</span><br><span class="line">                        params_changed += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.input_num):</span><br><span class="line">            <span class="comment"># 如果α&gt;0，说明是支持向量</span></span><br><span class="line">            <span class="keyword">if</span> self.alpha[i] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 将支持向量的索引保存起来</span></span><br><span class="line">                self.support_vector.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kernel_for_predict</span><span class="params">(self, x, z)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算核函数，用于测试集与开发集</span></span><br><span class="line"><span class="string">        :param x:向量x</span></span><br><span class="line"><span class="string">        :param z: 向量z</span></span><br><span class="line"><span class="string">        :return: 返回核函数</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        result = np.matmul((x - z), (x - z).T)</span><br><span class="line">        result = np.exp(-result / (<span class="number">2</span> * self.sigma ** <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        预测单个样本的类别</span></span><br><span class="line"><span class="string">        :param x: 样本的特征向量</span></span><br><span class="line"><span class="string">        :return: 样本的预测类别</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        result = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.support_vector:</span><br><span class="line">            result_temp = self.kernel_for_predict(x, self.train_data[i])</span><br><span class="line">            result += self.alpha[i] * self.train_label[i] * result_temp</span><br><span class="line"></span><br><span class="line">        result += self.b</span><br><span class="line">        <span class="keyword">return</span> np.sign(result)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, test_data, test_label)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        在测试集上测试模型</span></span><br><span class="line"><span class="string">        :param test_train:测试集的特征向量</span></span><br><span class="line"><span class="string">        :param test_label:测试集的类别</span></span><br><span class="line"><span class="string">        :return:准确率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        test_label=test_label.T</span><br><span class="line">        error = <span class="number">0</span></span><br><span class="line">        test_input_num = test_data.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(test_input_num):</span><br><span class="line">            result = self.predict(test_data[i])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (result != test_label[i]):</span><br><span class="line">                error += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (test_input_num - error) / test_input_num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line">    svm = SVM(train_data[:<span class="number">1000</span>], train_label[:<span class="number">1000</span>], sigma=<span class="number">10</span>, C=<span class="number">200</span>, toler=<span class="number">0.001</span>,iteration=<span class="number">50</span>)</span><br><span class="line">    svm.train()</span><br><span class="line">    print(<span class="string">"finish the training."</span>)</span><br><span class="line">    accuracy = svm.test(test_data[:<span class="number">100</span>], test_label[:<span class="number">100</span>])</span><br><span class="line">    print(<span class="string">f"accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="scikit-learn实现">scikit-learn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2019-12-20</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    data_list = []</span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">if</span> (int(curline[<span class="number">0</span>]) == <span class="number">0</span>):</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">-1</span>)</span><br><span class="line">            data_list.append([int(feature) <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../Mnistdata/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line">    <span class="comment">#创建模型</span></span><br><span class="line">    clf = svm.SVC()</span><br><span class="line">    <span class="comment">#训练模型</span></span><br><span class="line">    clf.fit(train_data[:<span class="number">1000</span>], train_label[:<span class="number">1000</span>])</span><br><span class="line">    print(<span class="string">"finished training."</span>)</span><br><span class="line">    <span class="comment">#在测试集上测试模型</span></span><br><span class="line">    accuracy = clf.score(test_data, test_label)</span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|最大熵模型原理详解与实现</title>
    <url>/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>最大熵模型(MaxEntropy)，是机器学习中一个非常重要的分类模型，其是最大熵思想应用到分类问题的结果。本篇博客将对最大熵模型的原理进行详细的讲解，并采用python实现对最大熵模型进行实现(scikit-learn库没有关于最大熵的类库)。此外，还会对比讲解其logistic回归的区别，并实现logistic回归模型(python与scikit-learn库)。</p>
<a id="more"></a>
<h2 id="最大熵模型概述">最大熵模型概述</h2>
<p>关于使用最大熵模型，我们最终要求的是：给定实例的特征向量<span class="math inline">\(x\)</span>，使得条件概率<span class="math inline">\(p(y|x)\)</span>最大的<span class="math inline">\(y\)</span>。但是它与朴素贝叶斯不一样，朴素贝叶斯是通过求联合概率，然后根据贝叶斯定理求得<span class="math inline">\(p(y|x)\)</span>；而最大熵模型是直接求<span class="math inline">\(p(y|x)\)</span>。它所使用的思想就是：<strong>最大熵思想</strong>。所谓的最大熵思想就是：<strong>当我们要求一个条件概率分布的时候，如果我们没有任何的先验知识的时候，那么我们应该选熵最大的分布；如果我们已经有了一些先验知识，那么我们应该选满足这些先验知识的情况下，熵最大的分布。</strong>下面我将介绍最大熵模型如何贯彻最大熵思想的。</p>
<h2 id="最大熵模型">最大熵模型</h2>
<p>首先我们来看一下熵的概念，如果离散随机变量的概率分布是<span class="math inline">\(P(x)\)</span>，那么它的熵如下： <span class="math display">\[
H(P)=-\sum_xP(x)log(P(x))
\]</span> 其中，<span class="math inline">\(x\)</span>表示随机变量<span class="math inline">\(X\)</span>的取值。那么对于条件概率分布<span class="math inline">\(P(y|x)\)</span>，它的熵，我们可以套用公式，如下： <span class="math display">\[
H(P)=-\sum_{y}P(y|x)log(P(y|x))
\]</span> 但是这样还不够。因为我们要保证在所有情况下，熵都是最大的，所以需要引入<span class="math inline">\(\tilde P(x)\)</span>。如下： <span class="math display">\[
H(P)=-\sum_{x,y}\tilde P(x)P(y|x)log(P(y|x))
\]</span> 关于<span class="math inline">\(\tilde P(x)\)</span>，我们可以这么理解：<strong>由于我们要求的是最大熵，也就是说，在满足先验知识的基础上，总的熵是最大的。由于对于每一个<span class="math inline">\(x\)</span>，都会对应一个概率分布<span class="math inline">\(P(y|x)\)</span>，也就会对应一个熵。所以，最大化熵，也就是最大化这些所有的熵的和。此外，我们还需要体现出每一个概率分布的熵的重要性，我们需要对其进行加权求和，<span class="math inline">\(\tilde P(x)\)</span>就相当于权重。</strong></p>
接下来，我们还需要定义一些约束条件。因为训练集就相当于是先验知识，最大熵就是满足先验知识的基础上的熵最大的条件概率分布。我们定义特征函数<span class="math inline">\(f_i(x,y)\)</span>，如下： $$ f_i(x,y)=
<span class="math display">\[\begin{cases}
1&amp;,\ x与y满足某一事实 \\
0&amp;,\ 否则 

\end{cases}\]</span>
<p><span class="math display">\[
其中$i$表示第$i$个特征，$i=1,2,3,...,n$。那么$f(x,y)$关于$\tilde P(x,y)$的期望如下：
\]</span> E_{P}=<em>{x,y}P(x,y)f(x,y)=</em>{x,y}P(x)P(y|x)f(x,y) $$ 如果说我们的模型正确的话，那么<span class="math inline">\(P(y|x)=\tilde P(y|x)\)</span>。所以这就是一种约束，约束我们必须要去得到正确的模型！</p>
<p><strong>最大熵模型总结如下：</strong></p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg" style="zoom:50%;"></p>
<p>由于我们习惯最小化问题，所以：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom:50%;"></p>
<h2 id="最大熵模型的学习">最大熵模型的学习</h2>
<p>我们首先将最大熵模型摘抄下来：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg" style="zoom:50%;"></p>
<p>那么，对于有约束的最优化问题，很自然地就想到拉格朗日乘子法，所以我们可以写成如下形式：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.jpg" style="zoom:50%;"></p>
<p>所以，对于原始问题，我们的目标是：<span class="math inline">\(min_Pmax_wL(P,w)\)</span>；对于对偶问题，我们的目标就是：<span class="math inline">\(max_wmin_PL(P,w)\)</span>。我们首先求<span class="math inline">\(min_PL(P,w)\)</span>，也就是求<span class="math inline">\(\frac {\partial L}{\partial P}=0\)</span>，假设我们得到的最优解记为：<span class="math inline">\(P_w(y|x)\)</span>，最后结果是：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg" style="zoom:50%;"></p>
<p>在求得了<span class="math inline">\(P_w(y|x)\)</span>之后，我们就只需要求<span class="math inline">\(max_w(P_w,w)\)</span>即可。将<span class="math inline">\(P_w\)</span> 带入到原来的拉格朗日函数，化简，然后求导，即：<span class="math inline">\(\frac {\partial L(P_w,w)}{\partial w}=0\)</span>。但是我们会发现，化简后太难求了，所以我们就采用另外一种方式，就是<strong>将对偶函数的最大化转化为最大熵模型的极大似然估计</strong>。下面是推导出最大熵模型的极大似然估计过程(《统计学习方法》中并没有给出推导过程)：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/4.jpeg" style="zoom:50%;"></p>
<p>此外，可以证明化简后的对偶问题与最大熵模型的对数似然函数是一样的，感兴趣的读者可以自行证明一下。</p>
<h2 id="最大熵模型学习的优化算法">最大熵模型学习的优化算法</h2>
<p>现在，我们整理一下，我们已经得到的东西：<strong>最大熵模型、模型的对数似然函数</strong>。那么，接下来，我们只要对对数似然函数求导，得出参数<span class="math inline">\(w\)</span>的值，就可以得到最大熵模型<span class="math inline">\(P_w(y|x)\)</span>。由于最大化对数似然函数是一个凸优化问题，我们可以使用牛顿法、IIS等算法求解。在这里主要介绍IIS（改进的迭代程度法）。</p>
<p>IIS的思想是：<strong>如果我们能找到一种参数更新方法：<span class="math inline">\(r:w-&gt;w+\delta\)</span>，使得对数似然函数能够增大，那么，我们就可以重复使用改方法，从而得到最优解。</strong>我们可以首先计算似然函数的改变量：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/5.jpg" style="zoom:50%;"></p>
<p>根据不等式：<span class="math inline">\(-loga&gt;=1-a\)</span>，那么将上式进行放缩，得到如下式子：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/6.jpg" style="zoom:50%;"></p>
<p>我们记右边的式子为：<span class="math inline">\(A(\delta|w)\)</span>。这就是似然函数改变量的下界。也就是说，只要我们找到合适的<span class="math inline">\(\delta\)</span>，不断提高下界，就能够使得似然函数不断增大。但是IIS算法一次只能优化一个参数<span class="math inline">\(w_i\)</span>，所以我们需要再对下界进行放缩（这其中用到了Jensen不等式）。如下：</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/7.jpg" style="zoom:50%;"></p>
<p>记右边的式子为<span class="math inline">\(B(\delta|w)\)</span>，接下来我们只要对<span class="math inline">\(B(\delta|w\)</span>求导，就可以得到最后的结果了。<strong>总结如下：</strong></p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/8.jpg" style="zoom:50%;"></p>
<p>到此，最大熵模型的理论部分求讲解完了～</p>
<h2 id="最大熵模型与lr模型的区别与联系">最大熵模型与LR模型的区别与联系</h2>
<p>由于最大熵模型与LR模型总是被放在一起讲，所以这里就简单概括一下两者的区别与联系。最大熵模型与logistic回归模型最大的相似之处在于：<strong>最后都是求条件概率分布关于训练集的对数似然函数的最大化</strong>，区别在于：<strong>条件概率分布的形式不同</strong>，LR模型的条件概率分布就是sigmoid函数。具体关于LR模型的细节，我就不讲了，因为实在太简单了，我就放一张LR模型的对数似然函数的最大化的推导过程吧～</p>
<p><img src="/2020/02/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/9.jpeg" style="zoom:50%;"></p>
<h2 id="最大熵模型的实现">最大熵模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了python来实现最大熵模型与logistic回归模型。我的github里面可以下在到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="logistic回归模型python实现">logistic回归模型python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2020-1-2</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">准确率：0.9919</span></span><br><span class="line"><span class="string">时间：29.48268699645996</span></span><br><span class="line"><span class="string">--------------</span></span><br><span class="line"><span class="string">tips:在加载数据的时候，把&gt;=5为1，&lt;5为0这样处理数据的时候，在同样的训练次数与学习率的时候，最后的准确率只有78%左右。</span></span><br><span class="line"><span class="string">可能是数据类别太多，导致比较混乱，所以在这里，我采取的是标签为0的为1，不为0的全为0。</span></span><br><span class="line"><span class="string">这样准确率大大提高了。</span></span><br><span class="line"><span class="string">--------------</span></span><br><span class="line"><span class="string">注意，这里实现的LR模型采用的是神经网络的思想，就是将LR模型视为一个单层神经网络。按照前向传播与反向传播的方式来写的代码。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName:数据路径名</span></span><br><span class="line"><span class="string">    :return: 特征向量矩阵、还有标签矩阵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = [];</span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">if</span> (int(curline[<span class="number">0</span>]) ==<span class="number">0</span>):</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">0</span>)</span><br><span class="line">            data_list.append([int(feature) / <span class="number">255</span> <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    定义sigmoid函数</span></span><br><span class="line"><span class="string">    :param z: 输入</span></span><br><span class="line"><span class="string">    :return: 返回（0，1）的数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    result = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_params</span><span class="params">(feature_dim)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    初始化参数w,b</span></span><br><span class="line"><span class="string">    :param feature_dim:实例特征数目</span></span><br><span class="line"><span class="string">    :return: 参数w,b</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w = np.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagation</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    一次前向与反向传播过程</span></span><br><span class="line"><span class="string">    :param w:参数w</span></span><br><span class="line"><span class="string">    :param b: 参数b</span></span><br><span class="line"><span class="string">    :param X: 输入的特征向量</span></span><br><span class="line"><span class="string">    :param Y: 输入的类别向量</span></span><br><span class="line"><span class="string">    :return:dw,db,costs</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    N, _ = np.shape(X)  <span class="comment"># 训练集数目</span></span><br><span class="line">    X = X.T</span><br><span class="line">    <span class="comment"># print(X.shape)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line">    <span class="comment"># epsilon=1e-5</span></span><br><span class="line">    cost = <span class="number">-1</span> / N * (np.sum(Y * np.log(A) + (<span class="number">1</span> - Y) * np.log(<span class="number">1</span> - A)))</span><br><span class="line"></span><br><span class="line">    dz = A - Y</span><br><span class="line">    dw = <span class="number">1</span> / N * np.dot(X, dz.T)</span><br><span class="line">    db = <span class="number">1</span> / N * np.sum(dz)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.dtype == float)</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span> (cost.shape == ())</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw, <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimization</span><span class="params">(w, b, X, Y, iterations, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    优化，使用batch GD</span></span><br><span class="line"><span class="string">    :param w: 参数w</span></span><br><span class="line"><span class="string">    :param b: 参数b</span></span><br><span class="line"><span class="string">    :param X: 输入的特征向量</span></span><br><span class="line"><span class="string">    :param Y: 输入的类别向量</span></span><br><span class="line"><span class="string">    :param iterations: 迭代次数(其实就是epoch)</span></span><br><span class="line"><span class="string">    :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">    :return: 最优化的参数w,b,以及costs（costs可有可无，取决于你是否想看训练过程中的cost的变化）</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(iterations):</span><br><span class="line">        grads, cost = propagation(w, b, X, Y)</span><br><span class="line"></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line"></span><br><span class="line">        w = w - learning_rate * dw</span><br><span class="line">        b = b - learning_rate * db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每100次epoch打印一次信息</span></span><br><span class="line">        <span class="keyword">if</span> (iter % <span class="number">100</span> == <span class="number">0</span>):</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            print(<span class="string">f"the current iteration is <span class="subst">&#123;iter&#125;</span>,the current cost is <span class="subst">&#123;cost&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">        params = &#123;<span class="string">"w"</span>: w, <span class="string">"b"</span>: b&#125;</span><br><span class="line">        grads = &#123;<span class="string">"dw"</span>: dw, <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测新实例的类别</span></span><br><span class="line"><span class="string">    :param w:最优化的参数w</span></span><br><span class="line"><span class="string">    :param b:最优化的参数b</span></span><br><span class="line"><span class="string">    :param X:实例的特征向量</span></span><br><span class="line"><span class="string">    :return:实例的类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    N = X.shape[<span class="number">0</span>]</span><br><span class="line">    prediction = np.zeros((<span class="number">1</span>, N))</span><br><span class="line">    X = X.T</span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">        <span class="keyword">if</span> (A[<span class="number">0</span>][i] &lt;= <span class="number">0.5</span>):</span><br><span class="line">            prediction[<span class="number">0</span>][i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prediction[<span class="number">0</span>][i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (prediction.shape == (<span class="number">1</span>, N))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prediction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(train_data, train_label, test_data, test_label, iterations, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    将上述定义的函数结合起来，就是整个LR模型的执行过程</span></span><br><span class="line"><span class="string">    :param train_data: 训练数据集</span></span><br><span class="line"><span class="string">    :param train_label: 训练数据集的标签</span></span><br><span class="line"><span class="string">    :param test_data: 测试数据集</span></span><br><span class="line"><span class="string">    :param test_label: 测试数据集的标签</span></span><br><span class="line"><span class="string">    :param iterations: 迭代次数(epoch)</span></span><br><span class="line"><span class="string">    :param learning_rate: 学习率</span></span><br><span class="line"><span class="string">    :return: 在测试数据集上的准确率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    w, b = initialize_params(train_data.shape[<span class="number">1</span>])</span><br><span class="line">    params, grads, costs = optimization(w, b, train_data, train_label, iterations, learning_rate)</span><br><span class="line"></span><br><span class="line">    w = params[<span class="string">"w"</span>]</span><br><span class="line">    b = params[<span class="string">"b"</span>]</span><br><span class="line"></span><br><span class="line">    prediction = predict(w, b, test_data)</span><br><span class="line">    error = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(prediction.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> (prediction[<span class="number">0</span>][i] != test_label[<span class="number">0</span>][i]):</span><br><span class="line">            error += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    accuracy = (prediction.shape[<span class="number">1</span>] - error) / prediction.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"w"</span>: w, <span class="string">"b"</span>: b, <span class="string">"costs"</span>: costs&#125;</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start load data."</span>)</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../MnistData/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    d = model(train_data, train_label, test_data, test_label, iterations=<span class="number">200</span>, learning_rate=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="logistic回归模型sckit-learn实现">logistic回归模型sckit-learn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># Author:codewithzichao</span></span><br><span class="line"><span class="comment"># Date:2020-1-2</span></span><br><span class="line"><span class="comment"># E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">准确率：0.8707.</span></span><br><span class="line"><span class="string">时间：89.82440423965454.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName:数据路径名</span></span><br><span class="line"><span class="string">    :return: 特征向量矩阵、还有标签矩阵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = []</span><br><span class="line">    label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">if</span> (int(curline[<span class="number">0</span>]) &gt;= <span class="number">5</span>):</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">0</span>)</span><br><span class="line">            data_list.append([int(feature) / <span class="number">255</span> <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"start load data."</span>)</span><br><span class="line">    train_data, train_label = loadData(<span class="string">"../Mnistdata/mnist_train.csv"</span>)</span><br><span class="line">    test_data, test_label = loadData(<span class="string">"../MnistData/mnist_test.csv"</span>)</span><br><span class="line">    print(<span class="string">"finished load data."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 默认迭代次数为100,使用的算法是lbfgs，使用L2正则化。这里要加大迭代次数，要不然的化，不会收敛。</span></span><br><span class="line">    clf = linear_model.LogisticRegression(max_iter=<span class="number">1000</span>)</span><br><span class="line">    clf.fit(train_data, train_label)</span><br><span class="line"></span><br><span class="line">    accuracy = clf.score(test_data, test_label)</span><br><span class="line">    print(<span class="string">f"the  accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">f"the total time is <span class="subst">&#123;end - start&#125;</span>."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="最大熵模型的实现-1">最大熵模型的实现</h3>
<p><strong>此实现借用于pkudodo的实现，因为实在是写的太好了！</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#Author:codewithzichao</span></span><br><span class="line"><span class="comment">#Date:2020-1-2</span></span><br><span class="line"><span class="comment">#E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000(实际使用:20000)</span></span><br><span class="line"><span class="string">测试集数量：10000</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    加载数据</span></span><br><span class="line"><span class="string">    :param fileName:数据路径名</span></span><br><span class="line"><span class="string">    :return: 特征向量矩阵、还有标签矩阵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = [];label_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(fileName, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            curline = line.strip().split(<span class="string">","</span>)</span><br><span class="line">            <span class="keyword">if</span> (int(curline[<span class="number">0</span>]) &gt;= <span class="number">5</span>):</span><br><span class="line">                label_list.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                label_list.append(<span class="number">0</span>)</span><br><span class="line">            data_list.append([int(int(feature)&gt;<span class="number">128</span>)  <span class="keyword">for</span> feature <span class="keyword">in</span> curline[<span class="number">1</span>:]])</span><br><span class="line"></span><br><span class="line">    data_matrix = np.array(data_list)</span><br><span class="line">    label_matrix = np.array(label_list)</span><br><span class="line">    <span class="keyword">return</span> data_matrix, label_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">maxEnt</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    最大熵类</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, trainDataList, trainLabelList, testDataList, testLabelList)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        各参数初始化</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.trainDataList = trainDataList          <span class="comment">#训练数据集</span></span><br><span class="line">        self.trainLabelList = trainLabelList        <span class="comment">#训练标签集</span></span><br><span class="line">        self.testDataList = testDataList            <span class="comment">#测试数据集</span></span><br><span class="line">        self.testLabelList = testLabelList          <span class="comment">#测试标签集</span></span><br><span class="line">        self.featureNum = len(trainDataList[<span class="number">0</span>])     <span class="comment">#特征数量</span></span><br><span class="line"></span><br><span class="line">        self.N = len(trainDataList)                 <span class="comment">#总训练集长度</span></span><br><span class="line">        self.n = <span class="number">0</span>                                  <span class="comment">#训练集中（xi，y）对数量</span></span><br><span class="line">        self.M = <span class="number">10000</span>                              <span class="comment">#</span></span><br><span class="line">        self.fixy = self.calc_fixy()                <span class="comment">#所有(x, y)对出现的次数</span></span><br><span class="line">        self.w = [<span class="number">0</span>] * self.n                       <span class="comment">#Pw(y|x)中的w</span></span><br><span class="line">        self.xy2idDict, self.id2xyDict = self.createSearchDict()        <span class="comment">#(x, y)-&gt;id和id-&gt;(x, y)的搜索字典</span></span><br><span class="line">        self.Ep_xy = self.calcEp_xy()               <span class="comment">#Ep_xy期望值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calcEpxy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算特征函数f(x, y)关于模型P(Y|X)与经验分布P_(X, Y)的期望值（P后带下划线“_”表示P上方的横线</span></span><br><span class="line"><span class="string">        程序中部分下划线表示“|”，部分表示上方横线，请根据具体公式自行判断,）</span></span><br><span class="line"><span class="string">        即“6.2.2 最大熵模型的定义”中第二个期望（83页最上方的期望）</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        Epxy = [<span class="number">0</span>] * self.n</span><br><span class="line">        <span class="comment">#对于每一个样本进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.N):</span><br><span class="line">            <span class="comment">#初始化公式中的P(y|x)列表</span></span><br><span class="line">            Pwxy = [<span class="number">0</span>] * <span class="number">2</span></span><br><span class="line">            <span class="comment">#计算P(y = 0 &#125; X)</span></span><br><span class="line">            <span class="comment">#注：程序中X表示是一个样本的全部特征，x表示单个特征，这里是全部特征的一个样本</span></span><br><span class="line">            Pwxy[<span class="number">0</span>] = self.calcPwy_x(self.trainDataList[i], <span class="number">0</span>)</span><br><span class="line">            <span class="comment">#计算P(y = 1 &#125; X)</span></span><br><span class="line">            Pwxy[<span class="number">1</span>] = self.calcPwy_x(self.trainDataList[i], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> range(self.featureNum):</span><br><span class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">                    <span class="keyword">if</span> (self.trainDataList[i][feature], y) <span class="keyword">in</span> self.fixy[feature]:</span><br><span class="line">                        id = self.xy2idDict[feature][(self.trainDataList[i][feature], y)]</span><br><span class="line">                        Epxy[id] += (<span class="number">1</span> / self.N) * Pwxy[y]</span><br><span class="line">        <span class="keyword">return</span> Epxy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calcEp_xy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算特征函数f(x, y)关于经验分布P_(x, y)的期望值（下划线表示P上方的横线，</span></span><br><span class="line"><span class="string">        同理Ep_xy中的“_”也表示p上方的横线）</span></span><br><span class="line"><span class="string">        即“6.2.2 最大熵的定义”中第一个期望（82页最下方那个式子）</span></span><br><span class="line"><span class="string">        :return: 计算得到的Ep_xy</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#初始化Ep_xy列表，长度为n</span></span><br><span class="line">        Ep_xy = [<span class="number">0</span>] * self.n</span><br><span class="line"></span><br><span class="line">        <span class="comment">#遍历每一个特征</span></span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> range(self.featureNum):</span><br><span class="line">            <span class="comment">#遍历每个特征中的(x, y)对</span></span><br><span class="line">            <span class="keyword">for</span> (x, y) <span class="keyword">in</span> self.fixy[feature]:</span><br><span class="line">                <span class="comment">#获得其id</span></span><br><span class="line">                id = self.xy2idDict[feature][(x, y)]</span><br><span class="line">                <span class="comment">#将计算得到的Ep_xy写入对应的位置中</span></span><br><span class="line">                <span class="comment">#fixy中存放所有对在训练集中出现过的次数，处于训练集总长度N就是概率了</span></span><br><span class="line">                Ep_xy[id] = self.fixy[feature][(x, y)] / self.N</span><br><span class="line"></span><br><span class="line">        <span class="comment">#返回期望</span></span><br><span class="line">        <span class="keyword">return</span> Ep_xy</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createSearchDict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        创建查询字典</span></span><br><span class="line"><span class="string">        xy2idDict：通过(x,y)对找到其id,所有出现过的xy对都有一个id</span></span><br><span class="line"><span class="string">        id2xyDict：通过id找到对应的(x,y)对</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#设置xy搜多id字典</span></span><br><span class="line">        <span class="comment">#这里的x指的是单个的特征，而不是某个样本，因此将特征存入字典时也需要存入这是第几个特征</span></span><br><span class="line">        <span class="comment">#这一信息，这是为了后续的方便，否则会乱套。</span></span><br><span class="line">        <span class="comment">#比如说一个样本X = (0, 1, 1) label =(1)</span></span><br><span class="line">        <span class="comment">#生成的标签对有(0, 1), (1, 1), (1, 1)，三个(x，y)对并不能判断属于哪个特征的，后续就没法往下写</span></span><br><span class="line">        <span class="comment">#不可能通过(1, 1)就能找到对应的id，因为对于(1, 1),字典中有多重映射</span></span><br><span class="line">        <span class="comment">#所以在生成字典的时总共生成了特征数个字典，例如在mnist中样本有784维特征，所以生成784个字典，属于</span></span><br><span class="line">        <span class="comment">#不同特征的xy存入不同特征内的字典中，使其不会混淆</span></span><br><span class="line">        xy2idDict = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.featureNum)]</span><br><span class="line">        <span class="comment">#初始化id到xy对的字典。因为id与(x，y)的指向是唯一的，所以可以使用一个字典</span></span><br><span class="line">        id2xyDict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#设置缩影，其实就是最后的id</span></span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="comment">#对特征进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> range(self.featureNum):</span><br><span class="line">            <span class="comment">#对出现过的每一个(x, y)对进行遍历</span></span><br><span class="line">            <span class="comment">#fixy：内部存放特征数目个字典，对于遍历的每一个特征，单独读取对应字典内的(x, y)对</span></span><br><span class="line">            <span class="keyword">for</span> (x, y) <span class="keyword">in</span> self.fixy[feature]:</span><br><span class="line">                <span class="comment">#将该(x, y)对存入字典中，要注意存入时通过[feature]指定了存入哪个特征内部的字典</span></span><br><span class="line">                <span class="comment">#同时将index作为该对的id号</span></span><br><span class="line">                xy2idDict[feature][(x, y)] = index</span><br><span class="line">                <span class="comment">#同时在id-&gt;xy字典中写入id号，val为(x, y)对</span></span><br><span class="line">                id2xyDict[index] = (x, y)</span><br><span class="line">                <span class="comment">#id加一</span></span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#返回创建的两个字典</span></span><br><span class="line">        <span class="keyword">return</span> xy2idDict, id2xyDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_fixy</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算(x, y)在训练集中出现过的次数</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#建立特征数目个字典，属于不同特征的(x, y)对存入不同的字典中，保证不被混淆</span></span><br><span class="line">        fixyDict = [defaultdict(int) <span class="keyword">for</span> i <span class="keyword">in</span> range(self.featureNum)]</span><br><span class="line">        <span class="comment">#遍历训练集中所有样本</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.trainDataList)):</span><br><span class="line">            <span class="comment">#遍历样本中所有特征</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.featureNum):</span><br><span class="line">                <span class="comment">#将出现过的(x, y)对放入字典中并计数值加1</span></span><br><span class="line">                fixyDict[j][(self.trainDataList[i][j], self.trainLabelList[i])] += <span class="number">1</span></span><br><span class="line">        <span class="comment">#对整个大字典进行计数，判断去重后还有多少(x, y)对，写入n</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> fixyDict:</span><br><span class="line">            self.n += len(i)</span><br><span class="line">        <span class="comment">#返回大字典</span></span><br><span class="line">        <span class="keyword">return</span> fixyDict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calcPwy_x</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        计算“6.23 最大熵模型的学习” 式6.22</span></span><br><span class="line"><span class="string">        :param X: 要计算的样本X（一个包含全部特征的样本）</span></span><br><span class="line"><span class="string">        :param y: 该样本的标签</span></span><br><span class="line"><span class="string">        :return: 计算得到的Pw(Y|X)</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#分子</span></span><br><span class="line">        numerator = <span class="number">0</span></span><br><span class="line">        <span class="comment">#分母</span></span><br><span class="line">        Z = <span class="number">0</span></span><br><span class="line">        <span class="comment">#对每个特征进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.featureNum):</span><br><span class="line">            <span class="comment">#如果该(xi,y)对在训练集中出现过</span></span><br><span class="line">            <span class="keyword">if</span> (X[i], y) <span class="keyword">in</span> self.xy2idDict[i]:</span><br><span class="line">                <span class="comment">#在xy-&gt;id字典中指定当前特征i，以及(x, y)对：(X[i], y)，读取其id</span></span><br><span class="line">                index = self.xy2idDict[i][(X[i], y)]</span><br><span class="line">                <span class="comment">#分子是wi和fi(x，y)的连乘再求和，最后指数</span></span><br><span class="line">                <span class="comment">#由于当(x, y)存在时fi(x，y)为1，因为xy对肯定存在，所以直接就是1</span></span><br><span class="line">                <span class="comment">#对于分子来说，就是n个wi累加，最后再指数就可以了</span></span><br><span class="line">                <span class="comment">#因为有n个w，所以通过id将w与xy绑定，前文的两个搜索字典中的id就是用在这里</span></span><br><span class="line">                numerator += self.w[index]</span><br><span class="line">            <span class="comment">#同时计算其他一种标签y时候的分子，下面的z并不是全部的分母，再加上上式的分子以后</span></span><br><span class="line">            <span class="comment">#才是完整的分母，即z = z + numerator</span></span><br><span class="line">            <span class="keyword">if</span> (X[i], <span class="number">1</span>-y) <span class="keyword">in</span> self.xy2idDict[i]:</span><br><span class="line">                <span class="comment">#原理与上式相同</span></span><br><span class="line">                index = self.xy2idDict[i][(X[i], <span class="number">1</span>-y)]</span><br><span class="line">                Z += self.w[index]</span><br><span class="line">        <span class="comment">#计算分子的指数</span></span><br><span class="line">        numerator = np.exp(numerator)</span><br><span class="line">        <span class="comment">#计算分母的z</span></span><br><span class="line">        Z = np.exp(Z) + numerator</span><br><span class="line">        <span class="comment">#返回Pw(y|x)</span></span><br><span class="line">        <span class="keyword">return</span> numerator / Z</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxEntropyTrain</span><span class="params">(self, iter = <span class="number">500</span>)</span>:</span></span><br><span class="line">        <span class="comment">#设置迭代次数寻找最优解</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(iter):</span><br><span class="line">            <span class="comment">#单次迭代起始时间点</span></span><br><span class="line">            iterStart = time.time()</span><br><span class="line"></span><br><span class="line">            <span class="comment">#计算“6.2.3 最大熵模型的学习”中的第二个期望（83页最上方哪个）</span></span><br><span class="line">            Epxy = self.calcEpxy()</span><br><span class="line"></span><br><span class="line">            <span class="comment">#使用的是IIS，所以设置sigma列表</span></span><br><span class="line">            sigmaList = [<span class="number">0</span>] * self.n</span><br><span class="line">            <span class="comment">#对于所有的n进行一次遍历</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(self.n):</span><br><span class="line">                <span class="comment">#依据“6.3.1 改进的迭代尺度法” 式6.34计算</span></span><br><span class="line">                sigmaList[j] = (<span class="number">1</span> / self.M) * np.log(self.Ep_xy[j] / Epxy[j])</span><br><span class="line"></span><br><span class="line">            <span class="comment">#按照算法6.1步骤二中的（b）更新w</span></span><br><span class="line">            self.w = [self.w[i] + sigmaList[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n)]</span><br><span class="line"></span><br><span class="line">            <span class="comment">#单次迭代结束</span></span><br><span class="line">            iterEnd = time.time()</span><br><span class="line">            <span class="comment">#打印运行时长信息</span></span><br><span class="line">            print(<span class="string">'iter:%d:%d, time:%d'</span>%(i, iter, iterStart - iterEnd))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        预测标签</span></span><br><span class="line"><span class="string">        :param X:要预测的样本</span></span><br><span class="line"><span class="string">        :return: 预测值</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#因为y只有0和1，所有建立两个长度的概率列表</span></span><br><span class="line">        result = [<span class="number">0</span>] * <span class="number">2</span></span><br><span class="line">        <span class="comment">#循环计算两个概率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            <span class="comment">#计算样本x的标签为i的概率</span></span><br><span class="line">            result[i] = self.calcPwy_x(X, i)</span><br><span class="line">        <span class="comment">#返回标签</span></span><br><span class="line">        <span class="comment">#max(result)：找到result中最大的那个概率值</span></span><br><span class="line">        <span class="comment">#result.index(max(result))：通过最大的那个概率值再找到其索引，索引是0就返回0，1就返回1</span></span><br><span class="line">        <span class="keyword">return</span> result.index(max(result))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        对测试集进行测试</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment">#错误值计数</span></span><br><span class="line">        errorCnt = <span class="number">0</span></span><br><span class="line">        <span class="comment">#对测试集中所有样本进行遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.testDataList)):</span><br><span class="line">            <span class="comment">#预测该样本对应的标签</span></span><br><span class="line">            result = self.predict(self.testDataList[i])</span><br><span class="line">            <span class="comment">#如果错误，计数值加1</span></span><br><span class="line">            <span class="keyword">if</span> result != self.testLabelList[i]:   errorCnt += <span class="number">1</span></span><br><span class="line">        <span class="comment">#返回准确率</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - errorCnt / len(self.testDataList)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取训练集及标签</span></span><br><span class="line">    print(<span class="string">'start read transSet'</span>)</span><br><span class="line">    trainData, trainLabel = loadData(<span class="string">'../Mnist/mnist_train.csv'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取测试集及标签</span></span><br><span class="line">    print(<span class="string">'start read testSet'</span>)</span><br><span class="line">    testData, testLabel = loadData(<span class="string">'../Mnist/mnist_test.csv'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化最大熵类</span></span><br><span class="line">    maxEnt = maxEnt(trainData[:<span class="number">20000</span>], trainLabel[:<span class="number">20000</span>], testData, testLabel)</span><br><span class="line"></span><br><span class="line">    maxEnt.maxEntropyTrain()</span><br><span class="line"></span><br><span class="line">    accuracy = maxEnt.test()</span><br><span class="line">    print(<span class="string">f"the accuracy is <span class="subst">&#123;accuracy&#125;</span>."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印时间</span></span><br><span class="line">    print(<span class="string">'time span:'</span>, time.time() - start)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Logistic Regression</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>MaxEntropy</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法|条件随机场模型原理详解与实现</title>
    <url>/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>条件随机场(CRF)，属于判别模型，它在自然语言处理领域用途非常广泛。由于其涉及较多的概率图模型的知识，所以，这篇博客将首先介绍一下关于CRF模型的提出的原因，接着再介绍CRF模型需要解决的问题，并针对每个问题介绍其解决的算法，最后将使用python与sklearn库来对其进行实现。</p>
<a id="more"></a>
<h2 id="条件随机场模型提出的原因">条件随机场模型提出的原因</h2>
<p>如果用一句话来总结CRF被提出的原因就是：<strong>CRF解决了MEMM模型的标注偏差问题，打破了齐次markov假设，使得模型更加合理。</strong>（如果不想看分析过程的话，可以直接跳过，看第二部分CRF模型的介绍。）具体分析过程，如下：</p>
<h3 id="分类模型">分类模型</h3>
<p>首先，我们可以大致看一下分类模型。整个分类模型可以被分为：<strong>硬分类与软分类</strong>。硬分类的代表模型有：<strong>感知机(PLA)、支持向量机(SVM)、线性判别分析(LDA)</strong>；软分类模型又可分为两类：判别模型与生成模型。判别模型的代表：<strong>逻辑回归(LR)模型以及最大熵(MaxEnt)模型</strong>；生成模型的代表：<strong>朴素贝叶斯(Naive Bayes)模型、隐马尔可夫(HMM)模型</strong>。之后，我们根据MaxEnt模型与HMM模型，可以得出一个<strong>判别模型——最大熵马尔可夫模型(MEMM)</strong>，最后在MEMM模型的基础上，得出了<strong>条件随机场(CRF)模型</strong>。</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/0.jpg"></p>
<p>所以，CRF模型是从HMM到MEMM模型，一步步推出来的。那么，接下来，我们来具体看看是怎么得来的。</p>
<h3 id="hmm-vs-memm">HMM vs MEMM</h3>
<p>HMM模型，相信大家都很熟悉了，它可被用来解决词性标注等问题。HMM模型有两个很著名的假设：<strong>齐次一阶Markov假设与观测独立性假设</strong>。但是这两个假设都是合理的吗？其实不是的，如果对概率图模型的基本知识有些了解的话，我们会发现这两个假设其实都是为了简化运算。譬如说，我们来看<strong>观测独立性假设</strong>。它被描述为观测变量 <span class="math inline">\(o_t\)</span> 只与 <span class="math inline">\(i_t\)</span> 有关系，其实这是不合理的。以词性标注问题为例，单词与单词之间其实都是有关系的。那么，为了解决这个问题，MEMM模型便被提出来了，具体做法是：<strong>将原来的观测变量变为输入(从图的角度来说，就是剪箭头反向，具体为什么这样做就打破了观测独立性假设呢？其实和有向图的D-seperation有关，感兴趣的可以去了解了解这个)</strong>。MEMM的优点有：<strong>1.打破了观测独立性假设，使得模型更加合理；2.MEMM模型是判别模型，相比HMM模型来说，计算更为简单</strong>。但是MEMM模型也存在一个非常严重的缺陷：<strong>标注偏差问题(label bias problem)</strong>。这个问题会导致词性标注准确率大为降低。这也是MEMM模型没有流行的原因。</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/1.jpg"></p>
<h3 id="memm-vs-crf">MEMM vs CRF</h3>
<p>MEMM模型存在标注偏差问题，那么到底什么是标注偏差问题呢？(如果想深入了解的话，可以去看John Lafferty的那篇论文)。 如下：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/2.png"></p>
<p>图中，每一列表示一个时刻，也就代表一个状态。当我们使用Viterbi算法去求具有最大概率的路径的时候，我们会得到1-&gt;2-&gt;2-&gt;2，概率是0.6*0.3*0.3=0.054，然而实际上最大概率的路径是:1-&gt;1-&gt;1-&gt;1，其概率是：0.4*0.45*0.5=0.09。那么，如果在词性标注问题中，就会导致标注发生错误。这就是标注偏差问题。那么导致这一问题的根本原因在于：<strong>局部归一化</strong>。也就是每一次从t时刻转移到t+1时刻的时候，都会对转移概率做归一化。从而，就会<strong>导致模型偏向于状态转移少的标签，从而使得后验概率最大</strong>，导致模型越不会去理睬观测变量的值，就会进行词性标注。</p>
<p>为了解决这一问题，就提出了CRF模型。CRF模型的具体做法是：<strong>将模型变成无向图，这样一来，就不需要在每一步都进行归一化，只需要进行全局归一化即可</strong>。这样，就解决了标注偏差问题。下面就讲具体介绍CRF模型。</p>
<h2 id="条件随机场模型介绍">条件随机场模型介绍</h2>
<p>在介绍CRF之前，首先得先去了解有向图模型(贝叶斯网络)的因子分解与D-seperation，以及无向图(markov随机场)的D-separation(严格来讲，无向图没有D-seperation，但是它的结构和有向图的D-separation差不多，其实就是全局markov性)与因子分解。实际上，所谓的因子分解其实都是为了体现概率图模型的条件独立性。在无向图中，条件独立性表现在三个方面：全局markov性、局部markov性、成对markov性。那么因子分解为了表现其条件独立性，使用了基于团的方法，并有hammesley-Clifford定理得以保证。在这里，我就不详细讲了，如果讲的话，那就太多太多啦～感兴趣的同学可以去补补概率图的知识，就按照我上面讲的这些即可～</p>
<p>OK，直接进入主题。</p>
<p>CRF模型是给定随机变量<span class="math inline">\(X\)</span>的条件下，随机变量<span class="math inline">\(Y\)</span>的markov随机场。其概率图如下：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/3.jpg"></p>
<p>由于CRF模型是判别式模型，所以建模对象就是：<span class="math inline">\(P(Y|X)\)</span>。也就是说，我们要求的是<span class="math inline">\(P(Y|X)\)</span>。下面我们就来推导一下线性链CRF的参数形式。如下：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/4.jpg"></p>
<p>所以，我们现在得到了CRF的参数形式，当随机变量<span class="math inline">\(X\)</span>取<span class="math inline">\(x\)</span>，随机变量<span class="math inline">\(Y\)</span>取<span class="math inline">\(y\)</span>的时候，如下： <span class="math display">\[
P(y|x)=\frac {1}{z}exp\sum_{t=1}^{T}[\sum_{k=1}^{K}\lambda_kf_k(y_{t-1},y_t,x_{1:T})+\sum_{l=1}^{L}\eta_lg_l(y_t,x_{1:T})]
\]</span> 其中，<span class="math inline">\(\lambda、\eta\)</span> 都是需要训练的参数，<span class="math inline">\(f、g\)</span> 是特征函数，当满足某一条件的时候，为1，否则为0。在这里，需要注意的是，我所使用的符号与李航老师的《统计学习方法》不太一样，但是表达都是一样的，《统计学习方法》中将最外面的那个<span class="math inline">\(\sum\)</span>放进去了，所以特征函数的参数就会多一个。其实都是一样的，<strong>我觉得最重要的是自己能够理解，并且方便自己记忆和推导就可以了</strong>：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/6.jpg" style="zoom:50%;"></p>
<p>那么，我们使用将其化简为向量化的形式，如下：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/5.jpg"></p>
<p>所以，我们就得到了CRF的向量化形式： <span class="math display">\[
P(y|x)=\frac {1}{z(x,\theta)}exp[\theta^T(H(y_{t-1},y_t,x)]
\]</span> 那么，得到这个之后，我们就来解决CRF模型中的问题。CRF总共需要解决三个问题：</p>
<ul>
<li>求边缘概率。即：求<span class="math inline">\(P(y_t|x)\)</span>。</li>
<li>学习问题。即：给定训练集<span class="math inline">\(\{x^{(i)},y^{(i)}\}_1^N\)</span>，求：<span class="math inline">\(\theta=\mathop{argmax}\limits_{\theta}\prod_{i=1}^{N}P(y^{(i)}|x^{(i)})\)</span>。N个样本，其中x都是T维的。</li>
<li>Decoding问题。即：求：<span class="math inline">\(y=\mathop{argmax}\limits_{y}P(y|x)\)</span>。</li>
</ul>
<h2 id="边缘概率问题的求解">边缘概率问题的求解</h2>
<p>在这里，其实计算过程与HMM中的概率计算其实是一样的，同样引进<strong>前向后向算法</strong>，来减小计算复杂度。具体推导过程如下（推导过程采用了<strong>变量消除法</strong>）：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/7.jpg"></p>
<p>我们可以看到，原来的计算复杂度为：<span class="math inline">\(O(T|S|^T)\)</span>。当<span class="math inline">\(T\)</span>很大的时候，复杂度是以指数形式增长的，这是不可解的。当我们已经引进前向向量与后向向量，那么就可解了。</p>
<h2 id="模型参数的求解">模型参数的求解</h2>
<p>关于CRF模型参数的求解方法，其实有很多。比如有：改进的迭代尺度法、拟牛顿法、牛顿法等等。在这里，我采用的是梯度上升法，因为是求最大，当然也可以将其转换为求最小问题，使用梯度下降法来解决。其实过程差不多的。具体推导过程如下(原谅我手机的渣渣像素，以后有时间再用markdown把公式打出来吧😩)：</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/9.jpg"></p>
<h2 id="decoding问题的求解">Decoding问题的求解</h2>
<p>每次写博客，都感觉好累🥱。CRF的decoding问题，仍然是使用Viterbi算法进行求解，在这里，我就不推导了，直接放《统计学习方法》的截图吧，如果你之前看过我关于HMM模型的讲解那一篇的话，那么CRF的Viterbi算法也绝对能看得懂～</p>
<p><img src="/2020/02/24/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E7%8E%B0/8.jpg"></p>
<p>至此，关于CRF的理论部分就讲完了～</p>
<h2 id="crf模型的实现">CRF模型的实现</h2>
<p>把模型实现一遍才算是真正的吃透了这个模型呀。在这里，我采取了两种方式来实现CRF模型：python实现以及调用scikit-learn库来实现。我的github里面可以下在到所有的代码，欢迎访问我的github，也欢迎大家star和fork。附上<strong>GitHub地址: <a href="https://github.com/codewithzichao/Machine_Learning_Code" target="_blank" rel="noopener">《统计学习方法》及常规机器学习模型实现</a></strong>。具体代码如下：</p>
<h3 id="python实现">python实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#Author:codewithzichao</span></span><br><span class="line"><span class="comment">#E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">由于CRF模型，一般都是来解决其Decoding问题，所以在此，我只实现Viterbi算法～</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''实现条件随机场预测问题的维特比算法</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, V, VW, E, EW)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        :param V:是定义在节点上的特征函数，称为状态特征</span></span><br><span class="line"><span class="string">        :param VW:是V对应的权值</span></span><br><span class="line"><span class="string">        :param E:是定义在边上的特征函数，称为转移特征</span></span><br><span class="line"><span class="string">        :param EW:是E对应的权值</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.V = V  <span class="comment"># 点分布表</span></span><br><span class="line">        self.VW = VW  <span class="comment"># 点权值表</span></span><br><span class="line">        self.E = E  <span class="comment"># 边分布表</span></span><br><span class="line">        self.EW = EW  <span class="comment"># 边权值表</span></span><br><span class="line">        self.D = []  <span class="comment"># Delta表，最大非规范化概率的局部状态路径概率</span></span><br><span class="line">        self.P = []  <span class="comment"># Psi表，当前状态和最优前导状态的索引表s</span></span><br><span class="line">        self.BP = []  <span class="comment"># BestPath，最优路径</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Viterbi</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        条件随机场预测问题的维特比算法，此算法一定要结合CRF参数化形式对应的状态路径图来理解，更容易理解.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.D = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        self.P = np.full(shape=(np.shape(self.V)), fill_value=<span class="number">.0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">0</span>]):</span><br><span class="line">            <span class="comment"># 初始化</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> == i:</span><br><span class="line">                self.D[i] = np.multiply(self.V[i], self.VW[i])</span><br><span class="line">                self.P[i] = np.array([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">                print(<span class="string">'self.V[%d]='</span> % i, self.V[i], <span class="string">'self.VW[%d]='</span> % i, self.VW[i], <span class="string">'self.D[%d]='</span> % i, self.D[i])</span><br><span class="line">                print(<span class="string">'self.P:'</span>, self.P)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="comment"># 递推求解布局最优状态路径</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> y <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]):  <span class="comment"># delta[i][y=1,2...]</span></span><br><span class="line">                    <span class="keyword">for</span> l <span class="keyword">in</span> range(np.shape(self.V)[<span class="number">1</span>]):  <span class="comment"># V[i-1][l=1,2...]</span></span><br><span class="line">                        delta = <span class="number">0.0</span></span><br><span class="line">                        delta += self.D[i - <span class="number">1</span>, l]  <span class="comment"># 前导状态的最优状态路径的概率</span></span><br><span class="line">                        delta += self.E[i - <span class="number">1</span>][l, y] * self.EW[i - <span class="number">1</span>][l, y]  <span class="comment"># 前导状态到当前状体的转移概率</span></span><br><span class="line">                        delta += self.V[i, y] * self.VW[i, y]  <span class="comment"># 当前状态的概率</span></span><br><span class="line">                        print(<span class="string">'(x%d,y=%d)--&gt;(x%d,y=%d):%.2f + %.2f + %.2f='</span> % (i - <span class="number">1</span>, l, i, y, \</span><br><span class="line">                                                                               self.D[i - <span class="number">1</span>, l], \</span><br><span class="line">                                                                               self.E[i - <span class="number">1</span>][l, y] * self.EW[i - <span class="number">1</span>][</span><br><span class="line">                                                                                   l, y], \</span><br><span class="line">                                                                               self.V[i, y] * self.VW[i, y]), delta)</span><br><span class="line">                        <span class="keyword">if</span> <span class="number">0</span> == l <span class="keyword">or</span> delta &gt; self.D[i, y]:</span><br><span class="line">                            self.D[i, y] = delta</span><br><span class="line">                            self.P[i, y] = l</span><br><span class="line">                    print(<span class="string">'self.D[x%d,y=%d]=%.2f\n'</span> % (i, y, self.D[i, y]))</span><br><span class="line">        print(<span class="string">'self.Delta:\n'</span>, self.D)</span><br><span class="line">        print(<span class="string">'self.Psi:\n'</span>, self.P)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回，得到所有的最优前导状态</span></span><br><span class="line">        N = np.shape(self.V)[<span class="number">0</span>]</span><br><span class="line">        self.BP = np.full(shape=(N,), fill_value=<span class="number">0.0</span>)</span><br><span class="line">        t_range = <span class="number">-1</span> * np.array(sorted(<span class="number">-1</span> * np.arange(N)))</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> t_range:</span><br><span class="line">            <span class="keyword">if</span> N - <span class="number">1</span> == t:  <span class="comment"># 得到最优状态</span></span><br><span class="line">                self.BP[t] = np.argmax(self.D[<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 得到最优前导状态</span></span><br><span class="line">                self.BP[t] = self.P[t + <span class="number">1</span>, int(self.BP[t + <span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最优状态路径表现在存储的是状态的下标，我们执行存储值+1转换成示例中的状态值</span></span><br><span class="line">        <span class="comment"># 也可以不用转换，只要你能理解，self.BP中存储的0是状态1就可以~~~~</span></span><br><span class="line">        self.BP += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">'最优状态路径为：'</span>, self.BP)</span><br><span class="line">        <span class="keyword">return</span> self.BP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    S = np.array([[<span class="number">1</span>, <span class="number">1</span>],  <span class="comment"># X1:S(Y1=1), S(Y1=2)</span></span><br><span class="line">                  [<span class="number">1</span>, <span class="number">1</span>],  <span class="comment"># X2:S(Y2=1), S(Y2=2)</span></span><br><span class="line">                  [<span class="number">1</span>, <span class="number">1</span>]])  <span class="comment"># X3:S(Y3=1), S(Y3=1)</span></span><br><span class="line">    SW = np.array([[<span class="number">1.0</span>, <span class="number">0.5</span>],  <span class="comment"># X1:SW(Y1=1), SW(Y1=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>],  <span class="comment"># X2:SW(Y2=1), SW(Y2=2)</span></span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.5</span>]])  <span class="comment"># X3:SW(Y3=1), SW(Y3=1)</span></span><br><span class="line">    E = np.array([[[<span class="number">1</span>, <span class="number">1</span>],  <span class="comment"># Edge:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">0</span>]],  <span class="comment"># Edge:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                  [[<span class="number">0</span>, <span class="number">1</span>],  <span class="comment"># Edge:Y2=1---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">                   [<span class="number">1</span>, <span class="number">1</span>]]])  <span class="comment"># Edge:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">    EW = np.array([[[<span class="number">0.6</span>, <span class="number">1</span>],  <span class="comment"># EdgeW:Y1=1---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                    [<span class="number">1</span>, <span class="number">0.0</span>]],  <span class="comment"># EdgeW:Y1=2---&gt;(Y2=1, Y2=2)</span></span><br><span class="line">                   [[<span class="number">0.0</span>, <span class="number">1</span>],  <span class="comment"># EdgeW:Y2=1---&gt;(Y3=1, Y3=2)</span></span><br><span class="line">                    [<span class="number">1</span>, <span class="number">0.2</span>]]])  <span class="comment"># EdgeW:Y2=2---&gt;(Y3=1, Y3=2)</span></span><br><span class="line"></span><br><span class="line">    crf = CRF(S, SW, E, EW)</span><br><span class="line">    ret = crf.Viterbi()</span><br><span class="line">    print(<span class="string">'最优状态路径为:'</span>, ret)</span><br></pre></td></tr></table></figure>
<h3 id="sklearn实现">sklearn实现</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line"><span class="comment">#Author:codewithzichao</span></span><br><span class="line"><span class="comment">#E-mail:lizichao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">训练数据集：1998人民日报标注语料库</span></span><br><span class="line"><span class="string">目标：对其进行命名实体识别</span></span><br><span class="line"><span class="string">结果：</span></span><br><span class="line"><span class="string">数据：新华社北京十二月三十一日电(中央人民广播电台记者刘振英、新华社记者张宿堂)今天是一九九七年的最后一天。</span></span><br><span class="line"><span class="string">结果：新华社 北京 十二月三十一日  中央人民广播电台  刘振英  新华社  张宿堂  今天  一九九七年</span></span><br><span class="line"><span class="string">----------------------------------------------------------</span></span><br><span class="line"><span class="string">数据：中国，我爱你。</span></span><br><span class="line"><span class="string">结果：中国</span></span><br><span class="line"><span class="string">来源：https://www.jianshu.com/p/7fa260e91382</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sklearn_crfsuite</span><br><span class="line"><span class="keyword">from</span> sklearn_crfsuite <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CorpusProcess</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化"""</span></span><br><span class="line">        self.train_corpus_path =<span class="string">"1980_01.txt"</span></span><br><span class="line">        self.process_corpus_path =<span class="string">"result-rmrb.txt"</span></span><br><span class="line">        self._maps = &#123;<span class="string">u't'</span>: <span class="string">u'T'</span>, <span class="string">u'nr'</span>: <span class="string">u'PER'</span>, <span class="string">u'ns'</span>: <span class="string">u'ORG'</span>, <span class="string">u'nt'</span>: <span class="string">u'LOC'</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read_corpus_from_file</span><span class="params">(self, file_path)</span>:</span></span><br><span class="line">        <span class="string">"""读取语料"""</span></span><br><span class="line">        f = open(file_path, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> lines</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">write_corpus_to_file</span><span class="params">(self, data, file_path)</span>:</span></span><br><span class="line">        <span class="string">"""写语料"""</span></span><br><span class="line">        f = open(file_path, <span class="string">'wb'</span>)</span><br><span class="line">        f.write(data)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">q_to_b</span><span class="params">(self, q_str)</span>:</span></span><br><span class="line">        <span class="string">"""全角转半角"""</span></span><br><span class="line">        b_str = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> uchar <span class="keyword">in</span> q_str:</span><br><span class="line">            inside_code = ord(uchar)</span><br><span class="line">            <span class="keyword">if</span> inside_code == <span class="number">12288</span>:  <span class="comment"># 全角空格直接转换</span></span><br><span class="line">                inside_code = <span class="number">32</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="number">65374</span> &gt;= inside_code &gt;= <span class="number">65281</span>:  <span class="comment"># 全角字符（除空格）根据关系转化</span></span><br><span class="line">                inside_code -= <span class="number">65248</span></span><br><span class="line">            b_str += chr(inside_code)</span><br><span class="line">        <span class="keyword">return</span> b_str</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">b_to_q</span><span class="params">(self, b_str)</span>:</span></span><br><span class="line">        <span class="string">"""半角转全角"""</span></span><br><span class="line">        q_str = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> uchar <span class="keyword">in</span> b_str:</span><br><span class="line">            inside_code = ord(uchar)</span><br><span class="line">            <span class="keyword">if</span> inside_code == <span class="number">32</span>:  <span class="comment"># 半角空格直接转化</span></span><br><span class="line">                inside_code = <span class="number">12288</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="number">126</span> &gt;= inside_code &gt;= <span class="number">32</span>:  <span class="comment"># 半角字符（除空格）根据关系转化</span></span><br><span class="line">                inside_code += <span class="number">65248</span></span><br><span class="line">            q_str += chr(inside_code)</span><br><span class="line">        <span class="keyword">return</span> q_str</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_process</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""语料预处理 """</span></span><br><span class="line">        lines = self.read_corpus_from_file(self.train_corpus_path)</span><br><span class="line">        new_lines = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            words = self.q_to_b(line.strip()).split(<span class="string">u'  '</span>)</span><br><span class="line">            pro_words = self.process_t(words)</span><br><span class="line">            pro_words = self.process_nr(pro_words)</span><br><span class="line">            pro_words = self.process_k(pro_words)</span><br><span class="line">            new_lines.append(<span class="string">'  '</span>.join(pro_words[<span class="number">1</span>:]))</span><br><span class="line">        self.write_corpus_to_file(data=<span class="string">'\n'</span>.join(new_lines).encode(<span class="string">'utf-8'</span>), file_path=self.process_corpus_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_k</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">"""处理大粒度分词,合并语料库中括号中的大粒度分词,类似：[国家/n  环保局/n]nt """</span></span><br><span class="line">        pro_words = []</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        temp = <span class="string">u''</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            word = words[index] <span class="keyword">if</span> index &lt; len(words) <span class="keyword">else</span> <span class="string">u''</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">u'['</span> <span class="keyword">in</span> word:</span><br><span class="line">                temp += re.sub(pattern=<span class="string">u'/[a-zA-Z]*'</span>, repl=<span class="string">u''</span>, string=word.replace(<span class="string">u'['</span>, <span class="string">u''</span>))</span><br><span class="line">            <span class="keyword">elif</span> <span class="string">u']'</span> <span class="keyword">in</span> word:</span><br><span class="line">                w = word.split(<span class="string">u']'</span>)</span><br><span class="line">                temp += re.sub(pattern=<span class="string">u'/[a-zA-Z]*'</span>, repl=<span class="string">u''</span>, string=w[<span class="number">0</span>])</span><br><span class="line">                pro_words.append(temp + <span class="string">u'/'</span> + w[<span class="number">1</span>])</span><br><span class="line">                temp = <span class="string">u''</span></span><br><span class="line">            <span class="keyword">elif</span> temp:</span><br><span class="line">                temp += re.sub(pattern=<span class="string">u'/[a-zA-Z]*'</span>, repl=<span class="string">u''</span>, string=word)</span><br><span class="line">            <span class="keyword">elif</span> word:</span><br><span class="line">                pro_words.append(word)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> pro_words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_nr</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">""" 处理姓名，合并语料库分开标注的姓和名，类似：温/nr  家宝/nr"""</span></span><br><span class="line">        pro_words = []</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            word = words[index] <span class="keyword">if</span> index &lt; len(words) <span class="keyword">else</span> <span class="string">u''</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">u'/nr'</span> <span class="keyword">in</span> word:</span><br><span class="line">                next_index = index + <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> next_index &lt; len(words) <span class="keyword">and</span> <span class="string">u'/nr'</span> <span class="keyword">in</span> words[next_index]:</span><br><span class="line">                    pro_words.append(word.replace(<span class="string">u'/nr'</span>, <span class="string">u''</span>) + words[next_index])</span><br><span class="line">                    index = next_index</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    pro_words.append(word)</span><br><span class="line">            <span class="keyword">elif</span> word:</span><br><span class="line">                pro_words.append(word)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> pro_words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_t</span><span class="params">(self, words)</span>:</span></span><br><span class="line">        <span class="string">"""处理时间,合并语料库分开标注的时间词，类似： （/w  一九九七年/t  十二月/t  三十一日/t  ）/w   """</span></span><br><span class="line">        pro_words = []</span><br><span class="line">        index = <span class="number">0</span></span><br><span class="line">        temp = <span class="string">u''</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            word = words[index] <span class="keyword">if</span> index &lt; len(words) <span class="keyword">else</span> <span class="string">u''</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">u'/t'</span> <span class="keyword">in</span> word:</span><br><span class="line">                temp = temp.replace(<span class="string">u'/t'</span>, <span class="string">u''</span>) + word</span><br><span class="line">            <span class="keyword">elif</span> temp:</span><br><span class="line">                pro_words.append(temp)</span><br><span class="line">                pro_words.append(word)</span><br><span class="line">                temp = <span class="string">u''</span></span><br><span class="line">            <span class="keyword">elif</span> word:</span><br><span class="line">                pro_words.append(word)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> pro_words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pos_to_tag</span><span class="params">(self, p)</span>:</span></span><br><span class="line">        <span class="string">"""由词性提取标签"""</span></span><br><span class="line">        t = self._maps.get(p, <span class="literal">None</span>)</span><br><span class="line">        <span class="keyword">return</span> t <span class="keyword">if</span> t <span class="keyword">else</span> <span class="string">u'O'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tag_perform</span><span class="params">(self, tag, index)</span>:</span></span><br><span class="line">        <span class="string">"""标签使用BIO模式"""</span></span><br><span class="line">        <span class="keyword">if</span> index == <span class="number">0</span> <span class="keyword">and</span> tag != <span class="string">u'O'</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">u'B_&#123;&#125;'</span>.format(tag)</span><br><span class="line">        <span class="keyword">elif</span> tag != <span class="string">u'O'</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">u'I_&#123;&#125;'</span>.format(tag)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> tag</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pos_perform</span><span class="params">(self, pos)</span>:</span></span><br><span class="line">        <span class="string">"""去除词性携带的标签先验知识"""</span></span><br><span class="line">        <span class="keyword">if</span> pos <span class="keyword">in</span> self._maps.keys() <span class="keyword">and</span> pos != <span class="string">u't'</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">u'n'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> pos</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化 """</span></span><br><span class="line">        lines = self.read_corpus_from_file(self.process_corpus_path)</span><br><span class="line">        words_list = [line.strip().split(<span class="string">'  '</span>) <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> line.strip()]</span><br><span class="line">        <span class="keyword">del</span> lines</span><br><span class="line">        self.init_sequence(words_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_sequence</span><span class="params">(self, words_list)</span>:</span></span><br><span class="line">        <span class="string">"""初始化字序列、词性序列、标记序列 """</span></span><br><span class="line">        words_seq = [[word.split(<span class="string">u'/'</span>)[<span class="number">0</span>] <span class="keyword">for</span> word <span class="keyword">in</span> words] <span class="keyword">for</span> words <span class="keyword">in</span> words_list]</span><br><span class="line">        pos_seq = [[word.split(<span class="string">u'/'</span>)[<span class="number">1</span>] <span class="keyword">for</span> word <span class="keyword">in</span> words] <span class="keyword">for</span> words <span class="keyword">in</span> words_list]</span><br><span class="line">        tag_seq = [[self.pos_to_tag(p) <span class="keyword">for</span> p <span class="keyword">in</span> pos] <span class="keyword">for</span> pos <span class="keyword">in</span> pos_seq]</span><br><span class="line">        self.pos_seq = [[[pos_seq[index][i] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(words_seq[index][i]))]</span><br><span class="line">                         <span class="keyword">for</span> i <span class="keyword">in</span> range(len(pos_seq[index]))] <span class="keyword">for</span> index <span class="keyword">in</span> range(len(pos_seq))]</span><br><span class="line">        self.tag_seq = [[[self.tag_perform(tag_seq[index][i], w) <span class="keyword">for</span> w <span class="keyword">in</span> range(len(words_seq[index][i]))]</span><br><span class="line">                         <span class="keyword">for</span> i <span class="keyword">in</span> range(len(tag_seq[index]))] <span class="keyword">for</span> index <span class="keyword">in</span> range(len(tag_seq))]</span><br><span class="line">        self.pos_seq = [[<span class="string">u'un'</span>] + [self.pos_perform(p) <span class="keyword">for</span> pos <span class="keyword">in</span> pos_seq <span class="keyword">for</span> p <span class="keyword">in</span> pos] + [<span class="string">u'un'</span>] <span class="keyword">for</span> pos_seq <span class="keyword">in</span></span><br><span class="line">                        self.pos_seq]</span><br><span class="line">        self.tag_seq = [[t <span class="keyword">for</span> tag <span class="keyword">in</span> tag_seq <span class="keyword">for</span> t <span class="keyword">in</span> tag] <span class="keyword">for</span> tag_seq <span class="keyword">in</span> self.tag_seq]</span><br><span class="line">        self.word_seq = [[<span class="string">u'&lt;BOS&gt;'</span>] + [w <span class="keyword">for</span> word <span class="keyword">in</span> word_seq <span class="keyword">for</span> w <span class="keyword">in</span> word] + [<span class="string">u'&lt;EOS&gt;'</span>] <span class="keyword">for</span> word_seq <span class="keyword">in</span> words_seq]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_feature</span><span class="params">(self, word_grams)</span>:</span></span><br><span class="line">        <span class="string">"""特征选取"""</span></span><br><span class="line">        features, feature_list = [], []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(word_grams)):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(word_grams[index])):</span><br><span class="line">                word_gram = word_grams[index][i]</span><br><span class="line">                feature = &#123;<span class="string">u'w-1'</span>: word_gram[<span class="number">0</span>], <span class="string">u'w'</span>: word_gram[<span class="number">1</span>], <span class="string">u'w+1'</span>: word_gram[<span class="number">2</span>],</span><br><span class="line">                           <span class="string">u'w-1:w'</span>: word_gram[<span class="number">0</span>] + word_gram[<span class="number">1</span>], <span class="string">u'w:w+1'</span>: word_gram[<span class="number">1</span>] + word_gram[<span class="number">2</span>],</span><br><span class="line">                           <span class="comment"># u'p-1': self.pos_seq[index][i], u'p': self.pos_seq[index][i+1],</span></span><br><span class="line">                           <span class="comment"># u'p+1': self.pos_seq[index][i+2],</span></span><br><span class="line">                           <span class="comment"># u'p-1:p': self.pos_seq[index][i]+self.pos_seq[index][i+1],</span></span><br><span class="line">                           <span class="comment"># u'p:p+1': self.pos_seq[index][i+1]+self.pos_seq[index][i+2],</span></span><br><span class="line">                           <span class="string">u'bias'</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">                feature_list.append(feature)</span><br><span class="line">            features.append(feature_list)</span><br><span class="line">            feature_list = []</span><br><span class="line">        <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">segment_by_window</span><span class="params">(self, words_list=None, window=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="string">"""窗口切分"""</span></span><br><span class="line">        words = []</span><br><span class="line">        begin, end = <span class="number">0</span>, window</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>, len(words_list)):</span><br><span class="line">            <span class="keyword">if</span> end &gt; len(words_list): <span class="keyword">break</span></span><br><span class="line">            words.append(words_list[begin:end])</span><br><span class="line">            begin = begin + <span class="number">1</span></span><br><span class="line">            end = end + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""训练数据"""</span></span><br><span class="line">        word_grams = [self.segment_by_window(word_list) <span class="keyword">for</span> word_list <span class="keyword">in</span> self.word_seq]</span><br><span class="line">        features = self.extract_feature(word_grams)</span><br><span class="line">        <span class="keyword">return</span> features, self.tag_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRF_NER</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化参数"""</span></span><br><span class="line">        self.algorithm = <span class="string">"lbfgs"</span></span><br><span class="line">        self.c1 = <span class="string">"0.1"</span></span><br><span class="line">        self.c2 = <span class="string">"0.1"</span></span><br><span class="line">        self.max_iterations = <span class="number">100</span></span><br><span class="line">        self.model_path =<span class="string">"model.pkl"</span></span><br><span class="line">        self.corpus = CorpusProcess()  <span class="comment"># Corpus 实例</span></span><br><span class="line">        self.corpus.pre_process()  <span class="comment"># 语料预处理</span></span><br><span class="line">        self.corpus.initialize()  <span class="comment"># 初始化语料</span></span><br><span class="line">        self.model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""初始化"""</span></span><br><span class="line">        algorithm = self.algorithm</span><br><span class="line">        c1 = float(self.c1)</span><br><span class="line">        c2 = float(self.c2)</span><br><span class="line">        max_iterations = int(self.max_iterations)</span><br><span class="line">        self.model = sklearn_crfsuite.CRF(algorithm=algorithm, c1=c1, c2=c2,</span><br><span class="line">                                          max_iterations=max_iterations, all_possible_transitions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""训练"""</span></span><br><span class="line">        self.initialize_model()</span><br><span class="line">        x, y = self.corpus.generator()</span><br><span class="line">        x_train, y_train = x[<span class="number">500</span>:], y[<span class="number">500</span>:]</span><br><span class="line">        x_test, y_test = x[:<span class="number">500</span>], y[:<span class="number">500</span>]</span><br><span class="line">        self.model.fit(x_train, y_train)</span><br><span class="line">        labels = list(self.model.classes_)</span><br><span class="line">        labels.remove(<span class="string">'O'</span>)</span><br><span class="line">        y_predict = self.model.predict(x_test)</span><br><span class="line">        metrics.flat_f1_score(y_test, y_predict, average=<span class="string">'weighted'</span>, labels=labels)</span><br><span class="line">        sorted_labels = sorted(labels, key=<span class="keyword">lambda</span> name: (name[<span class="number">1</span>:], name[<span class="number">0</span>]))</span><br><span class="line">        print(metrics.flat_classification_report(y_test, y_predict, labels=sorted_labels, digits=<span class="number">3</span>))</span><br><span class="line">        self.save_model()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">        <span class="string">"""预测"""</span></span><br><span class="line">        self.load_model()</span><br><span class="line">        u_sent = self.corpus.q_to_b(sentence)</span><br><span class="line">        word_lists = [[<span class="string">u'&lt;BOS&gt;'</span>] + [c <span class="keyword">for</span> c <span class="keyword">in</span> u_sent] + [<span class="string">u'&lt;EOS&gt;'</span>]]</span><br><span class="line">        word_grams = [self.corpus.segment_by_window(word_list) <span class="keyword">for</span> word_list <span class="keyword">in</span> word_lists]</span><br><span class="line">        features = self.corpus.extract_feature(word_grams)</span><br><span class="line">        y_predict = self.model.predict(features)</span><br><span class="line">        entity = <span class="string">u''</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(y_predict[<span class="number">0</span>])):</span><br><span class="line">            <span class="keyword">if</span> y_predict[<span class="number">0</span>][index] != <span class="string">u'O'</span>:</span><br><span class="line">                <span class="keyword">if</span> index &gt; <span class="number">0</span> <span class="keyword">and</span> y_predict[<span class="number">0</span>][index][<span class="number">-1</span>] != y_predict[<span class="number">0</span>][index - <span class="number">1</span>][<span class="number">-1</span>]:</span><br><span class="line">                    entity += <span class="string">u' '</span></span><br><span class="line">                entity += u_sent[index]</span><br><span class="line">            <span class="keyword">elif</span> entity[<span class="number">-1</span>] != <span class="string">u' '</span>:</span><br><span class="line">                entity += <span class="string">u' '</span></span><br><span class="line">        <span class="keyword">return</span> entity</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""加载模型 """</span></span><br><span class="line">        self.model = joblib.load(self.model_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""保存模型"""</span></span><br><span class="line">        joblib.dump(self.model, self.model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    ner = CRF_NER()</span><br><span class="line">    <span class="comment">#训练模型，当训练完毕后，就可以直接加载模型参数，不用再次训练了</span></span><br><span class="line">    <span class="comment">#mode=ner.train()</span></span><br><span class="line"></span><br><span class="line">    result1=ner.predict(<span class="string">u'新华社北京十二月三十一日电(中央人民广播电台记者刘振英、新华社记者张宿堂)今天是一九九七年的最后一天。'</span>)</span><br><span class="line">    print(result1)</span><br><span class="line">    result2=ner.predict(<span class="string">u'中国，我爱你。'</span>)</span><br><span class="line">    print(result2)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>统计学习方法</tag>
        <tag>scikit-learn</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP|BERT源码解读</title>
    <url>/2020/07/04/NLP-BERT%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>最近一直在看预训练模型，发现大部分模型的源代码基本上都是在Google官方发布的BERT源码的基础上进行修改的(但是全都是TF1.x😷，这点我要吐槽了，按道理TF2.x出来之后，Google在大力推广TF2.x，然而连Google自己发布的ELECTRA、Adapter-BERT、ALBERT等等源代码都是import tensorflow.compat.v1 as tf😷，excuse me？)。所以还是回头再仔细看了一遍原来BERT的源代码。不过，整体阅读下来，感觉还是非常顺畅的，不得不说代码写的真的好。所以这篇文章主要是记录一下自己看BERT源代码的过程。</p>
<a id="more"></a>
<h2 id="bert整体代码结构">BERT整体代码结构</h2>
<p>BERT原理我在这里就不多啰嗦了，网上一大堆，当然更加推荐的是取看原始论文。首先来看看BERT的代码文件与整体结构。如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">├── README.md</span><br><span class="line">├── create_pretraining_data.py</span><br><span class="line">├── extract_features.py</span><br><span class="line">├── modeling.py</span><br><span class="line">├── modeling_test.py</span><br><span class="line">├── multilingual.md</span><br><span class="line">├── optimization.py</span><br><span class="line">├── optimization_test.py</span><br><span class="line">├── predicting_movie_reviews_with_bert_on_tf_hub.ipynb</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── run_classifier.py</span><br><span class="line">├── run_classifier_with_tfhub.py</span><br><span class="line">├── run_pretraining.py</span><br><span class="line">├── run_squad.py</span><br><span class="line">├── sample_text.txt</span><br><span class="line">├── tokenization.py</span><br><span class="line">└── tokenization_test.py</span><br></pre></td></tr></table></figure>
<ul>
<li>create_pretraining_data.py：用来创建训练实例；</li>
<li>extract_features.py：提取出预训练的特征；</li>
<li>modeling.py：BERT的核心建模文件，模型主体部分；</li>
<li>modeling_test.py：对modeling.py文件进行unittest测试；</li>
<li>optimization.py：自定义的优化器；</li>
<li>optimization_test.py：对optimization.py文件的unittest测试；</li>
<li>predicting_movie_reviews_with_bert_on_tf_hub.ipynb：通过调用tfhub来使用BERT进行预测；</li>
<li>run_classifier.py：在多种数据集上(譬如：MRPC、XNLI、MNLI、COLA)来进行BERT模型的finetune；</li>
<li>run_classifier_with_tfhub.py：通过调用tfhub来进行finetune；</li>
<li>run_pretraining.py：通过MLM与NSP任务来对模型进行预训练；</li>
<li>run_squad.py：在squad数据集上进行finetune；</li>
<li>tokenization.py：对原始数据进行清洗、分词等操作；</li>
<li>tokenization_test.py：对 tokenization.py文件进行unittest测试。</li>
</ul>
<p>以上就是各文件的大致简介，下面将对核心代码文件进行走读，tnesor的维度以及重要注释我均已在代码里写明～</p>
<h2 id="核心代码文件走读">核心代码文件走读</h2>
<h3 id="modeling.py">modeling.py</h3>
<p>modeling.py文件是BERT模型的实现。首先来看BertConfig类，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Configuration for `BertModel`."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_act=<span class="string">"gelu"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               initializer_range=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs BertConfig.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.</span></span><br><span class="line"><span class="string">      hidden_size: Size of the encoder layers and the pooler layer.</span></span><br><span class="line"><span class="string">      num_hidden_layers: Number of hidden layers in the Transformer encoder.</span></span><br><span class="line"><span class="string">      num_attention_heads: Number of attention heads for each attention layer in</span></span><br><span class="line"><span class="string">        the Transformer encoder.</span></span><br><span class="line"><span class="string">      intermediate_size: The size of the "intermediate" (i.e., feed-forward)</span></span><br><span class="line"><span class="string">        layer in the Transformer encoder.</span></span><br><span class="line"><span class="string">      hidden_act: The non-linear activation function (function or string) in the</span></span><br><span class="line"><span class="string">        encoder and pooler.</span></span><br><span class="line"><span class="string">      hidden_dropout_prob: The dropout probability for all fully connected</span></span><br><span class="line"><span class="string">        layers in the embeddings, encoder, and pooler.</span></span><br><span class="line"><span class="string">      attention_probs_dropout_prob: The dropout ratio for the attention</span></span><br><span class="line"><span class="string">        probabilities.</span></span><br><span class="line"><span class="string">      max_position_embeddings: The maximum sequence length that this model might</span></span><br><span class="line"><span class="string">        ever be used with. Typically set this to something large just in case</span></span><br><span class="line"><span class="string">        (e.g., 512 or 1024 or 2048).</span></span><br><span class="line"><span class="string">      type_vocab_size: The vocabulary size of the `token_type_ids` passed into</span></span><br><span class="line"><span class="string">        `BertModel`.</span></span><br><span class="line"><span class="string">      initializer_range: The stdev of the truncated_normal_initializer for</span></span><br><span class="line"><span class="string">        initializing all weight matrices.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.vocab_size = vocab_size</span><br><span class="line">    self.hidden_size = hidden_size</span><br><span class="line">    self.num_hidden_layers = num_hidden_layers</span><br><span class="line">    self.num_attention_heads = num_attention_heads</span><br><span class="line">    self.hidden_act = hidden_act</span><br><span class="line">    self.intermediate_size = intermediate_size</span><br><span class="line">    self.hidden_dropout_prob = hidden_dropout_prob</span><br><span class="line">    self.attention_probs_dropout_prob = attention_probs_dropout_prob</span><br><span class="line">    self.max_position_embeddings = max_position_embeddings</span><br><span class="line">    self.type_vocab_size = type_vocab_size</span><br><span class="line">    self.initializer_range = initializer_range</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_dict</span><span class="params">(cls, json_object)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a Python dictionary of parameters."""</span></span><br><span class="line">    config = BertConfig(vocab_size=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> (key, value) <span class="keyword">in</span> six.iteritems(json_object):</span><br><span class="line">      config.__dict__[key] = value</span><br><span class="line">    <span class="keyword">return</span> config</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_json_file</span><span class="params">(cls, json_file)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a `BertConfig` from a json file of parameters."""</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(json_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      text = reader.read()</span><br><span class="line">    <span class="keyword">return</span> cls.from_dict(json.loads(text))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a Python dictionary."""</span></span><br><span class="line">    output = copy.deepcopy(self.__dict__)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">to_json_string</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Serializes this instance to a JSON string."""</span></span><br><span class="line">    <span class="keyword">return</span> json.dumps(self.to_dict(), indent=<span class="number">2</span>, sort_keys=<span class="literal">True</span>) + <span class="string">"\n"</span></span><br></pre></td></tr></table></figure>
<p>这个主要是BERT模型的配置文件，注释其实很详细了，但是需要特别说明的是<code>type_vocab_size</code>，这个表示的是segment id，默认是2，代码里没有修改，但是在bert_config.json文件里有非常详细的解释。</p>
<p>看完BertConfig类之后就是BERT模型了，但是由于BERT模型整体非常复杂，我们先来看看它的其实的component。首先来看token embedding部分，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># embedding_lookup用来获取token embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">#随机初始化embedding table</span></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># flatten，维度变为：[batch_size*seq_length*input_num]</span></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    <span class="comment"># 变为one-hot向量，维度是：[batch_size*seq_length*input_num,vocab_size]</span></span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    <span class="comment"># 维度：[batch_size*seq_length*input_num,embedding_size]</span></span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 维度：[batch_size*seq_length*input_num,embedding_size]</span></span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 维度：[batch_size,seq_length,input_num*embedding_size]</span></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure>
<p>其中每个tensor的维度我都标注的非常清楚了，看懂代码应该没有什么问题。通过<code>embedding_lookup</code>函数，我们就得到了token embedding以及embedding_table，其中embedding_table就是词向量表，如果我们不使用finetune的方式，那么我们也可以将训练好的embedding_table给抽出来，然后采用feasture based的方式来进行下游任务的训练。</p>
<p>除了token embedding之后，BERT中还有segment embedding与positional embedding，最终的embedding是这三个embedding相加得到的结果。具体实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># embedding_postprocessor用来将token embedding、segment embedding以及positional embedding进行相加，</span></span><br><span class="line"><span class="comment"># 来得到最终输入的embedding</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_postprocessor</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_token_type=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,<span class="comment"># 一般是2</span></span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_position_embeddings=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                            position_embedding_name=<span class="string">"position_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            max_position_embeddings=<span class="number">512</span>, <span class="comment"># 必须大于等于seq_length</span></span></span></span><br><span class="line"><span class="function"><span class="params">                            dropout_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 加入segment embedding</span></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"`token_type_ids` must be specified if"</span></span><br><span class="line">                       <span class="string">"`use_token_type` is True."</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># This vocab will be small so we always do one-hot here, since it is always</span></span><br><span class="line">    <span class="comment"># faster for a small vocabulary.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*seq_length]</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,token_type_vocab_size]</span></span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    <span class="comment"># [batch_size*seq_length,width]</span></span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    <span class="comment"># [batch_size,seq_length,width]</span></span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 加入positional embedding</span></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># [seq_length,width]</span></span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, <span class="number">-1</span>])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line"></span><br><span class="line">      <span class="comment"># [1,seq_length,width]</span></span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      <span class="comment"># [batch_size,seq_length,width]</span></span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 经过layer norm以及dropout</span></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p><strong>这里需要注意一下，在BERT中，embedding_size=hidden_size。</strong>得到embedding之后我们就需要将输入输入到transformer中了，在BERT当中，transformer由12个self-attention layer堆叠而成。所以，首先来看看self-attention layer的实现，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_return_2d_tensor=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(input_tensor, batch_size, num_attention_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                           seq_length, width)</span>:</span></span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之所以需要使用transpose，主要是因为后面要做QK.T，其实我觉得使用tf.einsum就不需要转置了</span></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line"></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  我们把from_tensor看作Q，to_tensor看作k，v！</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*from_seq_length,from_width]</span></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">   <span class="comment"># [batch_size*to_seq_length,to_width]</span></span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*from_seq_length,num_attention_heads*size_per_head]，即论文里提到的线性变换</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">"query"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*to_seq_length,num_attention_heads*size_per_head]，即论文里提到的线性变换</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">"key"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  <span class="comment"># [batch_size*to_seq_length,num_attention_heads*size_per_head]，即论文里提到的线性变换</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">"value"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,size_per_head]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,to_seq_length,size_per_head]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,to_seq_length]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># 放缩</span></span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 在softmax之后，对logits进行mask，防止被padding的位置的值也参与计算！</span></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在attention_mask中，1表示真实的长度，0表示是padding的</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们让padding的位置的值为无穷小，这样softmax之后的值就会接近于0！</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># # [batch_size,num_attention_heads,from_seq_length,to_seq_length]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,to_seq_length,num_attention_heads,size_per_head]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,to_seq_length,size_per_head]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,num_attention_heads,from_seq_length,size_per_head]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  <span class="comment"># [batch_size,from_seq_length,num_attention_heads,to_seq_length]</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure>
<p>这是标准的self-attention层的实现，我觉得有很多地方可以借鉴，譬如：让padding的位置经过softmax的值无限接近于0，以及在做QK.T的计算的时候，我们一开始就把输入给它reshape层2维的，即：<code>[batch_size,seq_length,width_size]</code>转化到<code>[batch_size*seq_length,num_attention_heads*size_per_head]</code>，从而加快训练，这个其实我之前都没想过，只有在看开源代码的时候才能知道。定义了sefl-attention layer之后，我们来看transfomer的实现，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,<span class="comment"># 最终输出结果的维度</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,<span class="comment"># self-attention layer的数目</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>, <span class="comment"># 每一个self-attention layer中head的数目</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>, <span class="comment"># 中间维度，即FFN的维度</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,<span class="comment"># FFN的激活函数</span></span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span> <span class="comment"># 是否返回所有层的输出结果</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># [batch_size*seq_length,input_width]</span></span><br><span class="line">  <span class="comment"># input_width=hidden_size</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># list，总共有num_hidden_layers，每一个元素的维度是：[batch_size*seq_length,hidden_size]</span></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          <span class="comment"># [batch_size*seq_length,num_attention_heads*size_per_head]</span></span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`.</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          <span class="comment"># 维度变换，变为：[batch_size*seq_length,hidden]</span></span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          <span class="comment"># dropout</span></span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          <span class="comment"># 残差连接</span></span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        <span class="comment"># 使用gelu，变为：[batch_size*seq_length,intermediate_size]</span></span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        <span class="comment"># 无激活函数，纯线性变换，变为：[batch_size*seq_length,hidden_size]</span></span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        <span class="comment"># 存储这一层的结果</span></span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure>
<p>现在，我们将embedding以及transfomer给串起来，得到完整的BERT模型。如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""BERT model ("Bidirectional Encoder Representations from Transformers").</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  ```python</span></span><br><span class="line"><span class="string">  # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">  pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">  logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for BertModel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      <span class="comment"># 如果不是训练的话，那么不能使用dropout！</span></span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        <span class="comment"># 获取token embedding，以及embedding table</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores.</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果do_return_all_layers=True，返回一个list，元素个数是num_hidden_layers,</span></span><br><span class="line">        <span class="comment"># 每一个元素维度：[batch_size,seq_length,hidden_size];</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果do_return_all_layers=False，那么就值返回最顶层的layer的输出结果，</span></span><br><span class="line">        <span class="comment"># 维度是：[batch_size,seq_length,hidden_size]</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 取到最顶层的结果，维度是：[batch_size,seq_length,hidden_size]</span></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [batch_size,hidden_size],相当于是pooling操作，我们只取与第一个元素相关的hidden state！</span></span><br><span class="line">        <span class="comment"># 为什么要取与第一个token相关的hidden state？</span></span><br><span class="line">        <span class="comment"># 因为在BERT中，第一个token是[CLS]，用于分类的！这个很重要！</span></span><br><span class="line"></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [batch_size,hidden_size]</span></span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_pooled_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.pooled_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_sequence_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets final hidden layer of encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the final hidden of the transformer encoder.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.sequence_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_all_encoder_layers</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets output of the embedding lookup (i.e., input to the transformer).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the output of the embedding layer, after summing the word</span></span><br><span class="line"><span class="string">      embeddings with the positional embeddings and the token type embeddings,</span></span><br><span class="line"><span class="string">      then performing layer normalization. This is the input to the transformer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_table</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_table</span><br></pre></td></tr></table></figure>
<p>获取最顶层的[CLS]token的tensor用于训练NSP任务，如果下游任务是分类任务的话，我们最终也是在这个的基础上，来介入softmax或者其他的结构来做；此外，获取最顶层self-ateention layer的输出结果，用来训练MLM任务。</p>
<h3 id="tokenization.py">tokenization.py</h3>
<p>tokenizaton.py文件用来对原始文本进行分词、词干化、小写、去除空格等等操作，并将原始文本向量化。在这里，需要特别提一下的话就是分词这块，BERT使用了两种分词，首先对原始文本进行粗粒度的分词，然后在此基础上，进行wordpiece分词，得到更加细粒度的分词结果。具体代码如下：</p>
<p>首先是粗粒度的分词，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs basic tokenization (punctuation splitting标点符号拆分, lower casing, etc.)."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, do_lower_case=True)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      do_lower_case: Whether to lower case the input.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.do_lower_case = do_lower_case</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="string">"""Tokenizes a piece of text."""</span></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line">    text = self._clean_text(text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was added on November 1st, 2018 for the multilingual and Chinese</span></span><br><span class="line">    <span class="comment"># models. This is also applied to the English models now, but it doesn't</span></span><br><span class="line">    <span class="comment"># matter since the English models were not trained on any Chinese data</span></span><br><span class="line">    <span class="comment"># and generally don't have any Chinese data in them (there are Chinese</span></span><br><span class="line">    <span class="comment"># characters in the vocabulary because Wikipedia does have some Chinese</span></span><br><span class="line">    <span class="comment"># words in the English Wikipedia.).</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加中文支持</span></span><br><span class="line">    text = self._tokenize_chinese_chars(text)</span><br><span class="line"></span><br><span class="line">    orig_tokens = whitespace_tokenize(text)</span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> orig_tokens:</span><br><span class="line">      <span class="keyword">if</span> self.do_lower_case:</span><br><span class="line">        token = token.lower()</span><br><span class="line">        token = self._run_strip_accents(token)</span><br><span class="line">      split_tokens.extend(self._run_split_on_punc(token))</span><br><span class="line"></span><br><span class="line">    output_tokens = whitespace_tokenize(<span class="string">" "</span>.join(split_tokens))</span><br><span class="line">    <span class="comment">#返回一个字符串</span></span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<p>然后是wordpiece分词，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">WordpieceTokenizer是将BasicTokenizer的结果进一步做更细粒度的切分。</span></span><br><span class="line"><span class="string">做这一步的目的主要是为了去除未登录词对模型效果的影响。</span></span><br><span class="line"><span class="string">这一过程对中文没有影响，</span></span><br><span class="line"><span class="string">因为在前面BasicTokenizer里面已经切分成以字为单位的了。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordpieceTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs WordPiece tokenziation."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab, unk_token=<span class="string">"[UNK]"</span>, max_input_chars_per_word=<span class="number">200</span>)</span>:</span></span><br><span class="line">    self.vocab = vocab</span><br><span class="line">    self.unk_token = unk_token</span><br><span class="line">    self.max_input_chars_per_word = max_input_chars_per_word <span class="comment"># 每一个token的最大字符数目</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    <span class="comment"># 使用贪心的最大正向匹配算法</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    我们用一个例子来看代码的执行过程。比如假设输入是”unaffable”。</span></span><br><span class="line"><span class="string">    我们跳到while循环部分，这是start=0，end=len(chars)=9，也就是先看看unaffable在不在词典里，</span></span><br><span class="line"><span class="string">    如果在，那么直接作为一个WordPiece，</span></span><br><span class="line"><span class="string">    如果不再，那么end-=1，也就是看unaffabl在不在词典里，最终发现”un”在词典里，把un加到结果里。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    接着start=2，看affable在不在，不在再看affabl，…，</span></span><br><span class="line"><span class="string">    最后发现 ##aff 在词典里。</span></span><br><span class="line"><span class="string">    注意：##表示这个词是接着前面的，这样使得WordPiece切分是可逆的——我们可以恢复出“真正”的词。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">"""Tokenizes a piece of text into its word pieces.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This uses a greedy longest-match-first algorithm to perform tokenization</span></span><br><span class="line"><span class="string">    using the given vocabulary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For example:</span></span><br><span class="line"><span class="string">      input = "unaffable"</span></span><br><span class="line"><span class="string">      output = ["un", "##aff", "##able"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      text: A single token or whitespace separated tokens. This should have</span></span><br><span class="line"><span class="string">        already been passed through `BasicTokenizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A list of wordpiece tokens.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    text = convert_to_unicode(text)</span><br><span class="line"></span><br><span class="line">    output_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> whitespace_tokenize(text):</span><br><span class="line">      chars = list(token)</span><br><span class="line">      <span class="comment"># 如果大于设置的每一个token的最大字符数目的话，那么就当作UNK</span></span><br><span class="line">      <span class="keyword">if</span> len(chars) &gt; self.max_input_chars_per_word:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      is_bad = <span class="literal">False</span></span><br><span class="line">      start = <span class="number">0</span></span><br><span class="line">      sub_tokens = []</span><br><span class="line">      <span class="keyword">while</span> start &lt; len(chars):</span><br><span class="line">        end = len(chars)</span><br><span class="line">        cur_substr = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">while</span> start &lt; end:</span><br><span class="line">          substr = <span class="string">""</span>.join(chars[start:end])</span><br><span class="line">          <span class="keyword">if</span> start &gt; <span class="number">0</span>:</span><br><span class="line">            substr = <span class="string">"##"</span> + substr</span><br><span class="line">          <span class="keyword">if</span> substr <span class="keyword">in</span> self.vocab:</span><br><span class="line">            cur_substr = substr</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">          end -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> cur_substr <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">          is_bad = <span class="literal">True</span></span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        sub_tokens.append(cur_substr)</span><br><span class="line">        start = end</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> is_bad:</span><br><span class="line">        output_tokens.append(self.unk_token)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        output_tokens.extend(sub_tokens)</span><br><span class="line">    <span class="keyword">return</span> output_tokens</span><br></pre></td></tr></table></figure>
<p>将这两分词进行结合，得到细粒度的分词结果，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullTokenizer</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Runs end-to-end tokenziation."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_file, do_lower_case=True)</span>:</span></span><br><span class="line">    self.vocab = load_vocab(vocab_file)</span><br><span class="line">    <span class="comment"># v表示index，k表示token本身</span></span><br><span class="line">    self.inv_vocab = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125;</span><br><span class="line">    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)</span><br><span class="line">    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tokenize</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    split_tokens = []</span><br><span class="line">    <span class="comment"># 调用BasicTokenizer粗粒度分词</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> self.basic_tokenizer.tokenize(text):</span><br><span class="line">       <span class="comment"># 调用WordpieceTokenizer细粒度分词</span></span><br><span class="line">      <span class="keyword">for</span> sub_token <span class="keyword">in</span> self.wordpiece_tokenizer.tokenize(token):</span><br><span class="line">        split_tokens.append(sub_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> split_tokens</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_tokens_to_ids</span><span class="params">(self, tokens)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.vocab, tokens)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">convert_ids_to_tokens</span><span class="params">(self, ids)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> convert_by_vocab(self.inv_vocab, ids)</span><br></pre></td></tr></table></figure>
<h3 id="create_pretraining_data.py">create_pretraining_data.py</h3>
<p>这部分主要是在tokenization.py的基础上，创建训练实例。我们来看BERT中是怎么实现随机MASK操作的。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span><span class="params">(tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates the predictions for the masked LM objective."""</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    <span class="comment"># [CLS]和[SEP]不能用于MASK</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">"[CLS]"</span> <span class="keyword">or</span> token == <span class="string">"[SEP]"</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> len(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">"##"</span>)):</span><br><span class="line">      cand_indexes[<span class="number">-1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  output_tokens = list(tokens)</span><br><span class="line"></span><br><span class="line">  num_to_predict = min(max_predictions_per_seq,</span><br><span class="line">                       max(<span class="number">1</span>, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = set()</span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">  <span class="comment">#-----------------------------------------whole word masking code-------------------------------------#</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">"[MASK]"</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#注意，这是20%的前50%,所以是10%！！！</span></span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="comment">#注意，这是20%的后50%,所以是10%！！！</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, len(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> len(masked_lms) &lt;= num_to_predict</span><br><span class="line">  masked_lms = sorted(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure>
<p>然后我们可以输入命令，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python create_pretraining_data.py \</span><br><span class="line">  --input_file=/Users/codewithzichao/Desktop/programs/bert/sample_text.txt \</span><br><span class="line">  --output_file=/Users/codewithzichao/Desktop/programs/bert/tf_examples.tfrecord \</span><br><span class="line">  --vocab_file=/Users/codewithzichao/Downloads/uncased_L-24_H-1024_A-16/vocab.txt \</span><br><span class="line">  --do_lower_case=True \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --masked_lm_prob=0.15 \</span><br><span class="line">  --random_seed=12345 \</span><br><span class="line">  --dupe_factor=5</span><br></pre></td></tr></table></figure>
<p>得到结果如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">I0704 18:10:11.231426 4486237632 create_pretraining_data.py:160] *** Example ***</span><br><span class="line">INFO:tensorflow:tokens: [CLS] and there burst on phil <span class="comment">##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea [MASK] ring ##ed with palaces and towers [MASK] [SEP] like most of [MASK] fellow gold - seekers , cass was super ##sti [MASK] . [SEP]</span></span><br><span class="line">I0704 18:10:11.231508 4486237632 create_pretraining_data.py:162] tokens: [CLS] and there burst on phil <span class="comment">##am ##mon ' s astonished eyes a vast semi ##ci ##rcle of blue sea [MASK] ring ##ed with palaces and towers [MASK] [SEP] like most of [MASK] fellow gold - seekers , cass was super ##sti [MASK] . [SEP]</span></span><br><span class="line">INFO:tensorflow:input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 103 3614 2098 2007 22763 1998 7626 103 102 2066 2087 1997 103 3507 2751 1011 24071 1010 16220 2001 3565 16643 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231609 4486237632 create_pretraining_data.py:172] input_ids: 101 1998 2045 6532 2006 6316 3286 8202 1005 1055 22741 2159 1037 6565 4100 6895 21769 1997 2630 2712 103 3614 2098 2007 22763 1998 7626 103 102 2066 2087 1997 103 3507 2751 1011 24071 1010 16220 2001 3565 16643 103 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231703 4486237632 create_pretraining_data.py:172] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231793 4486237632 create_pretraining_data.py:172] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_positions: 10 20 23 27 32 39 42 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231858 4486237632 create_pretraining_data.py:172] masked_lm_positions: 10 20 23 27 32 39 42 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_ids: 22741 1010 2007 1012 2010 2001 20771 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">I0704 18:10:11.231920 4486237632 create_pretraining_data.py:172] masked_lm_ids: 22741 1010 2007 1012 2010 2001 20771 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line">INFO:tensorflow:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br><span class="line">I0704 18:10:11.231988 4486237632 create_pretraining_data.py:172] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0</span><br><span class="line">INFO:tensorflow:next_sentence_labels: 1</span><br><span class="line">I0704 18:10:11.232054 4486237632 create_pretraining_data.py:172] next_sentence_labels: 1</span><br><span class="line">INFO:tensorflow:Wrote 60 total instances</span><br><span class="line">I0704 18:10:11.242784 4486237632 create_pretraining_data.py:177] Wrote 60 total instances</span><br></pre></td></tr></table></figure>
<p>我们可以看到，输出结果有：</p>
<ul>
<li>input_ids：padding之后的tokens；</li>
<li>input_mask：对input_ids进行mask得到的结果；</li>
<li>segment_ids：0表示的是第一个句子，1表示第二个句子，后面的0表示padding</li>
<li>masked_lm_positions：表示被随机MASK掉的token在instance中的位置；</li>
<li>masked_lm_ids：表示被随机MASK掉的token在词汇表中的编码；</li>
<li>masked_lm_weigths：表示被随机MASK掉的token的序列，其中1表示MASK掉的token是原始的文本token，0表示MASK的是padding之后的token。</li>
</ul>
<p>当然了，输入的文本也有要求的：一行表示一个句子，不同文章之间要隔一个空行。</p>
<h3 id="run_pretraining.py">run_pretraining.py</h3>
<p>这部分是对BERT模型进行预训练。一般我们这部分都是不用管的，直接加载已经训练好的BERT模型权重就可以了(直接训练个人基本上不太可能，太耗钱了，Google都是用多块TPU训练了好几天。。。)大致看一下它的代码结构吧～</p>
<p>由于在预训练阶段，BERT是使用MLM任务与NSP任务来进行预训练的，所以先来看一下这两个任务吧～</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义MLM任务</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_masked_lm_output</span><span class="params">(bert_config, input_tensor, output_weights, positions,</span></span></span><br><span class="line"><span class="function"><span class="params">                         label_ids, label_weights)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the masked LM."""</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size,seq_length,hidden_size]，是transformer最后一层的输出结果</span></span><br><span class="line"><span class="string">  output_weights:[vocab_size,embedding_size] 词汇表</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 获取mask词的encode，MLM loss只计算被mask掉的位置的 loss！</span></span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line">  <span class="comment"># 维度：[batch_size*max_pred_pre_seq,hidden_size]</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/predictions"</span>):</span><br><span class="line">    <span class="comment"># We apply one more non-linear transformation before the output layer.</span></span><br><span class="line">    <span class="comment"># This matrix is not used after pre-training.</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"transform"</span>):</span><br><span class="line">      <span class="comment"># 线性变换，在输出之前添加一个非线性变换，只在预训练阶段起作用</span></span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=bert_config.hidden_size,</span><br><span class="line">          activation=modeling.get_activation(bert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              bert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The output weights are the same as the input embeddings, but there is</span></span><br><span class="line">    <span class="comment"># an output-only bias for each token.</span></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>,</span><br><span class="line">        shape=[bert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment"># 得到这个batch下的结果</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq,vocab_size]</span></span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># label_ids表示mask掉的Token的id</span></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq]</span></span><br><span class="line">    label_ids = tf.reshape(label_ids, [<span class="number">-1</span>])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq,vocab_size]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The `positions` tensor might be zero-padded (if the sequence is too</span></span><br><span class="line">    <span class="comment"># short to have the maximum number of predictions). The `label_weights`</span></span><br><span class="line">    <span class="comment"># tensor has a value of 1.0 for every real prediction and 0.0 for the</span></span><br><span class="line">    <span class="comment"># padding predictions.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_pred_pre_seq]，这个相当于把mask的部分给抽出来,minimize (-log MLE)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># scalar，一个batch的loss，这个相当于把padding的部分给去除了</span></span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    <span class="comment"># scalar</span></span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + <span class="number">1e-5</span></span><br><span class="line">    loss = numerator / denominator <span class="comment"># 平均loss</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义NSP任务</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_sentence_output</span><span class="params">(bert_config, input_tensor, labels)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the next sentence prediction."""</span></span><br><span class="line"></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  input_tensor:[batch_size,hidden_size]</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Simple binary classification. Note that 0 is "next sentence" and 1 is</span></span><br><span class="line">  <span class="comment"># "random sentence". This weight matrix is not used after pre-training.</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/seq_relationship"</span>):</span><br><span class="line">    <span class="comment"># [2,hidden_size]</span></span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">"output_weights"</span>,</span><br><span class="line">        shape=[<span class="number">2</span>, bert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(bert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>, shape=[<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size,2]</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># [batch_size,2]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># minimize (-log MLE)</span></span><br><span class="line">    <span class="comment"># [batch_size]</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># scalar</span></span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gather_indexes</span><span class="params">(sequence_tensor, positions)</span>:</span></span><br><span class="line">  <span class="string">"""Gathers the vectors at the specific positions over a minibatch."""</span></span><br><span class="line">  sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = sequence_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = sequence_shape[<span class="number">1</span>]</span><br><span class="line">  width = sequence_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  flat_offsets = tf.reshape(</span><br><span class="line">      tf.range(<span class="number">0</span>, batch_size, dtype=tf.int32) * seq_length, [<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">  flat_positions = tf.reshape(positions + flat_offsets, [<span class="number">-1</span>])</span><br><span class="line">  flat_sequence_tensor = tf.reshape(sequence_tensor,</span><br><span class="line">                                    [batch_size * seq_length, width])</span><br><span class="line">  output_tensor = tf.gather(flat_sequence_tensor, flat_positions)</span><br><span class="line">  <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure>
<p>对于MLM任务，输入的是最顶层self-attention层的输出结果，维度是：<code>[batch_size,seq_length,hidden_size]</code>，但是我们需要注意的是，在MLM中，我们其实只计算被随机MASK掉的token的loss，这也是BERT需要大量的训练数据以及收敛慢的原因，我们最终需要计算的是：<code>[batch_size*max_pred_pre_seq,hidden_size]</code>，其中<code>max_pred_pre_seq</code>表示一个句子最多被MASK的数目。对于MLM loss的计算，我们是<font face="times new roman"><strong><em>minimize (-log MLE)</em></strong></font>。</p>
<p>对于NSP任务，输入的是最顶层的[CLS]token的tensor，维度是：<code>[batch_size,hidden_size]</code>，然后使用softmax进行二分类。关于NSP loss，仍然与MLM loss一样，是<font face="times new roman"><strong><em>minimize (-log MLE)</em></strong></font>。</p>
<p>定义完两个任务之后，我们就需要定义模型，来完成训练过程，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入部分</span></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>] <span class="comment"># padding后的tokens </span></span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>] <span class="comment"># 对padding后的tokens进行mask</span></span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>] <span class="comment"># segment id，用来区分两个句子</span></span><br><span class="line">    masked_lm_positions = features[<span class="string">"masked_lm_positions"</span>] <span class="comment"># 被随机MASK掉的token在句子中的位置</span></span><br><span class="line">    masked_lm_ids = features[<span class="string">"masked_lm_ids"</span>] <span class="comment"># 被随机MASK掉的token在词汇表中的编码</span></span><br><span class="line">    masked_lm_weights = features[<span class="string">"masked_lm_weights"</span>] <span class="comment"># 在被随机MASK掉的token中，如果token是原本的，那么为1，如果MASK的是padding，那么为0</span></span><br><span class="line">    next_sentence_labels = features[<span class="string">"next_sentence_labels"</span>] <span class="comment"># 是否为下一句，是为1，否则为0</span></span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化BERT模型</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得MLM任务的平均损失(scalar)，batch损失（[batch_size]）以及预测概率矩阵([batch_size*max_pred_pre_seq,vocab_size])</span></span><br><span class="line">    (masked_lm_loss,</span><br><span class="line">     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">         bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">         masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得NSP任务的平均损失(scalar)，batch损失([batch_size])以及预测概率矩阵([batch_size,2])</span></span><br><span class="line">    (next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">     next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">         bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 总的损失为两者相加</span></span><br><span class="line">    total_loss = masked_lm_loss + next_sentence_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型总的可训练参数</span></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 如果有之前保存的模型，则进行恢复</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                    masked_lm_weights, next_sentence_example_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">                    next_sentence_log_probs, next_sentence_labels)</span>:</span></span><br><span class="line">        <span class="string">"""Computes the loss and accuracy of the model."""</span></span><br><span class="line">        <span class="comment"># 计算损失与准确率</span></span><br><span class="line">        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,</span><br><span class="line">                                         [<span class="number">-1</span>, masked_lm_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        <span class="comment"># [batch_size,max_pred_pre_seq]</span></span><br><span class="line">        masked_lm_predictions = tf.argmax(</span><br><span class="line">            masked_lm_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_ids = tf.reshape(masked_lm_ids, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_weights = tf.reshape(masked_lm_weights, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># 计算在每一个位置上的准确率</span></span><br><span class="line">        masked_lm_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=masked_lm_ids,</span><br><span class="line">            predictions=masked_lm_predictions,</span><br><span class="line">            weights=masked_lm_weights)</span><br><span class="line">        masked_lm_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=masked_lm_example_loss, weights=masked_lm_weights)</span><br><span class="line"></span><br><span class="line">        next_sentence_log_probs = tf.reshape(</span><br><span class="line">            next_sentence_log_probs, [<span class="number">-1</span>, next_sentence_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        next_sentence_predictions = tf.argmax(</span><br><span class="line">            next_sentence_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        next_sentence_labels = tf.reshape(next_sentence_labels, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># 计算在NSP任务上的准确率</span></span><br><span class="line">        next_sentence_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=next_sentence_labels, predictions=next_sentence_predictions)</span><br><span class="line">        next_sentence_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=next_sentence_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"masked_lm_accuracy"</span>: masked_lm_accuracy,</span><br><span class="line">            <span class="string">"masked_lm_loss"</span>: masked_lm_mean_loss,</span><br><span class="line">            <span class="string">"next_sentence_accuracy"</span>: next_sentence_accuracy,</span><br><span class="line">            <span class="string">"next_sentence_loss"</span>: next_sentence_mean_loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn, [</span><br><span class="line">          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span><br><span class="line">          masked_lm_weights, next_sentence_example_loss,</span><br><span class="line">          next_sentence_log_probs, next_sentence_labels</span><br><span class="line">      ])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"Only TRAIN and EVAL modes are supported: %s"</span> % (mode))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure>
<h3 id="run_classifier.py">run_classifier.py</h3>
<p>如果我们想使用BERT在自己的任务上，我们需要修改的就是run_classifier.py文件。具体代码如下：</p>
<p>首先是对数据处理，对于BERT来说，Google官方实现的代码中，输入是from_tensor_slices，所以首先需要对数据集进行处理，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataProcessor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""Base class for data converters for sequence classification data sets."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_train_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the train set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_dev_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for the dev set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_test_examples</span><span class="params">(self, data_dir)</span>:</span></span><br><span class="line">    <span class="string">"""Gets a collection of `InputExample`s for prediction."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_labels</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets the list of labels for this data set."""</span></span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError()</span><br></pre></td></tr></table></figure>
<p>我们只需要继承这个类，重写这几个函数即可。数据处理完之后，来看看怎么接入下游任务，以分类任务为例，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span><span class="params">(bert_config, is_training, input_ids, input_mask, segment_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                 labels, num_labels, use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Creates a classification model."""</span></span><br><span class="line">  model = modeling.BertModel(</span><br><span class="line">      config=bert_config,</span><br><span class="line">      is_training=is_training,</span><br><span class="line">      input_ids=input_ids,</span><br><span class="line">      input_mask=input_mask,</span><br><span class="line">      token_type_ids=segment_ids,</span><br><span class="line">      use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># In the demo, we are doing a simple classification task on the entire</span></span><br><span class="line">  <span class="comment"># segment.</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If you want to use the token-level output, use model.get_sequence_output()</span></span><br><span class="line">  <span class="comment"># instead.</span></span><br><span class="line">  <span class="comment"># 如果是用于分类的话，那么我们是使用最后一层的[CLS]的向量，维度是：[batch_size,hidden_size]</span></span><br><span class="line">  output_layer = model.get_pooled_output()</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 获取hidden_size</span></span><br><span class="line">  hidden_size = output_layer.shape[<span class="number">-1</span>].value</span><br><span class="line"></span><br><span class="line">  output_weights = tf.get_variable(</span><br><span class="line">      <span class="string">"output_weights"</span>, [num_labels, hidden_size],</span><br><span class="line">      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">  output_bias = tf.get_variable(</span><br><span class="line">      <span class="string">"output_bias"</span>, [num_labels], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"loss"</span>):</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      <span class="comment"># 如果是训练的话，那就加dropout！</span></span><br><span class="line">      <span class="comment"># I.e., 0.1 dropout</span></span><br><span class="line">      output_layer = tf.nn.dropout(output_layer, keep_prob=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 线性变换，维度为：[batch_size,num_labels]</span></span><br><span class="line">    logits = tf.matmul(output_layer, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    <span class="comment"># softmax</span></span><br><span class="line">    probabilities = tf.nn.softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># log_softmax,[batch_size,num_labels]</span></span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size,num_labels]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># minimize (-log MLE)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, logits, probabilities)</span><br></pre></td></tr></table></figure>
<p>接入下游任务，得到整个模型的loss之后，然后就开始finetune，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, num_labels, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>]</span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>]</span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>]</span><br><span class="line">    label_ids = features[<span class="string">"label_ids"</span>]</span><br><span class="line">    is_real_example = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"is_real_example"</span> <span class="keyword">in</span> features:</span><br><span class="line">      is_real_example = tf.cast(features[<span class="string">"is_real_example"</span>], dtype=tf.float32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型，得到loss，batch loss，logits以及概率预测矩阵</span></span><br><span class="line">    (total_loss, per_example_loss, logits, probabilities) = create_model(</span><br><span class="line">        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,</span><br><span class="line">        num_labels, use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 训练模式</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      </span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="comment"># 评估模式</span></span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(per_example_loss, label_ids, logits, is_real_example)</span>:</span></span><br><span class="line">        predictions = tf.argmax(logits, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=label_ids, predictions=predictions, weights=is_real_example)</span><br><span class="line">        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"eval_accuracy"</span>: accuracy,</span><br><span class="line">            <span class="string">"eval_loss"</span>: loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn,</span><br><span class="line">                      [per_example_loss, label_ids, logits, is_real_example])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="comment"># 测试模式，预测</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          predictions=&#123;<span class="string">"probabilities"</span>: probabilities&#125;,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure>
<p>之后运行main函数就可以完成整个训练了。</p>
<p>整个BERT模型的代码就看完啦，读下来的感受就是：非常的舒爽，不得不说，Google写的代码质量还是非常好的，虽然我擅长的是tensorflow2.x，但是tensorflow1.x的代码读起来还是没有什么障碍的。之后最好在自己的任务上使用BERT来看看效果～除此之外，如非必要，以后关于各种BERT的变体模型就不会解读它的源代码了，基本上都是照搬BERT的源码实现，然后在预训练任务上或者BERT模型主体部分的实现上进行一些改动，整体上大同小异～</p>
<p>over～☕️</p>
]]></content>
      <categories>
        <category>NLP</category>
        <category>预训练模型</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
</search>
